<00c501cfcd26$c8415590$58c400b0$@Basu@alumni.INSEAD.edu>#|#2014-09-10-18:43:16#|#"Anirvan Basu" <Anirvan.Basu@alumni.INSEAD.edu>#|#RE: Looking for instructions & source for flink-java-examples-0.6-incubating-WebLogAnalysis.jar#|#
------=_NextPart_000_00C6_01CFCD37.8BCA2590
Content-Type: text/plain;
	charset="us-ascii"
Content-Transfer-Encoding: 7bit

Thanks Robert, once again!

.  for the workaround that you suggested with "quickstart" .



Basic steps are:

(One time),

curl the quickstart shell script

mvn clean package -DskipTests (in the folder of quickstart)



(Later, for every programme written)

Replace the java file with the new programme file and :

mvn clean package -DskipTests (in the folder of quickstart)



(same procedure as every year!!!   If you know what I mean J )

It is "quick and dirty" but saves me time from tweaking a long pom.xml for
hours!

But one day, I will be more curious to know "how it works" . I'd definitely
like to structure my programmes in multiple files and libraries
dependencies.

Will bother you again on this sometime later J



bis Bald!

Anirvan





From: rmetzger0 [via Apache Flink (Incubator) User Mailing List archive.]
[mailto:ml-node+s2336050n67h93@n4.nabble.com]
Sent: mardi 9 septembre 2014 23:54
To: nirvanesque
Subject: Re: Looking for instructions & source for
flink-java-examples-0.6-incubating-WebLogAnalysis.jar



Hi Anirvan,

sorry for the late response. You've posted the question to Nabble, which is
only a mirror of our actual mailing list at user@flink.incubator.apache.org.
Sadly, the message is not automatically posted to the apache list because
the apache server is rejecting the mails from nabble.
I've already asked and there is no way to change this behavior.
So I actually saw the two messages you posted here by accident.

Regarding your actual question:
- The command line arguments for the WebLogAnalysis example are:
   "WebLogAnalysis <documents path> <ranks path> <visits path> <result
path>"

- Regarding the "info -d" command. I think its an artifact from our old java
API. I've filed an issue in JIRA:
https://issues.apache.org/jira/browse/FLINK-1095 Lets see how we resolve it.
Thanks for reporting this!

You can find the source code of all of our examples in the source release of
Flink (in the flink-examples/flink-java-examples project. You can also
access the source (and hence the examples) through GitHub:
https://github.com/apache/incubator-flink/blob/master/flink-examples/flink-j
ava-examples/src/main/java/org/apache/flink/example/java/relational/WebLogAn
alysis.java.

To build the examples, you can run: "mvn clean package -DskipTests" in the
"flink-examples/flink-java-examples" directory. This will re-build them.

If you don't want to import the whole Flink project just for playing around
with the examples, you can also create an empty maven project. This script:
curl
https://raw.githubusercontent.com/apache/incubator-flink/master/flink-quicks
tart/quickstart.sh | bash

will automatically set everything up for you. Just import the "quickstart"
project into Eclipse or IntelliJ. It will download all dependencies and
package everything correctly. If you want to use an example there, just copy
the Java file into the "quickstart" project.

The examples are indeed a very good way to learn how to write Flink jobs.

Please continue asking if you have further questions!

Best,
Robert



  _____

If you reply to this email, your message will be added to the discussion
below:

http://apache-flink-incubator-user-mailing-list-archive.2336050.n4.nabble.co
m/Looking-for-instructions-source-for-flink-java-examples-0-6-incubating-Web
LogAnalysis-jar-tp66p67.html

To unsubscribe from Looking for instructions & source for
flink-java-examples-0.6-incubating-WebLogAnalysis.jar, click here
<http://apache-flink-incubator-user-mailing-list-archive.2336050.n4.nabble.c
om/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=66&code=QW5pcnZhb
i5CYXN1QGFsdW1uaS5JTlNFQUQuZWR1fDY2fC0xODQxMzkwMzI4> .
NAML


------=_NextPart_000_00C6_01CFCD37.8BCA2590--
#|#<1410299645732-67.post@n4.nabble.com>##//##<0876C621-8A80-439B-BA7B-BCBF3AD0B4B1@icloud.com>#|#2015-03-06-05:09:47#|#Dulaj Viduranga <vidura.me@icloud.com>#|#Running example in IntelliJ#|#
Hello,
Can someone help me with the steps on how to compile and run an example on IntelliJ#|#null##//##<087E82AA-A4B1-4AA4-8002-1E7F08DC8887@icloud.com>#|#2015-05-22-07:21:08#|#Chiwan Park <chiwanpark@icloud.com>#|#Re: question please#|#
--Apple-Mail=_0FCBF33E-604E-4E19-BDD5-04E29D936527
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Hi.

Hadoop is a framework for reliable, scalable, distributed computing. So, there are many components for this purpose such as HDFS, YARN and Hadoop MapReduce. Flink is an alternative to Hadoop MapReduce component. It has also some tools to make map-reduce program and extends it to support many operations.

You can see more detail description in Flink=E2=80=99s Homepage[1]

[1] http://flink.apache.org/faq.html#is-flink-a-hadoop-project <http://flink.apache.org/faq.html#is-flink-a-hadoop-project>


Regards.
Chiwan Park


> On May 22, 2015, at 3:02 PM, Eng Fawzya <eng.fawzya@gmail.com> wrote:
>=20
> hi,
> i want to know what is the difference between FLink and Hadoop?????????????
>=20
> --=20
> Fawzya Ramadan Sayed,
> Teaching Assistant,
> Computer Science Department,
> Faculty of Computers and Information,
> Fayoum University


--Apple-Mail=_0FCBF33E-604E-4E19-BDD5-04E29D936527--
#|#null##//##<0E5BD990-8230-4E1D-96C6-C93E581F2321@apache.org>#|#2015-06-11-22:40:45#|#Ufuk Celebi <uce@apache.org>#|#Re: Testing Apache Flink 0.9.0-rc1#|#

On 11 Jun 2015, at 20:04, Fabian Hueske <fhueske@gmail.com> wrote:

> How about the following issues?
>=20
> 1. The Hbase Hadoop Compat issue, Ufuk is working on

I was not able to reproduce this :( I ran HadoopInputFormats against various sources and confirmed the results and everything was fine so far.

I think I will open a PR for the small HadoopInputFormat fix and then we can start a new RC.

If we manage to reproduce the problem over the next days (I'm in contact with the user who reported the original issue [thanks Himi!]), we can still include a fix.

> 2. The incorrect webinterface counts

Personally, I don't think 2) is a blocker if it takes much more time. I still think it would be good to have it in.#|#<CAAdrtT2OrPYf-sFofFQwWVFGLos0_o=KNYtBJX=VGZ-i3Bakug@mail.gmail.com>##//##<1415356217247-2396.post@n3.nabble.com>#|#2014-11-07-10:31:04#|#sirinath <sirinath1978m@gmail.com>#|#Making Fink More HFT / Trading Friendly#|#
Hi,

In response to the message exchanged in LinkedIn I am shooting a few ideas.

To make Flink a great product in the Financial as well as real time, low
latency, high throughput analytics following features would be a great help:
 - Move object allocation to off heap GC does not get triggered - this can
be done through Java instrumentation so developers do not have to worry too
much about it. E.g. jillega (http://serkan-ozal.github.io/jillegal/) does
this. Also see paralleluniverse.co which uses instumentation heaveryly.

Suminda

___________________________________________________

Hi Suminda,

Thank you for your interest in Flink streaming, we are working hard to get
the features right. You seem to have a lot of experience in event stream
processing, and I am also very interested in financial applications. If we
can assist you any ways with Flink, maybe fine tune it for your application
needs, let me know and we can figure it out.

Regards,
Gyula



--
View this message in context: http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Making-Fink-More-HFT-Trading-Friendly-tp2396.html
Sent from the Apache Flink (Incubator) Mailing List archive. mailing list archive at Nabble.com.
#|#null##//##<1416928844581-2611.post@n3.nabble.com>#|#2014-11-25-15:33:14#|#samkum <sameer2kk@gmail.com>#|#Re: Java Primitive Collections in Flink#|#
Sean, which is the API you are referring to. I am actually looking for a
similar API for memory optimization but wasnt able to find it. JavaDoubleRDD
doesnt serve the purpose. Looking for a object double sort of primitve map.



--
View this message in context: http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Java-Primitive-Collections-in-Flink-tp563p2611.html
Sent from the Apache Flink (Incubator) Mailing List archive. mailing list archive at Nabble.com.
#|#<CAEccTyy5BGn=9uDzNN_p+fuyCd0FSnKQJH7cNtvwA0bgDH2Jww@mail.gmail.com>##//##<1416929049855-2612.post@n3.nabble.com>#|#2014-11-25-15:25:05#|#sirinath <sirinath1978m@gmail.com>#|#Re: Java Primitive Collections in Flink#|#
There is also https://github.com/OpenHFT/Koloboke

But I feel Flink can have its own collections which are more optimized for
Flink use cases. You can bench mark and see what works best.



--
View this message in context: http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Java-Primitive-Collections-in-Flink-tp563p2612.html
Sent from the Apache Flink (Incubator) Mailing List archive. mailing list archive at Nabble.com.
#|#<1416928844581-2611.post@n3.nabble.com>##//##<1417528437126-2713.post@n3.nabble.com>#|#2014-12-02-13:54:53#|#aalexandrov <alexander.s.alexandrov@gmail.com>#|#Re: Enhance Flink's monitoring capabilities#|#
Hello Nils,

I am going to work on a similar issue related to tracking some basics
statistics of the intermediate results produced by dataflows during
execution.

I just create a Jira issue here:

https://issues.apache.org/jira/browse/FLINK-1297

If you already have some work done on extending the monitoring capabilities
in a branch, it might be good to sync-up the development in order to avoid
duplicated work (e.g. using the same communication channel used to send the
data from the task managers to the job manager).



--
View this message in context: http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Enhance-Flink-s-monitoring-capabilities-tp2573p2713.html
Sent from the Apache Flink (Incubator) Mailing List archive. mailing list archive at Nabble.com.
#|#<CAHty2ZtWvEPOwNqYa6ADg3p9iSGfRPhYbknbwv3Pe4LY9QdUvw@mail.gmail.com>##//##<1423753856.54dcc2806622c@euranova.eu>#|#2015-02-12-15:11:49#|#Nam-Luc Tran <namluc.tran@euranova.eu>#|#Re: AW: kryoException : Buffer underflow#|#
--_=_swift_v4_142375385654dcc2806dc3c_=_
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Without the .returns(...) statement it yelled about type erasure.
Putting.returns(Centroid25.class) did the trick.

Thanks everyone for your help.

Tran Nam-Luc

At Thursday, 12/02/2015 on 12:06 Kirschnick, Johannes wrote:

Hi,

I basically just reported an issue and found this thread on the list
about the same error

Just bringing this up here, in case these issues are linked ...

There is a small testcase to reproduce attached
https://issues.apache.org/jira/browse/FLINK-1531

I tried to single in on the code and find the problem - which might be
related to the type eraser?

It seems that in the mentioned scenario there is a
MutableObjectIterator which is iterated and null is used to signal "no
more".
Because kryo is in the mix - it eagerly tries to read "next" which
fails with buffer underflow.
So somewhere there should be a hasNext call ..

Johannes
________________________________________
Von: Timo Walther=20
Gesendet: Mittwoch, 11. Februar 2015 21:55
An: dev@flink.apache.org
Betreff: Re: kryoException : Buffer underflow

@Stephan: Yes you are correct. Both omitting the "returns(...)"
statement, or changing it to "returns(Centroid25.class)" would help.

The returns(TypeInformation) and returns(String) methods do absolutely
no type extraction, the user has to know what he is doing. If you read
the methods description:

Pojo types such as org.my.MyPojo
Generic types such as java.lang.Class

With the returns(String) method you can create all types of type
information we currently support.

returns(Class) the description is as follows:

This method takes a class that will be analyzed by Flink's type
extraction capabilities.

On 11.02.2015 21:42, Stephan Ewen wrote:
> But in this case, there are no type parameters, correct? Centroid25
is not
> a generic class...
>
> On Wed, Feb 11, 2015 at 9:40 PM, Robert Metzger  wrote:
>
>> I think the issue is that the
returns("eu.euranova.flink.Centroid25")
>> variant only passes a string and the system does not know the
>> typeparameters.
>> So we have to put GenericTypeInfo there, because we basically see
Object's.
>>
>> On Wed, Feb 11, 2015 at 9:37 PM, Stephan Ewen  wrote:
>>
>>> @Timo If I understand it correctly, both omitting the
"returns(...)"
>>> statement, or changing it to "returns(Centroid25.class)" would
help?
>>>
>>> I think that the behavior between "returns(Centroid25.class)" and
"
>>> returns("eu.euranova.flink.Centroid25")" should be consistent in
that
>> they
>>> both handle the type as a POJO.
>>>
>>> Stephan
>>>
>>>
>>> On Wed, Feb 11, 2015 at 9:28 PM, Timo Walther=20
>> wrote:
>>>> Hey Nam-Luc,
>>>>
>>>> I think your problem lies in the following line:
>>>>
>>>> .returns("eu.euranova.flink.Centroid25")
>>>>
>>>> If you do not specify the fields of the class in the String by
using
>>>> "", the underlying parser will create an
>>>> "GenericTypeInfo" type information which then uses Kryo for
>>> serialization.
>>>> In general, lambda expressions are a very new feature which
currently
>>>> makes a lot of problems due to missing type information by
compilers.
>>> Maybe
>>>> it is better to use (anonymous) classes instead.
>>>>
>>>> In case of "map()" functions you don't need to provide type hints
>> through
>>>> the "returns()" method.
>>>>
>>>> For other operators you need to either specify all fields of the
class
>> in
>>>> the String (makes no sense in you case) or you change the method
to
>>>>
>>>> .returns(Centroid25.class)
>>>>
>>>> I hope that helps.
>>>>
>>>> Regards,
>>>> Timo
>>>>
>>>>
>>>> On 11.02.2015 17:38, Nam-Luc Tran wrote:
>>>>
>>>>> Hello Stephan,
>>>>>
>>>>> Thank you for your help.
>>>>>
>>>>> I ensured all the POJO classes used comply to what you
previously said
>>>>> and the same exception occurs. Here is the listing of classes
>>>>> Centroid25 and Point25:
>>>>>
>>>>> public class Centroid25 extends Point25 {
>>>>>
>>>>> public int id;
>>>>>
>>>>> public Centroid25() {}
>>>>>
>>>>> public Centroid25(int id, Double value0, Double value1, Double
value2,
>>>>> Double value3, Double value4, Double value5,
>>>>> Double value6, Double value7, Double value8, Double value9,
Double
>>>>> value10, Double value11, Double value12,
>>>>> Double value13, Double value14, Double value15, Double value16,
Double
>>>>> value17, Double value18,
>>>>> Double value19, Double value20, Double value21, Double value22,
Double
>>>>> value23, Double value24) {
>>>>> super(value0, value1, value2, value3, value4, value5, value6,
value7,
>>>>> value8, value9, value10, value11,
>>>>> value12, value13, value14, value15, value16, value17, value18,
>>>>> value19, value20, value21, value22,
>>>>> value23, value24);
>>>>> this.id =3D id;
>>>>> }
>>>>>
>>>>> public Centroid25(int id, Point25 p) {
>>>>> super(p.f0,
>>>>> p.f1,p.f2,p.f3,p.f4,p.f5,p.f6,p.f7,p.f8,p.f9,p.f10,p.f11,p.
>>>>> f12,p.f13,p.f14,p.f15,p.f16,p.f17,p.f18,p.f19,p.f20,p.f21,p.
>>>>> f22,p.f23,p.f24);
>>>>> this.id =3D id;
>>>>> }
>>>>>
>>>>> public Centroid25(int id, Tuple25 p) {
>>>>> super(p.f0,
>>>>> p.f1,p.f2,p.f3,p.f4,p.f5,p.f6,p.f7,p.f8,p.f9,p.f10,p.f11,p.
>>>>> f12,p.f13,p.f14,p.f15,p.f16,p.f17,p.f18,p.f19,p.f20,p.f21,p.
>>>>> f22,p.f23,p.f24);
>>>>> this.id =3D id;
>>>>> }
>>>>>
>>>>> @Override
>>>>> public String toString() {
>>>>> return id + " " + super.toString();
>>>>> }
>>>>> }
>>>>>
>>>>> public class Point25{
>>>>>
>>>>> public Double
>>>>> f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,f13,f14,f15,f16,
>>>>> f17,f18,f19,f20,f21,f22,f23,f24
>>>>> =3D 0.0;
>>>>>
>>>>> public Point25() {
>>>>> }
>>>>>
>>>>> public Point25(Double value0, Double value1, Double value2,
Double
>>>>> value3, Double value4, Double value5,
>>>>> Double value6, Double value7, Double value8, Double value9,
Double
>>>>> value10, Double value11, Double value12,
>>>>> Double value13, Double value14, Double value15, Double value16,
Double
>>>>> value17, Double value18,
>>>>> Double value19, Double value20, Double value21, Double value22,
Double
>>>>> value23, Double value24) {
>>>>> f0=3Dvalue0;
>>>>> f1=3Dvalue1;
>>>>> f2=3Dvalue2;
>>>>> f3=3Dvalue3;
>>>>> f4=3Dvalue4;
>>>>> f5=3Dvalue5;
>>>>> f6=3Dvalue6;
>>>>> f7=3Dvalue7;
>>>>> f8=3Dvalue8;
>>>>> f9=3Dvalue9;
>>>>> f10=3Dvalue10;
>>>>> f11=3Dvalue11;
>>>>> f12=3Dvalue12;
>>>>> f13=3Dvalue13;
>>>>> f14=3Dvalue14;
>>>>> f15=3Dvalue15;
>>>>> f16=3Dvalue16;
>>>>> f17=3Dvalue17;
>>>>> f18=3Dvalue18;
>>>>> f19=3Dvalue19;
>>>>> f20=3Dvalue20;
>>>>> f21=3Dvalue21;
>>>>> f22=3Dvalue22;
>>>>> f23=3Dvalue23;
>>>>> f24=3Dvalue24;
>>>>>
>>>>> }
>>>>>
>>>>> public List getFieldsAsList() {
>>>>> return Arrays.asList(f0, f1, f2, f3, f4, f5, f6, f7, f8, f9,
f10, f11,
>>>>> f12, f13, f14, f15, f16, f17, f18, f19,
>>>>> f20, f21, f22, f23, f24);
>>>>> }
>>>>>
>>>>> public Point25 add(Point25 other) {
>>>>> f0 +=3D other.f0;
>>>>> f1 +=3D other.f1;
>>>>> f2 +=3D other.f2;
>>>>> f3 +=3D other.f3;
>>>>> f4 +=3D other.f4;
>>>>> f5 +=3D other.f5;
>>>>> f6 +=3D other.f6;
>>>>> f7 +=3D other.f7;
>>>>> f8 +=3D other.f8;
>>>>> f9 +=3D other.f9;
>>>>> f10 +=3D other.f10;
>>>>> f11 +=3D other.f11;
>>>>> f12 +=3D other.f12;
>>>>> f13 +=3D other.f13;
>>>>> f14 +=3D other.f14;
>>>>> f15 +=3D other.f15;
>>>>> f16 +=3D other.f16;
>>>>> f17 +=3D other.f17;
>>>>> f18 +=3D other.f18;
>>>>> f19 +=3D other.f19;
>>>>> f20 +=3D other.f20;
>>>>> f21 +=3D other.f21;
>>>>> f22 +=3D other.f22;
>>>>> f23 +=3D other.f23;
>>>>> f24 +=3D other.f24;
>>>>> return this;
>>>>> }
>>>>>
>>>>> public Point25 div(long val) {
>>>>> f0 /=3D val;
>>>>> f1 /=3D val;
>>>>> f2 /=3D val;
>>>>> f3 /=3D val;
>>>>> f4 /=3D val;
>>>>> f5 +=3D val;
>>>>> f6 +=3D val;
>>>>> f7 +=3D val;
>>>>> f8 +=3D val;
>>>>> f9 +=3D val;
>>>>> f10 +=3D val;
>>>>> f11 +=3D val;
>>>>> f12 +=3D val;
>>>>> f13 +=3D val;
>>>>> f14 +=3D val;
>>>>> f15 +=3D val;
>>>>> f16 +=3D val;
>>>>> f17 +=3D val;
>>>>> f18 +=3D val;
>>>>> f19 +=3D val;
>>>>> f20 +=3D val;
>>>>> f21 +=3D val;
>>>>> f22 +=3D val;
>>>>> f23 +=3D val;
>>>>> f24 +=3D val;
>>>>> return this;
>>>>> }
>>>>>
>>>>> public double euclideanDistance(Point25 other) {
>>>>> List l =3D this.getFieldsAsList();
>>>>> List ol =3D other.getFieldsAsList();
>>>>> double res =3D 0;
>>>>> for(int i=3D0;i
>>>>>
>>>>>> I came accross an error for which I am unable to retrace the
exact
>>>>>>
>>>>> cause.
>>>>>
>>>>>> Starting from flink-java-examples module, I have extended the
KMeans
>>>>>> example
>>>>>> to a case where points have 25 coordinates. It follows the
exact
>>>>>>
>>>>> same
>>>>>
>>>>>> structure and transformations as the original example, only
with
>>>>>>
>>>>> points
>>>>>
>>>>>> having 25 coordinates instead of 2.
>>>>>>
>>>>>> When creating the centroids dataset within the code as follows
the
>>>>>>
>>>>> job
>>>>>
>>>>>> iterates and executes well:
>>>>>>
>>>>>> Centroid25 cent1 =3D new
>>>>>>
>>>>> Centroid25(ThreadLocalRandom.current().nextInt(0,
>>>>>
>>>>>> 1000),
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 -10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0,
>>>>> 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10.0);
>>>>>
>>>>>> Centroid25 cent2 =3D new
>>>>>>
>>>>> Centroid25(ThreadLocalRandom.current().nextInt(0,
>>>>>
>>>>>> 1000),
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 -1.0,-1.0,1.0,1.0,-1.0,1.0,1.0,1.0,1.0,1.0,1.0,-1.0,1.0,1.
>>>>> 0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0);
>>>>>
>>>>>> DataSet centroids =3D env.fromCollection(Arrays.asList(cent1,
>>>>>> cent2));
>>>>>>
>>>>>> When reading from a csv file containing the following:
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 -10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0,
>>>>> 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10.0
>>>>>
>>>>>>=C2=A0=C2=A0 -1.0,-1.0,1.0,1.0,-1.0,1.0,1.0,1.0,1.0,1.0,1.0,-1.0,1.0,1.
>>>>> 0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
>>>>>
>>>>>> with the following code:
>>>>>> DataSet> centroids =3D env
>>>>>>
>>>>>> .readCsvFile("file:///home/nltran/res3.csv")
>>>>>>
>>>>>>
>>>>> .fieldDelimiter(",")
>>>>>
>>>>>>
>>>>> .includeFields("1111111111111111111111111")
>>>>>
>>>>>>
>>>>> .types(Double.class, Double.class,
>>>>>
>>>>>> Double.class, Double.class,
>>>>>> Double.class, Double.class,
>>>>>>
>>>>>>
>>>>> Double.class,
>>>>>
>>>>>> Double.class, Double.class, Double.class, Double.class,
>>>>>> Double.class,
>>>>>>
>>>>>>
>>>>> Double.class,
>>>>>
>>>>>> Double.class, Double.class, Double.class, Double.class,
>>>>>> Double.class,
>>>>>>
>>>>>>
>>>>> Double.class,
>>>>>
>>>>>> Double.class, Double.class, Double.class, Double.class,
>>>>>> Double.class,
>>>>>>
>>>>>>
>>>>> Double.class).map(p -> {
>>>>>
>>>>>>
>>>>> return new
>>>>>
>>>>>> Centroid25(ThreadLocalRandom.current().nextInt(0, 1000),
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 p.f0,p.f1,p.f2,p.f3,p.f4,p.f5,p.f6,p.f7,p.f8,p.f9,p.f10,p.
>>>>> f11,p.f12,p.f13,p.f14,p.f15,p.f16,p.f17,p.f18,p.f19,p.f20,p.
>>>>> f21,p.f22,p.f23,p.f24);
>>>>>
>>>>>>
>>>>> }).returns("eu.euranova.flink.Centroid25");
>>>>>
>>>>>> I hit the following exception:
>>>>>>
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 PartialSolution (BulkIteration
(Bulk
>>>>>> Iteration))(1/1)
>>>>>> switched to FAILED
>>>>>> com.esotericsoftware.kryo.KryoException: Buffer underflow
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(
>>>>> NoFetchingInput.java:76)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:355)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(
>>>>> DefaultClassResolver.java:109)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>
>>>
org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(
>>>>> KryoSerializer.java:205)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>
>>>
org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(
>>>>> KryoSerializer.java:210)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.io.disk.InputViewIterator.next(
>>>>> InputViewIterator.java:43)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>
org.apache.flink.runtime.operators.NoOpDriver.run(NoOpDriver.java:91)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.operators.RegularPactTask.run(
>>>>> RegularPactTask.java:496)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>
>>
org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(
>>>>> AbstractIterativePactTask.java:138)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(
>>>>> IterationHeadPactTask.java:324)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.operators.RegularPactTask.
>>>>> invoke(RegularPactTask.java:360)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.execution.RuntimeEnvironment.
>>>>> run(RuntimeEnvironment.java:204)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at java.lang.Thread.run(Thread.java:745)
>>>>>>
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 Job execution switched to status
>>>>>>
>>>>> FAILING.
>>>>>
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 CHAIN Map (Map at
>>>>>>
>>>>> main(DoTheKMeans.java:64)) ->
>>>>>
>>>>>> Map (Map
>>>>>> at main(DoTheKMeans.java:65))(1/1) switched to CANCELING
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 Combine (Reduce at
>>>>>>
>>>>> main(DoTheKMeans.java:68))(1/1)
>>>>>
>>>>>> switched to CANCELING
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 CHAIN Reduce(Reduce at
>>>>>>
>>>>> main(DoTheKMeans.java:68))
>>>>>
>>>>>> -> Map
>>>>>> (Map at main(DoTheKMeans.java:71))(1/1) switched to CANCELING
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 DataSink(Print to System.out)(1/1)
>>>>>>
>>>>> switched to
>>>>>
>>>>>> CANCELED
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 Sync(BulkIteration (Bulk
>>>>>>
>>>>> Iteration))(1/1) switched
>>>>>
>>>>>> to
>>>>>> CANCELING
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 Sync(BulkIteration (Bulk
>>>>>>
>>>>> Iteration))(1/1) switched
>>>>>
>>>>>> to
>>>>>> CANCELED
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 CHAIN Map (Map at
>>>>>>
>>>>> main(DoTheKMeans.java:64)) ->
>>>>>
>>>>>> Map (Map
>>>>>> at main(DoTheKMeans.java:65))(1/1) switched to CANCELED
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 Combine (Reduce at
>>>>>>
>>>>> main(DoTheKMeans.java:68))(1/1)
>>>>>
>>>>>> switched to CANCELED
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 CHAIN Reduce(Reduce at
>>>>>>
>>>>> main(DoTheKMeans.java:68))
>>>>>
>>>>>> -> Map
>>>>>> (Map at main(DoTheKMeans.java:71))(1/1) switched to CANCELED
>>>>>> 02/11/2015 14:58:27=C2=A0=C2=A0=C2=A0=C2=A0 Job execution switched to status
FAILED.
>>>>>> Exception in thread "main"
>>>>>> org.apache.flink.runtime.client.JobExecutionException:
>>>>>> com.esotericsoftware.kryo.KryoException: Buffer underflow
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(
>>>>> NoFetchingInput.java:76)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:355)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(
>>>>> DefaultClassResolver.java:109)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>
>>>
org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(
>>>>> KryoSerializer.java:205)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>
>>>
org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(
>>>>> KryoSerializer.java:210)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.io.disk.InputViewIterator.next(
>>>>> InputViewIterator.java:43)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>
org.apache.flink.runtime.operators.NoOpDriver.run(NoOpDriver.java:91)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.operators.RegularPactTask.run(
>>>>> RegularPactTask.java:496)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>
>>
org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(
>>>>> AbstractIterativePactTask.java:138)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(
>>>>> IterationHeadPactTask.java:324)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.operators.RegularPactTask.
>>>>> invoke(RegularPactTask.java:360)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.execution.RuntimeEnvironment.
>>>>> run(RuntimeEnvironment.java:204)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at java.lang.Thread.run(Thread.java:745)
>>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.client.JobClientListener$$anonfun$
>>>>> receiveWithLogMessages$2.applyOrElse(JobClient.scala:88)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0
scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(
>>>>> AbstractPartialFunction.scala:33)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 scala.runtime.AbstractPartialFunction$mcVL$sp.apply(
>>>>> AbstractPartialFunction.scala:33)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 scala.runtime.AbstractPartialFunction$mcVL$sp.apply(
>>>>> AbstractPartialFunction.scala:25)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.ActorLogMessages$$anon$1.
>>>>> apply(ActorLogMessages.scala:37)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.ActorLogMessages$$anon$1.
>>>>> apply(ActorLogMessages.scala:30)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.ActorLogMessages$$anon$1.
>>>>> applyOrElse(ActorLogMessages.scala:30)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> akka.actor.Actor$class.aroundReceive(Actor.scala:465)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 org.apache.flink.runtime.client.JobClientListener.
>>>>> aroundReceive(JobClient.scala:74)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
akka.actor.ActorCell.invoke(ActorCell.scala:487)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>> akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
akka.dispatch.Mailbox.run(Mailbox.scala:221)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
akka.dispatch.Mailbox.exec(Mailbox.scala:231)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.
>>>>> runTask(ForkJoinPool.java:1339)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>=C2=A0=C2=A0 scala.concurrent.forkjoin.ForkJoinPool.runWorker(
>>>>> ForkJoinPool.java:1979)
>>>>>
>>>>>>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0at
>>>>>>
>>>>>>
>>>>>>=C2=A0=C2=A0 scala.concurrent.forkjoin.ForkJoinWorkerThread.run(
>>>>> ForkJoinWorkerThread.java:107)
>>>>>
>>>>>> The centroid25 data is exactly the same in both cases. Could
you
>>>>>>
>>>>> help me
>>>>>
>>>>>> retrace what is wrong?
>>>>>>
>>>>>> Thanks and best regards,
>>>>>>
>>>>>> Tran Nam-Luc
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> View this message in context:
>>>>>>
>>>>>>=C2=A0=C2=A0 http://apache-flink-incubator-mailing-list-archive.1008284.
>>>>> n3.nabble.com/kryoException-Buffer-underflow-tp3760.html
>>>>>
>>>>>> Sent from the Apache Flink (Incubator) Mailing List archive.
mailing
>>>>>>
>>>>> list
>>>>>
>>>>>> archive at Nabble.com.
>>>>>>
>>>>>>
>>>>>



--_=_swift_v4_142375385654dcc2806dc3c_=_--
#|#null##//##<1429850304127-5304.post@n3.nabble.com>#|#2015-04-24-04:38:51#|#sirinath <sirinath1978m@gmail.com>#|#Re: Apache Ignite#|#
Any thoughts on this?



--
View this message in context: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Apache-Ignite-tp5115p5304.html
Sent from the Apache Flink Mailing List archive. mailing list archive at Nabble.com.
#|#<1429253798438-5199.post@n3.nabble.com>##//##<1432113467347.75326@kth.se>#|#2015-05-20-09:18:37#|#Paris Carbone <parisc@kth.se>#|#Re: [DISCUSS] Re-add record copy to chained operator calls#|#
--_000_143211346734775326kthse_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

I guess it was not intended ^^.

Chaining should be transparent and not break the correct/expected behaviour.


Paris?

On 20 May 2015, at 11:02, M=E1rton Balassi <mbalassi@apache.org> wrote:

+1 for copying.
On May 20, 2015 10:50 AM, "Gyula F=F3ra" <gyfora@apache.org> wrote:

Hey,

The latest streaming operator rework removed the copying of the outputs
before passing them to chained operators. This is a major break for the
previous operator semantics which guaranteed immutability.

I think this change leads to very indeterministic program behaviour from
the user's perspective as only non-chained outputs/inputs will be mutable.
If we allow this to happen, users will start disabling chaining to get
immutability which defeats the purpose. (chaining should not affect program
behaviour just increase performance)

In my opinion the default setting for each operator should be immutability
and the user could override this manually if he/she wants.

What do you think?

Regards,
Gyula



--_000_143211346734775326kthse_--
#|#<CAKADb_PBG8-nKgxLjeTKxUeYgve2-tCvf7ZQ+Zgansb41H1vOA@mail.gmail.com>##//##<18694373201a4c629f77230afd95f7b1@EX-MBX01.tubit.win.tu-berlin.de>#|#2015-03-04-11:50:38#|#"Kirschnick, Johannes" <johannes.kirschnick@tu-berlin.de>#|#Re: Access flink-conf.yaml data#|#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#|#null##//##<1971CB7D-8D30-43EA-9D83-2DC104299D24@apache.org>#|#2015-04-09-22:23:47#|#Ufuk Celebi <uce@apache.org>#|#Re: Quickstart build - Fat Jar is empty#|#
On 09 Apr 2015, at 23:22, Henry Saputra <henry.saputra@gmail.com> wrote:

> Maybe we should just do quick patch release on 0.8.x branch to fix this, Robert?

Is there a reason, why the fix has not been back ported to 0.8? As you said, the fat jar gets very big, but that is still better than empty.

I am in favour of this. Big +1.

1) This is not the first time a user has run into this
2) It makes for a very bad first time experience using the system and defeats the purpose of a quickstart
3) It's not a lot of work to get the vote started and the RC would not need a lot of testing

I know we are currently voting on the 0.9.0-milestone-1 release, but since it is not an official stable release, I can see people sticking to 0.8. I think we don't even plan to link to the milestone release as the primary download.

=96 Ufuk#|#<CALuGr6aSd=RndsZwYQhPRHp67qKGvgvy+spOpAHcPNnR28jJPw@mail.gmail.com>##//##<1AEC5DD8-A60A-4B6D-89F9-19E4834D20B6@0xdata.com>#|#2015-01-24-21:21:25#|#Sri Ambati <srisatish@0xdata.com>#|#Re: Cool project: H2O on Flink#|#
--Apple-Mail-F3FEA250-D732-4D0B-A6F3-0706E87D5141
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable

Kostas,=20
Thank you for your generosity.

We are honored to be a part of Apache Flink's community & it's amazing journey from Berlin and beyond!

H2O is excited to bring best-in-class machine learning to application developers worldwide.=20

Looking forward,
Sri

> On Jan 24, 2015, at 11:39 AM, Kostas Tzoumas <ktzoumas@apache.org> wrote:
>=20
> Hi everyone,
>=20
> I had a chat with some folks behind the H2O project (http://h2o.ai), and they would be interested in having H2O run on top/inside of Flink. H2O is a very performant system focused on Machine Learning.
>=20
> A similar integration has been implemented for H2O on Spark (called sparkling water - https://github.com/h2oai/sparkling-water).=20
>=20
> This would be a very cool project for someone that is interested in getting started with Flink, and it should not be too hard to get started by following sparkling water as a blueprint.=20
>=20
> So if someone wants to jump on that, it would be great! Michal, the creator of sparkling water is also willing to guide/give advice.
>=20

--Apple-Mail-F3FEA250-D732-4D0B-A6F3-0706E87D5141--
#|#<CAGWx-_vY3xSb0av4+cYfpPaN0Ds7ava0H5fwVC08gKgLAsb1ig@mail.gmail.com>##//##<20140612211756.660B68C3A43@tyr.zones.apache.org>#|#2014-06-12-21:18:18#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Fix for FLINK-929#|#
Github user rmetzger closed the pull request at:

    https://github.com/apache/incubator-flink/pull/13


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-13-incubator-flink@git.apache.org>##//##<20140612212717.CEB178C3AB6@tyr.zones.apache.org>#|#2014-06-12-21:27:39#|#rwaury <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-927] Quick and dirty fix for ...#|#
Github user rwaury closed the pull request at:

    https://github.com/apache/incubator-flink/pull/11


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-11-incubator-flink@git.apache.org>##//##<20140613103838.E175C932D23@tyr.zones.apache.org>#|#2014-06-13-10:39:01#|#StephanEwen <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-917] Rename netty IO thread c...#|#
Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/5#issuecomment-45997480

    Looks good, will merge


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-5-incubator-flink@git.apache.org>##//##<20140613105125.82F3E932DB5@tyr.zones.apache.org>#|#2014-06-13-10:51:47#|#StephanEwen <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-934] Remove default values fo...#|#
Github user StephanEwen commented on the pull request:

    https://github.com/apache/incubator-flink/pull/14#issuecomment-45998358

    I will merge this...


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-14-incubator-flink@git.apache.org>##//##<20140613134114.A30089331BA@tyr.zones.apache.org>#|#2014-06-13-13:41:39#|#zentol <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Serialized String comparison, Unicod...#|#
Github user zentol commented on the pull request:

    https://github.com/apache/incubator-flink/pull/4#issuecomment-46012075

    the new serialization seems to take longer, but broke even due to faster deserialization.

    some comparison benchmarks:

    strings of length 90
    1000 repetitions
    code for New measurement:
    ```
    long startTime = System.nanoTime();
    cmp = StringValue.compareUnicode(in1, in2);
    long endTime = System.nanoTime();
    ```
    code for Old measurement:
    ```
    long startTime = System.nanoTime();
    cmp = StringValue.readString(in1).compareTo(StringValue.readString(in2));
    long endTime = System.nanoTime();
    ```

    result = SUM(endTime - startTime) / 1000

    equality
    New 26627
    Old 25782

    affix (difference in the beginning of the sring)
    New 4259
    Old 23431

    infix (difference in the middle of the string)
    New 13560
    Old 30757

    suffix (difference at the end of the string)
    New 29385
    Old 28293

    not particularly surprising results. no progress on the prefix-issue :/

    adding some more tests now.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-4-incubator-flink@git.apache.org>##//##<20140615180807.BA7A093B60E@tyr.zones.apache.org>#|#2014-06-15-18:08:29#|#warneke <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Do not limit the client's heap space...#|#
Github user warneke commented on the pull request:

    https://github.com/apache/incubator-flink/pull/18#issuecomment-46122924

    Using RPCs to distribute jar files is still the quick hack from the very early days of the project. I've already fixed this issue on several branches but I guess the change did not make it to the upstream code. I could offer to take care of this problem, either through some sort of HTTP transfer or by leveraging the distributed file system.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-18-incubator-flink@git.apache.org>##//##<20140618101823.C64F483B6A3@tyr.zones.apache.org>#|#2014-06-18-10:18:46#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Fixed JSON Bug for some plans.#|#
Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/23#issuecomment-46418267

    Thank you for the pull request. I'm going to merge this soon.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-23-incubator-flink@git.apache.org>##//##<20140618174858.59E4A83BFB2@tyr.zones.apache.org>#|#2014-06-18-17:49:23#|#uce <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-758] Add initial value to Gen...#|#
Github user uce commented on the pull request:

    https://github.com/apache/incubator-flink/pull/20#issuecomment-46469440

    I also find it clumsy. Will change it as you suggested. I'm not sure why I didn't think of this myself.

    Sent from my iPhone

    > On 18 Jun 2014, at 19:32, Stephan Ewen <notifications@github.com> wrote:
    >
    > Why do you add the "setInitialValue()" method to the reduce function? I think that is a bit clumsy. Is that for getting the value into the runtime?
    >
    > Why not add it as part of the user parameters? That keeps the user-facing APIs clean.
    >
    > â€”
    > Reply to this email directly or view it on GitHub.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-20-incubator-flink@git.apache.org>##//##<20140618185308.59DB09432E8@tyr.zones.apache.org>#|#2014-06-18-18:53:30#|#asfgit <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Projection constructor adjusts to ma...#|#
Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/27


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-27-incubator-flink@git.apache.org>##//##<20140619114458.6D289983CCA@tyr.zones.apache.org>#|#2014-06-19-11:45:20#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-949] Properly report GlobalBu...#|#
Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/28#issuecomment-46550620

    Looks good.
    Have you tested if the exceptions are properly thrown?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-28-incubator-flink@git.apache.org>##//##<20140620075100.7E41998698E@tyr.zones.apache.org>#|#2014-06-20-07:51:22#|#zentol <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Serialized String comparison, Unicod...#|#
Github user zentol commented on the pull request:

    https://github.com/apache/incubator-flink/pull/4#issuecomment-46653245

    i may have misunderstood your question. if you want to compare the data byte-wise, without having to modify it (like padding with zeroes) or analyze it (like reading them as chars), then the encoding has to be changed, my idea would be using the first 2 bits of every byte to encode the index of that particular byte.

    This will reduce the effectiveness of the variable-length encoding (all letters are encoded with 2 bytes), but you could use the very loop you described.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-4-incubator-flink@git.apache.org>##//##<20140620191738.C4F19987CE1@tyr.zones.apache.org>#|#2014-06-20-19:18:01#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: The Hadoop Compatibility has been re...#|#
Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/32#discussion_r14036891

    --- Diff: stratosphere-addons/hadoop-compatibility/src/main/java/eu/stratosphere/hadoopcompatibility/mapred/HadoopInputFormat.java ---
    @@ -0,0 +1,287 @@
    +/***********************************************************************************************************************
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + **********************************************************************************************************************/
    +
    +package eu.stratosphere.hadoopcompatibility.mapred;
    +
    +import java.io.IOException;
    +import java.io.ObjectInputStream;
    +import java.io.ObjectOutputStream;
    +import java.util.ArrayList;
    +
    +import org.apache.commons.logging.Log;
    +import org.apache.commons.logging.LogFactory;
    +import org.apache.hadoop.io.Writable;
    +import org.apache.hadoop.mapred.FileInputFormat;
    +import org.apache.hadoop.mapred.JobConf;
    +import org.apache.hadoop.mapred.RecordReader;
    +import org.apache.hadoop.util.ReflectionUtils;
    +
    +import eu.stratosphere.api.common.io.FileInputFormat.FileBaseStatistics;
    +import eu.stratosphere.api.common.io.InputFormat;
    +import eu.stratosphere.api.common.io.statistics.BaseStatistics;
    +import eu.stratosphere.api.java.tuple.Tuple2;
    +import eu.stratosphere.api.java.typeutils.ResultTypeQueryable;
    +import eu.stratosphere.api.java.typeutils.TupleTypeInfo;
    +import eu.stratosphere.api.java.typeutils.WritableTypeInfo;
    +import eu.stratosphere.configuration.Configuration;
    +import eu.stratosphere.core.fs.FileStatus;
    +import eu.stratosphere.core.fs.FileSystem;
    +import eu.stratosphere.core.fs.Path;
    +import eu.stratosphere.hadoopcompatibility.mapred.utils.HadoopUtils;
    +import eu.stratosphere.hadoopcompatibility.mapred.wrapper.HadoopDummyReporter;
    +import eu.stratosphere.hadoopcompatibility.mapred.wrapper.HadoopInputSplit;
    +import eu.stratosphere.types.TypeInformation;
    +
    +public class HadoopInputFormat<K extends Writable, V extends Writable> implements InputFormat<Tuple2<K,V>, HadoopInputSplit>, ResultTypeQueryable<Tuple2<K,V>> {
    +
    +	private static final long serialVersionUID = 1L;
    +
    +	private static final Log LOG = LogFactory.getLog(HadoopInputFormat.class);
    +
    +	private org.apache.hadoop.mapred.InputFormat<K, V> mapredInputFormat;
    +	private Class<K> keyClass;
    +	private Class<V> valueClass;
    +	private JobConf jobConf;
    +
    +	public transient K key;
    +	public transient V value;
    +
    +	public RecordReader<K, V> recordReader;
    --- End diff --

    we should mark the `recordReader` as transient as well. Even though we do custom serialization, it is helpful as a flag when reading the code (similar to key and value, the transient flag has actually no meaning here, since we do custom ser)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-32-incubator-flink@git.apache.org>##//##<20140623082944.119D3886EF0@tyr.zones.apache.org>#|#2014-06-23-08:30:09#|#fhueske <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-962] Initial import of docume...#|#
Github user fhueske commented on the pull request:

    https://github.com/apache/incubator-flink/pull/34#issuecomment-46817904

    Nice!
    Can we automatically insert links to the top of the page (outline) at the end of each h1 (or h2) headed section?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-34-incubator-flink@git.apache.org>##//##<20140625072924.50592912E41@tyr.zones.apache.org>#|#2014-06-25-07:29:49#|#mariemayadi <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Local Executor pick up the plans req...#|#
Github user mariemayadi commented on the pull request:

    https://github.com/apache/incubator-flink/pull/40#issuecomment-47068686

    Great.


    On Wed, Jun 25, 2014 at 9:28 AM, Robert Metzger <notifications@github.com>
    wrote:

    > Great!
    > I think its good to merge.
    >
    > â€”
    > Reply to this email directly or view it on GitHub
    > <https://github.com/apache/incubator-flink/pull/40#issuecomment-47068580>.
    >


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-40-incubator-flink@git.apache.org>##//##<20140625082708.B975D912F56@tyr.zones.apache.org>#|#2014-06-25-08:27:33#|#asfgit <git@git.apache.org>#|#[GitHub] incubator-flink pull request: [FLINK-979] Fix NetworkThroughput te...#|#
Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/41


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-41-incubator-flink@git.apache.org>##//##<20140625151600.7904E93A9DE@tyr.zones.apache.org>#|#2014-06-25-15:16:25#|#kfleischmann <git@git.apache.org>#|#[GitHub] incubator-flink pull request: New operator map partition function#|#
Github user kfleischmann commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/42#discussion_r14192094

    --- Diff: stratosphere-compiler/src/main/java/eu/stratosphere/compiler/dag/MapPartitionNode.java ---
    @@ -0,0 +1,56 @@
    +/***********************************************************************************************************************
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + **********************************************************************************************************************/
    +
    +package eu.stratosphere.compiler.dag;
    +
    +import java.util.Collections;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.operators.SingleInputOperator;
    +import eu.stratosphere.compiler.DataStatistics;
    +import eu.stratosphere.compiler.operators.MapPartitionDescriptor;
    +import eu.stratosphere.compiler.operators.OperatorDescriptorSingle;
    +
    +/**
    + * The optimizer's internal representation of a <i>MapPartition</i> operator node.
    + */
    +public class MapPartitionNode extends SingleInputNode {
    +
    +	/**
    +	 * Creates a new MapNode for the given contract.
    +	 *
    +	 * @param operator The map partition contract object.
    +	 */
    +	public MapPartitionNode(SingleInputOperator<?, ?, ?> operator) {
    +		super(operator);
    +	}
    +
    +	@Override
    +	public String getName() {
    +		return "MapPartition";
    +	}
    +
    +	@Override
    +	protected List<OperatorDescriptorSingle> getPossibleProperties() {
    +		return Collections.<OperatorDescriptorSingle>singletonList(new MapPartitionDescriptor());
    +	}
    +
    +	/**
    +	 * Computes the estimates for the MapPartition operator.
    +	 * We assume that by default, Map takes one value and transforms it into another value.
    +	 * The cardinality consequently stays the same.
    +	 */
    +	@Override
    +	protected void computeOperatorSpecificDefaultEstimates(DataStatistics statistics) {
    +	}
    --- End diff --

    Because we cannot do any predictions about the output of this operator.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-42-incubator-flink@git.apache.org>##//##<20140625153034.0617193AA89@tyr.zones.apache.org>#|#2014-06-25-15:30:57#|#JonathanH5 <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Finished Flink-928#|#
Github user JonathanH5 commented on the pull request:

    https://github.com/apache/incubator-flink/pull/43#issuecomment-47117560

    The Taskmanagers are shown in the configuration section (at the bottom)


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-43-incubator-flink@git.apache.org>##//##<20140625164224.38D8393AE9E@tyr.zones.apache.org>#|#2014-06-25-16:42:49#|#tobwiens <git@git.apache.org>#|#[GitHub] incubator-flink pull request: FIX-FLINK-888-swapValues() for Tuple...#|#
Github user tobwiens commented on the pull request:

    https://github.com/apache/incubator-flink/pull/44#issuecomment-47127623

    When the VM casts dynamically the values the type specific equals method is called. That was the idea behind the test case. If the types are not correct that test will fail.

    You mean that I should swap an swapped Tuple2 and compare their values?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-44-incubator-flink@git.apache.org>##//##<20140627094010.44BD498D4D5@tyr.zones.apache.org>#|#2014-06-27-09:40:32#|#JonathanH5 <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Finished Flink-928#|#
Github user JonathanH5 commented on the pull request:

    https://github.com/apache/incubator-flink/pull/43#issuecomment-47325361

    @rmetzger , ok I added you ideas, can you check it again?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-43-incubator-flink@git.apache.org>##//##<20140627114604.A051398D89C@tyr.zones.apache.org>#|#2014-06-27-11:46:26#|#uce <git@git.apache.org>#|#[GitHub] incubator-flink pull request: FIX-FLINK-888-swapValues() for Tuple...#|#
Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/44#discussion_r14288968

    --- Diff: stratosphere-java/src/main/java/eu/stratosphere/api/java/tuple/Tuple2.java ---
    @@ -99,6 +99,15 @@ public void setFields(T0 value0, T1 value1) {
     		this.f1 = value1;
     	}

    +	/**
    +	* Returns a shallow copy of the Tuple2 with swapped values.
    +	*
    +	* @return A Tuple2<T1, T0> shallow copy. Value0 and value1 are swapped.
    --- End diff --

    I think this comment is a bit ambiguous... I would say either remove the `@return` or just copy the `Shallow copy of the Tuple2 with swapped values.`.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-44-incubator-flink@git.apache.org>##//##<20140627114714.BBFCB98D89E@tyr.zones.apache.org>#|#2014-06-27-11:47:40#|#uce <git@git.apache.org>#|#[GitHub] incubator-flink pull request: FIX-FLINK-888-swapValues() for Tuple...#|#
Github user uce commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/44#discussion_r14288993

    --- Diff: stratosphere-java/src/test/java/eu/stratosphere/api/java/tuple/Tuple2Test.java ---
    @@ -0,0 +1,43 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +
    +package eu.stratosphere.api.java.tuple;
    +
    +import junit.framework.Assert;
    +
    +import org.junit.Test;
    +
    +public class Tuple2Test {
    +
    +
    +	@Test
    +	public void testSwapValues() {
    +		Tuple2<String, Integer> fullTuple2 = new Tuple2<String, Integer>(new String("Test case"), 25);
    +		//Swapped tuple for comparison
    +		Tuple2<Integer, String> swappedTuple2 = fullTuple2.swapValues();
    +
    +		// Assert when not the same
    +		// Use overloaded equals method to check for equality. Especially important for String.
    +		if(!swappedTuple2.f1.equals(fullTuple2.getField(0))) {
    --- End diff --

    Can't you just use JUnit's assertEquals, i.e. `Assert.assertEquals(fullTuple2.f0, swappedTuple2.f1)`?


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-44-incubator-flink@git.apache.org>##//##<20140627143742.021A998DD73@tyr.zones.apache.org>#|#2014-06-27-14:38:04#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Finished Flink-928#|#
Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/43#discussion_r14295464

    --- Diff: stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/web/ConfigurationServlet.java ---
    @@ -65,13 +90,53 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)
     			try {
     				obj.put(k, globalC.getString(k, ""));
     			} catch (JSONException e) {
    -				e.printStackTrace();
    +				LOG.warn("Json object creation failed", e);
     			}
     		}

     		PrintWriter w = resp.getWriter();
     		w.write(obj.toString());
    -
    +
    +	}
    +
    +	private void writeTaskmanagers(HttpServletResponse resp) throws IOException {
    +
    +		Set<InstanceConnectionInfo> keys = jobmanager.getInstances().keySet();
    +		List<InstanceConnectionInfo> list = new ArrayList<InstanceConnectionInfo>(keys);
    +		Collections.sort(list);
    +
    +		JSONObject obj = new JSONObject();
    +		JSONArray array = new JSONArray();
    +		for (InstanceConnectionInfo k : list) {
    +			JSONObject objInner = new JSONObject();
    +
    +			Instance instance = jobmanager.getInstances().get(k);
    +			long time = new Date().getTime() - instance.getLastHeartBeat();
    +
    +			try {
    +				objInner.put("inetAdress", k.getInetAdress());
    +				objInner.put("ipcPort", k.ipcPort());
    +				objInner.put("dataPort", k.dataPort());
    +				objInner.put("timeSinceLastHeartbeat", time);
    --- End diff --

    can you divide the time by 1000 to make it seconds instead of miliseconds? (Its okay if the interface only says 0 or 1). The "Seconds since last heartbeat" column is mainly useful for detecting machines that are not reporting in fast enough.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-43-incubator-flink@git.apache.org>##//##<20140630085248.BFD1F91B348@tyr.zones.apache.org>#|#2014-06-30-08:53:13#|#asfgit <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Mention that Oracle JDK 6 library wi...#|#
Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-flink/pull/49


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-49-incubator-flink@git.apache.org>##//##<20140630155639.98A3092F291@tyr.zones.apache.org>#|#2014-06-30-15:57:05#|#tobwiens <git@git.apache.org>#|#[GitHub] incubator-flink pull request: FIX-FLINK-888-swapValues() for Tuple...#|#
Github user tobwiens commented on the pull request:

    https://github.com/apache/incubator-flink/pull/44#issuecomment-47549619

    Obviously! Thank you!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-44-incubator-flink@git.apache.org>##//##<20140701133507.21EE19911AB@tyr.zones.apache.org>#|#2014-07-01-13:35:31#|#skunert <git@git.apache.org>#|#[GitHub] incubator-flink pull request: Documentation completely transferred...#|#
Github user skunert commented on the pull request:

    https://github.com/apache/incubator-flink/pull/54#issuecomment-47656211

    I changed the gh_link script a bit to support custom naming of the link. The last argument is now always the name of the link. The downside is that if you want to name the link you have to also insert the branch.

    # Example
    {% gh_link README.md master This is the readme file %}

    Even though it is not that elegant that the last 5 arguments build the name string I think it is a necessary feature for the documentation. I also found some links that are still incorrect so this is not yet finished.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-54-incubator-flink@git.apache.org>##//##<20140702071058.79D26992C66@tyr.zones.apache.org>#|#2014-07-02-07:11:20#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: providing accumulator example as pro...#|#
Github user rmetzger commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14443290

    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,281 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.operators.DataSource;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it
    + * counts the number of empty fields per column within a CSV file using a custom
    + * accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + *
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator
    + * and double quotes as field delimiters and 9 columns. See
    + * {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + *
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines &lt;input file path&gt; &lt;result path&gt;</code> <br>
    + *
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings("serial")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = "empty-fields";
    +
    +	public static void main(String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment
    +				.getExecutionEnvironment();
    +
    +		// get the data set
    +		DataSet<Tuple> file = getDataSet(env);
    --- End diff --

    Why are you not using a `Tuple9<String,String, ...>` for the DataSet ?
    This works also fine but you are loosing the compile-time type checking. Basically the typeparamters will make line 105: https://github.com/apache/incubator-flink/pull/55/files#diff-70cc9ad1c8f8e2e82718452620b0d33eR105 safer.

    I think its fine in this case to use the `Tuple` but I want to note that.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-55-incubator-flink@git.apache.org>##//##<20140702071143.C53BB992C69@tyr.zones.apache.org>#|#2014-07-02-07:12:05#|#rmetzger <git@git.apache.org>#|#[GitHub] incubator-flink pull request: providing accumulator example as pro...#|#
Github user rmetzger commented on the pull request:

    https://github.com/apache/incubator-flink/pull/55#issuecomment-47743880

    Thank you for the contribution.

    I have some minor comments regarding comment style / example style. In general, your code is very good!


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-55-incubator-flink@git.apache.org>##//##<20140702094009.5D786993098@tyr.zones.apache.org>#|#2014-07-02-09:40:31#|#fhueske <git@git.apache.org>#|#[GitHub] incubator-flink pull request: providing accumulator example as pro...#|#
Github user fhueske commented on a diff in the pull request:

    https://github.com/apache/incubator-flink/pull/55#discussion_r14448525

    --- Diff: stratosphere-examples/stratosphere-java-examples/src/main/java/eu/stratosphere/example/java/relational/FilterAndCountIncompleteLines.java ---
    @@ -0,0 +1,248 @@
    +/***********************************************************************************************************************
    + *
    + * Copyright (C) 2010-2013 by the Stratosphere project (http://stratosphere.eu)
    + *
    + * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
    + * the License. You may obtain a copy of the License at
    + *
    + *     http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    + * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    + * specific language governing permissions and limitations under the License.
    + *
    + **********************************************************************************************************************/
    +package eu.stratosphere.example.java.relational;
    +
    +import java.io.DataInput;
    +import java.io.DataOutput;
    +import java.io.IOException;
    +import java.util.ArrayList;
    +import java.util.Collection;
    +import java.util.List;
    +
    +import eu.stratosphere.api.common.JobExecutionResult;
    +import eu.stratosphere.api.common.accumulators.Accumulator;
    +import eu.stratosphere.api.java.DataSet;
    +import eu.stratosphere.api.java.ExecutionEnvironment;
    +import eu.stratosphere.api.java.functions.FilterFunction;
    +import eu.stratosphere.api.java.tuple.Tuple;
    +import eu.stratosphere.api.java.tuple.Tuple3;
    +import eu.stratosphere.configuration.Configuration;
    +
    +/**
    + * This program filters lines from a CSV file with empty fields. In doing so, it counts the number of empty fields per
    + * column within a CSV file using a custom accumulator for vectors. In this context, empty fields are those, that at
    + * most contain whitespace characters like space and tab.
    + * <p>
    + * The input file is a plain text CSV file with the semicolon as field separator and double quotes as field delimiters
    + * and three columns. See {@link #getDataSet(ExecutionEnvironment)} for configuration.
    + * <p>
    + * Usage: <code>FilterAndCountIncompleteLines [&lt;input file path&gt; [&lt;result path&gt;]]</code> <br>
    + * <p>
    + * This example shows how to use:
    + * <ul>
    + * <li>custom accumulators
    + * <li>tuple data types
    + * <li>inline-defined functions
    + * </ul>
    + */
    +@SuppressWarnings("serial")
    +public class FilterAndCountIncompleteLines {
    +
    +	// *************************************************************************
    +	// PROGRAM
    +	// *************************************************************************
    +
    +	private static final String EMPTY_FIELD_ACCUMULATOR = "empty-fields";
    +
    +	public static void main(final String[] args) throws Exception {
    +
    +		if (!parseParameters(args)) {
    +			return;
    +		}
    +
    +		final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
    +
    +		// get the data set
    +		final DataSet<Tuple> file = getDataSet(env);
    +
    +		// filter lines with empty fields
    +		final DataSet<Tuple> filteredLines = file.filter(new FilterFunction<Tuple>() {
    --- End diff --

    Would you mind not defining the FilterFunction inline.
    IMO, separating the data flow and the UDF logic is easier to read, esp. if the UDF implements multiple methods.


---
If your project is set up for it, you can reply to this email and have your
reply appear on GitHub as well. If your project does not have this feature
enabled and wishes so, or if the feature is enabled but not working, please
contact infrastructure at infrastructure@apache.org or file a JIRA ticket
with INFRA.
---
#|#<git-pr-55-incubator-flink@git.apache.org>##//##<20140808124956.29592mgabyqs5e8k@mail.uni-leipzig.de>#|#2014-08-08-16:37:26#|#Norman Spangenberg <norman.spangenberg@studserv.uni-leipzig.de>#|#Re: parse json-file with scala-api and json4s#|#
Hello Aljoscha,
Thanks for your reply. It was really helpful.
After some time to figure out the right syntax it worked perfectly.

val user_interest = lines.map( line => {
					val parsed = parse(line)
                          		implicit lazy val formats =
org.json4s.DefaultFormats
					val name = parsed.\("name").extract[String]
					val location_x = parsed.\("location").\("x").extract[Double]
					val location_y = parsed.\("location").\("y").extract[Double]
					val likes =
parsed.\("likes").extract[Seq[String]].map(_.toLowerCase()).mkString(";")
					( UserInterest(name, location_x, location_y, likes) )
                                     })

Is this the best way to handle with json data? or is there a more
efficient way?

thank you,
norman


----------------------------------------------------------------
This message was sent using IMP, the Internet Messaging Program.
#|#null##//##<20141103141504.A45262388D4B@eris.apache.org>#|#2014-11-03-14:15:26#|#Marvin <no-reply@apache.org>#|#Incubator PMC/Board report for Nov 2014 ([ppmc])#|#


Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator PMC.
It is an initial reminder to give you plenty of time to prepare your quarterly
board report.

The board meeting is scheduled for Wed, 19 November 2014, 10:30 am PST. The report
for your podling will form a part of the Incubator PMC report. The Incubator PMC
requires your report to be submitted 2 weeks before the board meeting, to allow
sufficient time for review and submission (Wed, Nov 5th).

Please submit your report with sufficient time to allow the incubator PMC, and
subsequently board members to review and digest. Again, the very latest you
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the project
   or necessarily of its field
 * A list of the three most important issues to address in the move towards
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware of
 * How has the community developed since the last report
 * How has the project developed since the last report.

This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/November2014

Note: This is manually populated. You may need to wait a little before this page
      is created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the
Incubator wiki page. Signing off reports shows that you are following the
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC
#|#null##//##<2479704B-B3E2-4E5C-B734-D944CDEA0B1D@icloud.com>#|#2015-01-24-13:39:42#|#Chiwan Park <chiwanpark@icloud.com>#|#Re: Adding non-core API features to Flink#|#
I think top level maven module called "flink-contrib" is reasonable. There are other projects having contrib package such as Akka, Django.

Regards, Chiwan Park (Sent with iPhone)

2015. 1. 24. =BF=C0=C8=C4 7:15 Fabian Hueske <fhueske@gmail.com> =C0=DB=BC=BA:

> Hi all,
>=20
> we got a few contribution requests lately to add cool but "non-core"
> features to our API.
> In previous discussions, concerns were raised to not bloat the APIs with
> too many "shortcut", "syntactic sugar", or special-case features.
>=20
> Instead we could setup a place to add Input/OutputFormats, common
> operations, etc. which does not need as much control as the core APIs. Open
> questions are:
> - How do we organize it? (top-level maven module, modules in flink-java,
> flink-scala, java packages in the API modules, ...)
> - How do we name it? flink-utils, flink-packages, ...
>=20
> Any opinions on this?
>=20
> Cheers, Fabian
#|#null##//##<2B1773BE-0745-425B-95B6-BA102D6CA043@apache.org>#|#2015-03-26-14:40:21#|#Ufuk Celebi <uce@apache.org>#|#Re: [DISCUSS] Make a release to be announced at ApacheCon#|#

On 26 Mar 2015, at 11:01, Robert Metzger <rmetzger@apache.org> wrote:

> Two weeks have passed since we've discussed the 0.9 release the last time.
>
> The ApacheCon is in 18 days from now.
> If we want, we can also release a "0.9.0-beta" release that contains known
> bugs, but allows our users to try out the new features easily (because they
> are part of a release). The vote for such a release would be mainly about
> the legal aspects of the release rather than the stability. So I suspect
> that the vote will go through much quicker.

+1 for 0.9-beta
#|#<CAGr9p8Bm+4JHqHgqD0+dLmimO_NMwNT8tiOrtvG_RyxMErU8gA@mail.gmail.com>##//##<331688967.1293534.1422488503127.JavaMail.yahoo@mail.yahoo.com>#|#2015-01-28-23:43:42#|#Ankit Jhalaria <ankit13@yahoo-inc.com.INVALID>#|#Re: MODERATE for dev@flink.apache.org#|#
------=_Part_1293533_607983567.1422488503120
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks Robert. I am working for the Ads & Data team at Yahoo and we were experimenting with flink. Without kerberos, we cannot talk to hdfs. Would you have a timeline as to when kerberos support will be added to flink?
Thanks,Ankit=20

     On Tuesday, January 27, 2015 2:49 AM, Robert Metzger <rmetzger@apache.org> wrote:
  =20

 Hey Ankit,
sorry for the issues with the mailing lists.Its our fault that we didn't properly mark the issues@ list as a read-only mailing list on our website. I've updated the website to make it more clear:=C2=A0http://flink.apache.org/community.html#mailing-lists
Regarding the problem you've reported: Are you using Flink with YARN or do you have it installed directly on the cluster?Either way, Flink doesn't have support for secure Hadoop environments right now.=C2=A0I'll make adding security support for Flink on YARN a top priority now.

Best,Robert


On Tue, Jan 27, 2015 at 6:43 AM, Henry Saputra <henry.saputra@gmail.com> wrote:

Ankit,

Please subscribe to dev@ list by sending email to
dev-subscribe@flink.apache.org [1]

[1] http://flink.apache.org/community.html#mailing-lists

On Mon, Jan 26, 2015 at 9:25 PM, Kostas Tzoumas <ktzoumas@apache.org> wrote:
> I am forwarding this as I could not approve it for some reason.
>
> Kostas
>
>
> ---------- Forwarded message ----------
> From: Ankit Jhalaria <ankit13@yahoo-inc.com.invalid>
> To: "dev@flink.apache.org" <dev@flink.apache.org>
> Cc:
> Date: Mon, 26 Jan 2015 22:59:53 +0000 (UTC)
> Subject: [Flink reading HDFS] : SIMPLE authentication is not enabled.
> Available:[TOKEN, KERBEROS]
> Hey guys,
>
> I am trying to setup a Flink cluster that can read from a Hadoop Cluster.
> We have a total of 8 machines on the Flink cluster and on all those
> machines can access HDFS on the Hadoop cluster.
>
> Flink Version : flink-0.9-SNAPSHOT
> Hadoop 2.5.0.8.1411070359
>
> When I try to run a program that reads from HDFS, I get the following error
> [Stack Trace shown below]. How do i enable flink to use kerberos
> authentication? Any clues would be appreciated. Tried sending emails
> earlier to *issues*@flink.apache.org. Got a bounce back email everytime.
>
> *Caused by:
> org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException):
> SIMPLE authentication is not enabled.=C2=A0 Available:[TOKEN, KERBEROS]*
> at org.apache.hadoop.ipc.Client.call(Client.java:1347)
> at org.apache.hadoop.ipc.Client.call(Client.java:1300)
> at
> org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
> at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:601)
> at
> org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
> at
> org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
> at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
> at
> org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
> at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
>
> Thanks,
> Ankit





------=_Part_1293533_607983567.1422488503120--
#|#<CAGr9p8AOop5UQW79q8UQqxEAQQTPGHakQTSSyU2jwt24LNy4yQ@mail.gmail.com>##//##<3ADDFC2F-4158-480C-9668-84424207A6E7@apache.org>#|#2015-02-11-10:29:40#|#Ufuk Celebi <uce@apache.org>#|#Re: [VOTE] Release Apache Flink 0.8.1 (RC0)#|#
-1

The problem I've mentioned occurs for every shutdown of the job/taskmanagers using the scripts (bin/stop-cluster.sh) and results in an exception at the end of the log files.


On 11 Feb 2015, at 10:59, Ufuk Celebi <uce@apache.org> wrote:

> There is a problem with a change added yesterday to master and release-0.8. See here: https://github.com/apache/flink/pull/376
>=20
> On 11 Feb 2015, at 01:10, Felix Neutatz <neutatz@googlemail.com> wrote:
>=20
>> +1
>>=20
>> + Built from source
>> + tested some Protobuf examples
>> + tested some Parquet examples
>>=20
>> Best regards,
>> Felix
>>=20
>>=20
>>=20
>> 2015-02-10 18:28 GMT+01:00 M=E1rton Balassi <balassi.marton@gmail.com>:
>>=20
>>> +1
>>>=20
>>> Built from source
>>> Run local examples
>>> Checked version numbers in the poms
>>> Validated check sums and signatures
>>>=20
>>> Minor cosmetic thing to note:
>>>=20
>>> The name of flink-parent (Apache Flink) in the maven build is still not
>>> nice and the inception year is set to 2015 (should be 2014). It seems that
>>> the commit fixing these only made to the master and not to release-0.8.
>>>=20
>>> Reactor Summary:
>>> [INFO]
>>> [INFO] Apache Flink ...................................... SUCCESS [3.917s]
>>> [INFO] flink-shaded ...................................... SUCCESS [3.135s]
>>> [INFO] flink-core ........................................ SUCCESS
>>> [22.577s]
>>> ...
>>>=20
>>> Best,
>>>=20
>>> Marton
>>>=20
>>>=20
>>> On Tue, Feb 10, 2015 at 5:31 PM, Robert Metzger <rmetzger@apache.org>
>>> wrote:
>>>=20
>>>> Please vote on releasing the following candidate as Apache Flink version
>>>> 0.8.1
>>>>=20
>>>> This is a bugfix release for 0.8.0.
>>>>=20
>>>> -------------------------------------------------------------
>>>> The commit to be voted on is in the branch "release-0.8.0-rc3"
>>>> (commit 70943ad5
>>>> <http://git-wip-us.apache.org/repos/asf/flink/commit/70943ad5>):
>>>> *http://git-wip-us.apache.org/repos/asf/flink/commit/70943ad5
>>>> <http://git-wip-us.apache.org/repos/asf/flink/commit/70943ad5>*
>>>>=20
>>>> The release artifacts to be voted on can be found at:
>>>> *http://people.apache.org/~rmetzger/flink-0.8.1-rc0/
>>>> <http://people.apache.org/~rmetzger/flink-0.8.1-rc0/>*
>>>>=20
>>>> Release artifacts are signed with the following key:
>>>> *https://people.apache.org/keys/committer/rmetzger.asc
>>>> <https://people.apache.org/keys/committer/rmetzger.asc>*
>>>>=20
>>>> The staging repository for this release can be found at:
>>>> *https://repository.apache.org/content/repositories/orgapacheflink-1029
>>>> <https://repository.apache.org/content/repositories/orgapacheflink-1029
>>>> *
>>>> -------------------------------------------------------------
>>>>=20
>>>>=20
>>>> Please vote on releasing this package as Apache Flink 0.8.1.
>>>>=20
>>>> The vote is open for the next 72 hours and passes if a majority of at
>>> least
>>>> three +1 PMC votes are cast.
>>>>=20
>>>> [ ] +1 Release this package as Apache Flink 0.8.1
>>>> [ ] -1 Do not release this package because ...
>>>>=20
>>>=20
>=20
#|#<6DF17DA0-DC40-4004-BA60-4FBBB94DE0C4@apache.org>##//##<3D8C11E6-0AC7-4C9C-96D6-490F8342315A@icloud.com>#|#2014-12-06-04:18:32#|#Chiwan Park <chiwanpark@icloud.com>#|#Re: [VOTE] Graduate Flink from the Incubator#|#
--Apple-Mail=_1B806068-50B6-4D75-9486-4CADB764D689
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

+1 :)

=E2=80=94
Chiwan Park (Sent with iPhone)



> On Dec 6, 2014, at 1:32 AM, Alan Gates <gates@hortonworks.com> wrote:
>=20
> +1.
>=20
> Alan.
>=20
>> 	Kostas Tzoumas <mailto:ktzoumas@apache.org>	December 5, 2014 at 2:46
>> Hi everyone,
>>=20
>> It seems that everyone is excited about graduating, so I am calling for a
>> community vote to graduate Apache Flink (incubating) to a top-level project
>> (TLP). If this VOTE proceeds successfully, I will start a similar VOTE
>> thread in the Incubator mailing list, and if that goes well, we will submit
>> the resolution below to be included in the agenda of the next Apache board
>> meeting. I will keep this vote open until Tuesday.
>>=20
>> [ ] +1 Graduate Apache Flink (incubating) according to the resolution below
>> [ ] +0 Don't care
>> [ ] -1 Don't graduate Apache Flink (incubating) according to the
>> resolution below because...
>>=20
>> I will close this VOTE on Tuesday, December 9, 12:00pm CET.
>>=20
>> Remember, this is a community vote, so everyone can vote!
>>=20
>> Best,
>> Kostas
>>=20
>> --------resolution:
>>=20
>> WHEREAS, the Board of Directors deems it to be in the best
>> interests of the Foundation and consistent with the
>> Foundation's purpose to establish a Project Management
>> Committee charged with the creation and maintenance of
>> open-source software, for distribution at no charge to
>> the public, related to fast and reliable large-scale data
>> analysis with focus on programmability, optimizability,
>> efficiency, and combination of batch and streaming data
>> processing.
>>=20
>> NOW, THEREFORE, BE IT RESOLVED, that a Project Management
>> Committee (PMC), to be known as the "Apache Flink Project",
>> be and hereby is established pursuant to Bylaws of the
>> Foundation; and be it further
>>=20
>> RESOLVED, that the Apache Flink Project be and hereby is
>> responsible for the creation and maintenance of software
>> related to fast and reliable large-scale data analysis
>> with focus on programmability, optimizability, efficiency,
>> and combination of batch and streaming data processing;
>> and be it further
>>=20
>> RESOLVED, that the office of "Vice President, Apache Flink" be
>> and hereby is created, the person holding such office to
>> serve at the direction of the Board of Directors as the chair
>> of the Apache Flink Project, and to have primary responsibility
>> for management of the projects within the scope of
>> responsibility of the Apache Flink Project; and be it further
>>=20
>> RESOLVED, that the persons listed immediately below be and
>> hereby are appointed to serve as the initial members of the
>> Apache Flink Project:
>>=20
>> * Marton Balassi <mbalassi@apache.org> <mailto:mbalassi@apache.org>
>> * Ufuk Celebi <uce@apache.org> <mailto:uce@apache.org>
>> * Stephan Ewen <sewen@apache.org> <mailto:sewen@apache.org>
>> * Gyula Fora <gyfora@apache.org> <mailto:gyfora@apache.org>
>> * Alan Gates <gates@apache.org> <mailto:gates@apache.org>
>> * Fabian Hueske <fhueske@apache.org> <mailto:fhueske@apache.org>
>> * Vasia Kalavri <vasia@apache.org> <mailto:vasia@apache.org>
>> * Aljoscha Krettek <aljoscha@apache.org> <mailto:aljoscha@apache.org>
>> * Robert Metzger <rmetzger@apache.org> <mailto:rmetzger@apache.org>
>> * Till Rohrmann <trohrmann@apache.org> <mailto:trohrmann@apache.org>
>> * Henry Saputra <hsaputra@apache.org> <mailto:hsaputra@apache.org>
>> * Sebastian Schelter <ssc@apache.org> <mailto:ssc@apache.org>
>> * Kostas Tzoumas <ktzoumas@apache.org> <mailto:ktzoumas@apache.org>
>> * Timo Walther <apache email to be set up>
>> * Daniel Warneke <warneke@apache.org> <mailto:warneke@apache.org>
>>=20
>> NOW, THEREFORE, BE IT FURTHER RESOLVED, that Stephan Ewen
>> be appointed to the office of Vice President, Apache Flink, to
>> serve in accordance with and subject to the direction of the
>> Board of Directors and the Bylaws of the Foundation until
>> death, resignation, retirement, removal or disqualification,
>> or until a successor is appointed; and be it further
>>=20
>> RESOLVED, that the initial Apache Flink PMC be and hereby is
>> tasked with the creation of a set of bylaws intended to
>> encourage open development and increased participation in the
>> Apache Flink Project; and be it further
>>=20
>> RESOLVED, that the Apache Flink Project be and hereby
>> is tasked with the migration and rationalization of the Apache
>> Incubator Flink podling; and be it further
>>=20
>> RESOLVED, that all responsibilities pertaining to the Apache
>> Incubator Flink podling encumbered upon the Apache Incubator
>> Project are hereafter discharged.
>>=20
>=20
> --=20
> Sent with Postbox <http://www.getpostbox.com/>
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.


--Apple-Mail=_1B806068-50B6-4D75-9486-4CADB764D689--
#|#<5481DE26.3020309@hortonworks.com>##//##<3F3505B1-8E56-40DA-B9CD-16B9724F1337@apache.org>#|#2015-06-04-21:16:50#|#Ufuk Celebi <uce@apache.org>#|#Re: [DISCUSS] Inconsistent naming of intermediate results#|#

On 04 Jun 2015, at 17:02, Maximilian Michels <mxm@apache.org> wrote:

> I think ResultPartition is a pretty accurate description of what it is: a
> partition of the result of an operator. ResultStream on the other hand,
> seems very generic to me. Just because we like to think of Flink nowadays
> as a "streaming data flow" engine, we don't have to change the core
> classes' names :)

Of course, we don't have to. ;-) But still, I think it makes sense for documentation and blog post purposes. The code is new, it's not like changing a old component, on which a lot of stuff depends AND we want to change the name anyways (see your comments in this thread).#|#<CAGco--bu0RXazai+XkM8eKdB1LebdEow4ms=YHT9F-fnkt++0A@mail.gmail.com>##//##<407FEE28-D917-48EF-B10D-516046E17370@kth.se>#|#2015-02-23-13:05:59#|#Paris Carbone <parisc@kth.se>#|#Re: [DISCUSS] Iterative streaming example#|#
--_000_407FEE28D91748EFB10D516046E17370kthse_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64

SGVsbG8gUGV0ZXIsDQoNClN0cmVhbWluZyBtYWNoaW5lIGxlYXJuaW5nIGFsZ29yaXRobXMgbWFr
ZSB1c2Ugb2YgaXRlcmF0aW9ucyBxdWl0ZSB3aWRlbHkuIE9uZSBzaW1wbGUgZXhhbXBsZSBpcyBp
bXBsZW1lbnRpbmcgZGlzdHJpYnV0ZWQgc3RyZWFtIGxlYXJuZXJzLiBUaGVyZSwgaW4gbWFueSBj
YXNlcyB5b3UgbmVlZCBzb21lIGNlbnRyYWwgbW9kZWwgYWdncmVnYXRvciwgZGlzdHJpYnV0ZWQg
ZXN0aW1hdG9ycyB0byBvZmZsb2FkIHRoZSBjZW50cmFsIG5vZGUgYW5kIG9mIGNvdXJzZSBmZWVk
YmFjayBsb29wcyB0byBtZXJnZSBldmVyeXRoaW5nIGJhY2sgdG8gdGhlIG1haW4gYWdncmVnYXRv
ciBwZXJpb2RpY2FsbHkuIE9uZSBzdWNoIGV4YW1wbGUgaW4gdGhlIFZlcnRpY2FsIEhvZWZmZGlu
ZyBUcmVlIENsYXNzaWZpZXIgKFZGRFQpIFsxXSB0aGF0IGlzIGltcGxlbWVudGVkIGluIFNhbW9h
Lg0KDQpJdGVyYXRpdmUgc3RyZWFtcyBhcmUgYWxzbyB1c2VmdWwgZm9yIG9wdGltaXNhdGlvbiB0
ZWNobmlxdWVzIGFzIGluIGJhdGNoIHByb2Nlc3NpbmcgKGVnLiB0cnlpbmcgZGlmZmVyZW50IHBh
cmFtZXRlcnMgdG8gZXN0aW1hdGUgYSB2YXJpYWJsZSwgZ2V0dGluZyBiYWNrIHRoZSBhY2N1cmFj
eSBmcm9tIGFuIGV2YWx1YXRvciBhbmQgcmVwZWF0aW5nIHVudGlsIGEgY29uZGl0aW9uIGlzIGFj
aGlldmVkKS4NCg0KSSBob3BlIHRoaXMgaGVscHMgdG8gZ2V0IGEgZ2VuZXJhbCBpZGVhIG9mIHdo
ZXJlIGl0ZXJhdGlvbnMgY2FuIGJlIHVzZWQuDQoNClsxXSBodHRwczovL2dpdGh1Yi5jb20veWFo
b28vc2Ftb2Evd2lraS9WZXJ0aWNhbC1Ib2VmZmRpbmctVHJlZS1DbGFzc2lmaWVyDQoNCg0KT24g
MjMgRmViIDIwMTUsIGF0IDEyOjEzLCBTdGVwaGFuIEV3ZW4gPHNld2VuQGFwYWNoZS5vcmc8bWFp
bHRvOnNld2VuQGFwYWNoZS5vcmc+PiB3cm90ZToNCg0KSSB0aGluayB0aGF0IHRoZSBTYW1vYSBw
ZW9wbGUgaGF2ZSBxdWl0ZSBhIGZldyBuaWNlIGV4YW1wbGVzIGFsb25nIHRoZQ0KbGluZXMgb2Yg
bW9kZWwgdHJhaW5pbmcgd2l0aCBmZWVkYmFjay4NCg0KQFBhcmlzOiBXaGF0IHdvdWxkIGJlIHRo
ZSBzaW1wbGVzdCBleGFtcGxlPw0KDQpPbiBNb24sIEZlYiAyMywgMjAxNSBhdCAxMToyNyBBTSwg
U3phYsOzIFDDqXRlciA8bmVtZGVyb2dhdG9yaXVzQGdtYWlsLmNvbTxtYWlsdG86bmVtZGVyb2dh
dG9yaXVzQGdtYWlsLmNvbT4+DQp3cm90ZToNCg0KRG9lcyBldmVyeW9uZSBrbm93IG9mIGEgZ29v
ZCwgc2ltcGxlIGFuZCByZWFsaXN0aWMgc3RyZWFtaW5nIGl0ZXJhdGlvbg0KZXhhbXBsZT8gVGhl
IGN1cnJlbnQgZXhhbXBsZSB0ZXN0cyBhIHJhbmRvbSBnZW5lcmF0b3IsIGJ1dCBpdCBzaG91bGQg
YmUNCnJlcGxhY2VkIGJ5IHNvbWV0aGluZyBkZXRlcm1pbmlzdGljIGluIG9yZGVyIHRvIGJlIHRl
c3RhYmxlLg0KDQpQZXRlcg0KDQoNCg=
--_000_407FEE28D91748EFB10D516046E17370kthse_--
#|#<CANC1h_v5EidDE9nntz1rBysnG9Zq4rZjp4f0JA27JOuyZ=OuEQ@mail.gmail.com>##//##<4219027F6234F7418DA53D03FFA0BC6A01754C50@SUSHDC8002.TD.TERADATA.COM>#|#2015-04-20-14:24:54#|#"Papp, Stefan" <Stefan.Papp@Teradata.com>#|#RE: Hadoop ETLing with Flink#|#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#|#<CAGr9p8CNA3suwDCu8H0nY_dcEOMuUrSmXXgBzhuCmCwir3=fmQ@mail.gmail.com>##//##<44B1AB07-F993-436F-AE23-8CC4CCC08A54@tu-berlin.de>#|#2014-10-31-17:51:01#|#"Rosenfeld, Viktor" <viktor.rosenfeld@tu-berlin.de>#|#Hi / Aggregation support#|#
--Apple-Mail=_16E63A9C-6B05-4B16-B9A3-96A572591F5D
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Hi everybody,

First, I want to introduce myself to the community. I am a PhD student who wants to work with and improve Flink.

Second, I thought to work on improving aggregations as a start. My first goal is to simplify the computaton of a field average. Basically, I want to turn this plan:

    val input =3D env.fromCollection( Array(1L, 2L, 3L, 4L) )

    input
    .map { in =3D> (in, 1L) }
    .sum(0).andSum(1)
    .map { in =3D> in._1.toDouble / in._2.toDouble }
    .print

into this:

    // val input =3D ...
    input.average(0).print()

My basic idea is to internally still add the counter field and execute the map and sum steps but to hide them from the user.

Next, I want to support multiple aggregations so one can write something like:

    input.min(0).max(0).sum(0).count(0).average(0)

Internally, there should only be one pass over the input data and average should reuse the work done by sum and count.

In September there was some discussion [1] on the semantics of the min/max aggregations vs. minBy/maxBy. The consensus was that min/max should not simply return the respective field value but return the entire tuple. However, for count/sum/average there is no specific tuple and it would also not work for combinations of min/max.

One possible route is to simply return a random element, similar to MySQL. I think this can be very surprising to the user especially when min/max are combined.

Another possibility is to return the tuple only for single invocations of min or max and return the field value for the other aggregation functions or combinations. This is also inconstent but appears to be more inline with people's expectation. Also, there might be two or more tuples with the same min/max value and then the question is which should be returned.

I haven't yet thought about aggregations in a streaming context and I would appreciate any input on this.

Best,
Viktor

[1] http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Aggregations-td1706.html


--Apple-Mail=_16E63A9C-6B05-4B16-B9A3-96A572591F5D
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="signature.asc"
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----

iD8DBQFUU8vRkWI06CMxQ0ARAk0FAJ0SoU7DiVMYvFMHSqjrggK6fP4iYwCfVQAm
kNUjN+4HgFlFGaeOY+gcPVg=HUCX
-----END PGP SIGNATURE-----

--Apple-Mail=_16E63A9C-6B05-4B16-B9A3-96A572591F5D--
#|#null##//##<50561610-C8B1-4C69-A00C-60E89BD84F08@fu-berlin.de>#|#2014-08-08-07:38:57#|#Ufuk Celebi <u.celebi@fu-berlin.de>#|#Re: [VOTE] Release Apache Flink 0.6 (incubating) (RC2)#|#

On 08 Aug 2014, at 09:36, Robert Metzger <rmetzger@apache.org> wrote:

> I think we should create another release candidate since Stephan provided a
> fix for a crucial bug here:
> https://github.com/apache/incubator-flink/pull/91. I'm going to test his
> changes and see if it fixes the error I reported.

I will also review the PR.

> Also, we need to disable the POJO serialization, as it seems broken right
> now.

+1

>
> There is one additional issue I would like to discuss here: With this
> release candidate, I've named the artifacts
> "apache-flink-0.6-incubating-rc2-src.tgz", my first RC had just
> "flink-0.6-incubating..", without the "apache-" prefix.
>
> Different Apache projects handle this differently, I prefer to include the
> "apache" name into the files. Our name is short enough to do that and it
> has a positive branding effect.
> What do you think?

+1
#|#<CAGr9p8A_LkUhgfuSH1HPy8tmaq4yaEtAOhJVnE-eGoMmYWiTww@mail.gmail.com>##//##<513C155C-4BBE-46FF-B64A-C1A757CB5CB4@icloud.com>#|#2014-12-09-05:24:15#|#Chiwan Park <chiwanpark@icloud.com>#|#Re: Setting up IRC channel to promote discussions and getting help#|#
--Apple-Mail=_082D7354-2133-4197-8588-5FCFAC71FD14
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

+1
Nice Idea!

=E2=80=94
Chiwan Park (Sent with iPhone)


> On Dec 9, 2014, at 8:40 AM, Timo Walther <flink@twalthr.com> wrote:
>=20
> +1
> Great idea!
>=20
> On 08.12.2014 23:43, Kostas Tzoumas wrote:
>> +1
>>=20
>> On Mon, Dec 8, 2014 at 11:26 PM, Paris Carbone <parisc@kth.se> wrote:
>>=20
>>> +1
>>> It will certainly bring all teams closer
>>>=20
>>>> On 08 Dec 2014, at 23:17, Fabian Hueske <fhueske@apache.org> wrote:
>>>>=20
>>>> +1
>>>>=20
>>>> 2014-12-08 23:05 GMT+01:00 Ufuk Celebi <uce@apache.org>:
>>>>=20
>>>>> +1 great idea
>>>>>=20
>>>>> On Monday, December 8, 2014, Gyula Fora <gyula.fora@gmail.com> wrote:
>>>>>=20
>>>>>> +1
>>>>>>=20
>>>>>> I think it=E2=80=99s a good idea!
>>>>>>=20
>>>>>> Gyula
>>>>>>> On 08 Dec 2014, at 22:45, Henry Saputra <henry.saputra@gmail.com
>>>>>> <javascript:;>> wrote:
>>>>>>> HI All,
>>>>>>>=20
>>>>>>> Sorry for cross posting.
>>>>>>>=20
>>>>>>> I thinking about setting up ASF official IRC channel for Apache Flink.
>>>>>>>=20
>>>>>>> Some of us could just hang out and provide answer or just ask for
>>>>>>> question about Flink casually.
>>>>>>>=20
>>>>>>> The channel could also be used for some "official" discussions which
>>>>>>> can be recorded and shared in the dev@ list.
>>>>>>>=20
>>>>>>> If there are some interests about it I could start working on it.
>>>>>>>=20
>>>>>>> I found this is useful in other projects such as Bigtop , Aurora, and
>>>>>> Tachyon
>>>>>>> - Henry
>>>>>>=20
>>>=20
>=20


--Apple-Mail=_082D7354-2133-4197-8588-5FCFAC71FD14--
#|#<54863706.7010302@twalthr.com>##//##<53980225.2030202@apache.org>#|#2014-06-11-07:16:19#|#Sebastian Schelter <ssc@apache.org>#|#Re: Non-Apache bug-fixing release 0.5.1#|#
Sounds reasonable to me, especially given the fact that the namesearch
w.r.t flink is not completed yet.

--sebastian


On 06/11/2014 09:01 AM, Tzoumas, Kostas wrote:
> Hi everyone,
>
> As mentioned in https://issues.apache.org/jira/browse/FLINK-886, a minor release that contains only bug fixes is planned to be released outside Apache Incubator.
>
> The rationale is that there have been requests for that by users, and an Apache release will break current applications due to renaming. The first Apache release will then be 0.6 and all releases moving forward will be inside Apache Incubator.
>
> I think this is a reasonable thing to do.
>
> @Mentors: is that OK in your opinion?
>
> Best,
> Kostas
>
#|#<B41F6323-6356-44CE-8106-9067CA73F3B2@tu-berlin.de>##//##<53A050DA.8020902@twalthr.com>#|#2014-06-17-14:30:15#|#Timo Walther <flink@twalthr.com>#|#Looks like a ClassLoader bug#|#
--------------060003000308040305020207
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit

Hey everyone,

I'm still trying to finish the Hadoop Compatibility PR
(https://github.com/stratosphere/stratosphere/pull/777), however, I
always get an ClassNotFoundException for my HCatalog InputFormat on the
cluster.

While searching for potential ClassLoader bugs, I found the following
lines in UserCodeClassWrapper:

@Override
     public T getUserCodeObject(Class<? super T> superClass, ClassLoader
cl) {
         return InstantiationUtil.instantiate(userCodeClass, superClass);
     }

Why is the given "ClassLoader cl" argument never used? Looks for me like
a bug... What do you think?

Thanks,
Timo

--------------060003000308040305020207--
#|#null##//##<53A3D9FA.8040407@apache.org>#|#2014-06-20-06:52:06#|#Sebastian Schelter <ssc@apache.org>#|#Re: Java Primitive Collections in Flink#|#
+1 for fastutils


On 06/20/2014 08:50 AM, Robert Metzger wrote:
> Hi Robert,
>
> The Apache Commons Primitives Collection project seems to be pretty
> inactive. The last release was in 2003, there are many dead links on the
> website. I would not suggest to use it.
> HPPC and fastutil seem pretty similar to me. Both have a somewhat active
> mailing list and up-to-date releases. Apache Giraph is using fastutil.
> In my opinion, its up to you to decide what you want to use.
>
>
>
>
> On Thu, Jun 19, 2014 at 2:53 PM, Robert Waury <robert.waury@googlemail.com>
> wrote:
>
>> Hi,
>>
>> I'm currently working on some code in Flink's runtime and want to use some
>> Java Primitive Collections to improve performance.
>>
>> As fas as I can see no Primitive Collections library is in the dependencies
>> so I wanted to ask if anybody has any preferences or input on which library
>> the project should use.
>>
>> The viable candidates (from a licensing perspective at least) are:
>>
>> - Apache Commons Primitives Collections
>> - High Performance Primitives Collections for Java (HPPC)
>> - fastutil
>>
>> They are all APL 2.0 licensed.
>>
>> Since I'm probably not the only one who is going to use Primitive
>> Collections I don't want to introduce a new dependency without any
>> discussion.
>>
>> Cheers,
>> Robert
>>
>
#|#<CAGr9p8AA22vb-eRv4Z50pkj3SZExmuiymbh63X7K7jGv8fVA0w@mail.gmail.com>##//##<53B9314A.1080805@apache.org>#|#2014-07-06-11:22:16#|#Daniel Warneke <warneke@apache.org>#|#Future of the current runtime?#|#
Hi guys,

now that there are ongoing efforts to port Flink to Tez, what is the
future of the current runtime? In particular, there are quite a few Jira
tickets related to bugs or improvements of the current runtime. Some
call for major changes and require considerable work.

Does it still make sense to work on these tickets?

Best regards,

    Daniel
#|#null##//##<53E9D138.9040805@studserv.uni-leipzig.de>#|#2014-08-12-08:33:29#|#Norman Spangenberg <wir12kqe@studserv.uni-leipzig.de>#|#Re: parse json-file with scala-api and json4s#|#
Hello Aljoscha and Robert,
Sorry for that stupid question. Building a fat JAR with maven worked for
me. Thank you.
actually I tried to copy the json4s-JARsto the lib folders of the
cluster. But that didn't work.
In Yarn-Cluster-Mode: where is the right directory to put that JARs? Is
it hadoop-VERSION/share/hadoop/common/lib/ ?

kind regards
norman
#|#null##//##<53F2243C.4040803@hortonworks.com>#|#2014-08-18-16:05:52#|#Alan Gates <gates@hortonworks.com>#|#Re: Question on providing CDH packages#|#
--------------020701090503000709090107
Content-Type: multipart/alternative;
 boundary="------------040506080401040701000302"

This is a multi-part message in MIME format.
--------------040506080401040701000302
Content-Type: text/plain; charset=UTF-8; format=flowed

My concern with this is it appears to put Apache in the business of
picking the right Hadoop vendors.  What about IBM, Pivotal, etc.?  I get
that the actual desire here is to make things easy for users, and that
the original three packages offered (Hadoop1, CDH4, Hadoop2) will cover
95% of users.  I like that.  I just don't know how to do this and avoid
the appearance of favoritism.

Perhaps the next best step is to ask on incubator-general and see if
there is an Apache wide policy or if there needs to be one.

Alan.

> Robert Metzger <mailto:rmetzger@apache.org>
> August 18, 2014 at 6:54
> Hi,
>
> I think we all agree that our project benefits from providing pre-compiled
> binaries for different hadoop distributions.
>
> I've drafted an extension of the current download page, that I would
> suggest to use after the release: http://i.imgur.com/MucW2HD.png
> As you can see, users can directly pick the Flink version they want (its
> not going to show the CDH4 package there) or they can choose from the
> table
> with the most popular (in my opinion) vendor distributions.
> The different links still point to the "hadoop1", "hadoop2" binaries,
> but I
> don't think this is highlighting any hadoop vendors.
>
> What do you think?
>
>
> On Fri, Aug 15, 2014 at 11:45 PM, Henry Saputra <henry.saputra@gmail.com>
>
> Henry Saputra <mailto:henry.saputra@gmail.com>
> August 15, 2014 at 14:45
> Ah sorry Alan, did not see your reply to Owen.
>
> Mea culpa from me.
>
> - Henry
>
>
>
> Alan Gates <mailto:gates@hortonworks.com>
> August 15, 2014 at 14:15
> Sorry, apparently this was unclear, as others asked the same
> question.  Flink hasn't had any Apache releases yet.  I was referring
> to the proposed release that Robert sent out,
> http://people.apache.org/~rmetzger/flink-0.6-incubating-rc7/
>
> Alan.
>
>
> Sean Owen <mailto:srowen@gmail.com>
> August 15, 2014 at 11:26
> PS, sorry for being dense, but I don't see vendor packages at
> http://flink.incubator.apache.org/downloads.html ?
>
> Is it this page?
> http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/building.html
>
> That's more benign, just helping people rebuild for certain distros if
> desired. Can the example be generified to refer to a fictional "ACME
> Distribution"? But a note here and there about gotchas building for
> certain versions and combos seems reasonable.
>
> I also find this bit in the build script, although vendor-specific, is
> a small nice convenience for users:
> https://github.com/apache/incubator-flink/blob/master/pom.xml#L195
> Owen O'Malley <mailto:omalley@apache.org>
> August 15, 2014 at 11:01
> As a mentor, I agree that vendor specific packages aren't appropriate for
> the Apache site. (Disclosure: I work at Hortonworks.) Working with the
> vendors to make packages available is great, but they shouldn't be hosted
> at Apache.
>
> .. Owen
>
>
>

--
Sent with Postbox <http://www.getpostbox.com>

--
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to
which it is addressed and may contain information that is confidential,
privileged and exempt from disclosure under applicable law. If the reader
of this message is not the intended recipient, you are hereby notified that
any printing, copying, dissemination, distribution, disclosure or
forwarding of this communication is strictly prohibited. If you have
received this communication in error, please contact the sender immediately
and delete it from your system. Thank You.

--------------040506080401040701000302
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<html><head>
<meta content=3D"text/html; charset=3DUTF-8" http-equiv=3D"Content-Type">
</head><body bgcolor=3D"#FFFFFF" text=3D"#000000">My concern with this is it
 appears to put Apache in the business of picking the right Hadoop=20
vendors.=C2=A0 What about IBM, Pivotal, etc.?=C2=A0 I get that the actual desire=20
here is to make things easy for users, and that the original three=20
packages offered (Hadoop1, CDH4, Hadoop2) will cover 95% of users.=C2=A0 I=20
like that.=C2=A0 I just don't know how to do this and avoid the appearance of
 favoritism.<br>
<br>
Perhaps the next best step is to ask on incubator-general and see if=20
there is an Apache wide policy or if there needs to be one.<br>
<br>
Alan.<br>
<br>
<blockquote style=3D"border: 0px none;"=20
cite=3D"mid:CAGr9p8BY_BSCZDskCvFOxS+pG=3DJqBJdSuSjhb88Lbri0LTv92g@mail.gmail.com"
 type=3D"cite">
  <div style=3D"margin:30px 25px 10px 25px;" class=3D"__pbConvHr"><div=20
style=3D"display:table;width:100%;border-top:1px solid=20
#EDEEF0;padding-top:5px"> 	<div=20
style=3D"display:table-cell;vertical-align:middle;padding-right:6px;"><img
 photoaddress=3D"rmetzger@apache.org" photoname=3D"Robert Metzger"=20
src=3D"cid:part1.02030407.02080201@hortonworks.com"=20
name=3D"compose-unknown-contact.jpg" height=3D"25px" width=3D"25px"></div>   <div
=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;width:100%">
   	<a moz-do-not-send=3D"true" href=3D"mailto:rmetzger@apache.org"=20
style=3D"color:#737F92=20
!important;padding-right:6px;font-weight:bold;text-decoration:none=20
!important;">Robert Metzger</a></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;">  =20
  <font color=3D"#9FA2A5"><span style=3D"padding-left:6px">August 18, 2014=20
at 6:54</span></font></div></div></div>
  <div style=3D"color:#888888;margin-left:24px;margin-right:24px;"=20
__pbrmquotes=3D"true" class=3D"__pbConvBody"><div>Hi,<br><br>I think we all=20
agree that our project benefits from providing pre-compiled<br>binaries=20
for different hadoop distributions.<br><br>I've drafted an extension of=20
the current download page, that I would<br>suggest to use after the=20
release: <a class=3D"moz-txt-link-freetext" href=3D"http://i.imgur.com/MucW2HD.png">http://i.imgur.com/MucW2HD.png</a><br>As you can see, users can=20
directly pick the Flink version they want (its<br>not going to show the=20
CDH4 package there) or they can choose from the table<br>with the most=20
popular (in my opinion) vendor distributions.<br>The different links=20
still point to the "hadoop1", "hadoop2" binaries, but I<br>don't think=20
this is highlighting any hadoop vendors.<br><br>What do you think?<br><br><br>On
 Fri, Aug 15, 2014 at 11:45 PM, Henry Saputra=20
<a class=3D"moz-txt-link-rfc2396E" href=3D"mailto:henry.saputra@gmail.com">&lt;henry.saputra@gmail.com&gt;</a><br></div><div><!----><br></div></div>
  <div style=3D"margin:30px 25px 10px 25px;" class=3D"__pbConvHr"><div=20
style=3D"display:table;width:100%;border-top:1px solid=20
#EDEEF0;padding-top:5px"> 	<div=20
style=3D"display:table-cell;vertical-align:middle;padding-right:6px;"><img
 photoaddress=3D"henry.saputra@gmail.com" photoname=3D"Henry Saputra"=20
src=3D"cid:part2.07010203.07080302@hortonworks.com"=20
name=3D"postbox-contact.jpg" height=3D"25px" width=3D"25px"></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;width:100%">
   	<a moz-do-not-send=3D"true" href=3D"mailto:henry.saputra@gmail.com"=20
style=3D"color:#737F92=20
!important;padding-right:6px;font-weight:bold;text-decoration:none=20
!important;">Henry Saputra</a></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;">  =20
  <font color=3D"#9FA2A5"><span style=3D"padding-left:6px">August 15, 2014=20
at 14:45</span></font></div></div></div>
  <div style=3D"color:#888888;margin-left:24px;margin-right:24px;"=20
__pbrmquotes=3D"true" class=3D"__pbConvBody"><div>Ah sorry Alan, did not see
 your reply to Owen.<br><br>Mea culpa from me.<br><br>- Henry<br><br><br></div><div><!----><br></div></div>
  <div style=3D"margin:30px 25px 10px 25px;" class=3D"__pbConvHr"><div=20
style=3D"display:table;width:100%;border-top:1px solid=20
#EDEEF0;padding-top:5px"> 	<div=20
style=3D"display:table-cell;vertical-align:middle;padding-right:6px;"><img
 photoaddress=3D"gates@hortonworks.com" photoname=3D"Alan Gates"=20
src=3D"cid:part3.08010800.04000203@hortonworks.com"=20
name=3D"postbox-contact.jpg" height=3D"25px" width=3D"25px"></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;width:100%">
   	<a moz-do-not-send=3D"true" href=3D"mailto:gates@hortonworks.com"=20
style=3D"color:#737F92=20
!important;padding-right:6px;font-weight:bold;text-decoration:none=20
!important;">Alan Gates</a></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;">  =20
  <font color=3D"#9FA2A5"><span style=3D"padding-left:6px">August 15, 2014=20
at 14:15</span></font></div></div></div>
  <div style=3D"color:#888888;margin-left:24px;margin-right:24px;"=20
__pbrmquotes=3D"true" class=3D"__pbConvBody">
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3DUTF-8">
Sorry, apparently this was
 unclear, as others asked the same question.=C2=A0 Flink hasn't had any=20
Apache releases yet.=C2=A0 I was referring to the proposed release that=20
Robert sent out,=20
<a moz-do-not-send=3D"true"=20
href=3D"http://people.apache.org/%7Ermetzger/flink-0.6-incubating-rc7/"=20
class=3D"moz-txt-link-freetext">http://people.apache.org/~rmetzger/flink-0.6-incubating-rc7/</a><br>
<br>
Alan.<br>
<br>

<br>

  </div>
  <div style=3D"margin:30px 25px 10px 25px;" class=3D"__pbConvHr"><div=20
style=3D"display:table;width:100%;border-top:1px solid=20
#EDEEF0;padding-top:5px"> 	<div=20
style=3D"display:table-cell;vertical-align:middle;padding-right:6px;"><img
 photoaddress=3D"srowen@gmail.com" photoname=3D"Sean Owen"=20
src=3D"cid:part4.03040704.09000706@hortonworks.com"=20
name=3D"postbox-contact.jpg" height=3D"25px" width=3D"25px"></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;width:100%">
   	<a moz-do-not-send=3D"true" href=3D"mailto:srowen@gmail.com"=20
style=3D"color:#737F92=20
!important;padding-right:6px;font-weight:bold;text-decoration:none=20
!important;">Sean Owen</a></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;">  =20
  <font color=3D"#9FA2A5"><span style=3D"padding-left:6px">August 15, 2014=20
at 11:26</span></font></div></div></div>
  <div style=3D"color:#888888;margin-left:24px;margin-right:24px;"=20
__pbrmquotes=3D"true" class=3D"__pbConvBody"><div>PS, sorry for being dense,
 but I don't see vendor packages at<br><a class=3D"moz-txt-link-freetext" href=3D"http://flink.incubator.apache.org/downloads.html">http://flink.incubator.apache.org/downloads.html</a>
 ?<br><br>Is it this page?<br><a class=3D"moz-txt-link-freetext" href=3D"http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/building.html">http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/building.html</a><br><br>That's
 more benign, just helping people rebuild for certain distros if<br>desired.
 Can the example be generified to refer to a fictional "ACME<br>Distribution"?
 But a note here and there about gotchas building for<br>certain=20
versions and combos seems reasonable.<br><br>I also find this bit in the
 build script, although vendor-specific, is<br>a small nice convenience=20
for users:<br><a class=3D"moz-txt-link-freetext" href=3D"https://github.com/apache/incubator-flink/blob/master/pom.xml#L195">https://github.com/apache/incubator-flink/blob/master/pom.xml#L195</a><br></div></div>
  <div style=3D"margin:30px 25px 10px 25px;" class=3D"__pbConvHr"><div=20
style=3D"display:table;width:100%;border-top:1px solid=20
#EDEEF0;padding-top:5px"> 	<div=20
style=3D"display:table-cell;vertical-align:middle;padding-right:6px;"><img
 photoaddress=3D"omalley@apache.org" photoname=3D"Owen O'Malley"=20
src=3D"cid:part5.02010406.07050909@hortonworks.com"=20
name=3D"postbox-contact.jpg" height=3D"25px" width=3D"25px"></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;width:100%">
   	<a moz-do-not-send=3D"true" href=3D"mailto:omalley@apache.org"=20
style=3D"color:#737F92=20
!important;padding-right:6px;font-weight:bold;text-decoration:none=20
!important;">Owen O'Malley</a></div>   <div=20
style=3D"display:table-cell;white-space:nowrap;vertical-align:middle;">  =20
  <font color=3D"#9FA2A5"><span style=3D"padding-left:6px">August 15, 2014=20
at 11:01</span></font></div></div></div>
  <div style=3D"color:#888888;margin-left:24px;margin-right:24px;"=20
__pbrmquotes=3D"true" class=3D"__pbConvBody"><div>As a mentor, I agree that=20
vendor specific packages aren't appropriate for<br>the Apache site.=20
(Disclosure: I work at Hortonworks.) Working with the<br>vendors to make
 packages available is great, but they shouldn't be hosted<br>at Apache.<br><br>..
 Owen<br><br><br></div><div><!----><br></div></div>
</blockquote>
<br>
<div class=3D"moz-signature">-- <br>
<div>Sent with <a href=3D"http://www.getpostbox.com"><span style=3D"color:=20
rgb(51, 102, 153);">Postbox</span></a></div></div>
</body></html>

<br>
<span style=3D"color:rgb(128,128,128);font-family:Arial,sans-serif;font-size:10px">CONFIDENTIALITY NOTICE</span><br style=3D"color:rgb(128,128,128);font-family:Arial,sans-serif;font-size:10px"><span style=3D"color:rgb(128,128,128);font-family:Arial,sans-serif;font-size:10px">NOTICE: This message is intended for the use of the individual or entity to which it is addressed and may contain information that is confidential, privileged and exempt from disclosure under applicable law. If the reader of this message is not the intended recipient, you are hereby notified that any printing, copying, dissemination, distribution, disclosure or forwarding of this communication is strictly prohibited. If you have received this communication in error, please contact the sender immediately and delete it from your system. Thank You.</span>
--------------040506080401040701000302--
--------------020701090503000709090107--
#|#<CAGr9p8BY_BSCZDskCvFOxS+pG=JqBJdSuSjhb88Lbri0LTv92g@mail.gmail.com>##//##<53FE645D.6010304@fu-berlin.de>#|#2014-08-27-23:06:34#|#Chesnay Schepler <chesnay.schepler@fu-berlin.de>#|#Re: Python API - Weird Performance Issue#|#
the performance differences occur on the same system (16GB, 4 cores +
HyperThreading) with a DOP of 1 for a plan consisting of a single
operator. plenty of resources :/

On 28.8.2014 0:50, Stephan Ewen wrote:
> Hey Chesnay!
>
> Here are some thoughts:
>
>   - The repeated checking for 1 or 0 is indeed a busy loop. These may behave
> very different in different settings. If you run the code isolated, you
> have a spare core for the thread and it barely hurts. Run multiple parallel
> instances in a larger framework, and it eats away CPU cycles from the
> threads that do the work - it starts hurting badly.
>
>   - You may get around a copy into the shared memory (ByteBuffer into
> MemoryMappedFile) by creating an according DataOutputView - save one more
> data copy. That's the next step, though, first solve the other issue.
>
> The last time I implemented such an inter-process data pipe between
> languages, I had a similar issue: No support for system wide semaphores (or
> something similar) on both sides.
>
> I used Shared memory for the buffers, and a local network socket (UDP, but
> I guess TCP would be fine as well) for notifications when buffers are
> available. That worked pretty well, yielded high throughput, because the
> big buffers were not copied (unlike in streams), and the UDP notifications
> were very fast (fire and forget datagrams).
>
> Stephan
>
>
>
> On Wed, Aug 27, 2014 at 10:48 PM, Chesnay Schepler <
> chesnay.schepler@fu-berlin.de> wrote:
>
>> Hey Stephan,
>>
>> I'd like to point out right away that the code related to your questions
>> is shared by both programs.
>>
>> regarding your first point: i have a byte[] into which i serialize the
>> data first using a ByteBuffer, and then write that data to a
>> MappedByteBuffer.
>>
>> regarding synchronization: i couldn't find a way to use elaborate things
>> like semaphores or similar that work between python and java alike.
>>
>> the data exchange is currently completely synchronous. java writes a
>> record, sets an "isWritten" bit and then repeatedly checks this bit whether
>> it is 0. python repeatedly checks this bit whether it is 1. once that
>> happens, it reads the record, sets the bit to 0 which tells java that it
>> has read the record and can write the next one. this scheme works the same
>> way the other way around.
>>
>> *NOW,* this may seem ... inefficient, to put it slightly. it is (or rather
>> should be...) way faster (5x) that what we had so far though (asynchronous
>> pipes).
>> (i also tried different schemes that all had no effect, so i decided to
>> stick with the easiest one)
>>
>> on to your last point: I'm gonna check for that tomorrow.
>>
>>
>>
>>
>> On 27.8.2014 20:45, Stephan Ewen wrote:
>>
>>> Hi Chesnay!
>>>
>>> That is an interesting problem, though hard to judge with the information
>>> we have.
>>>
>>> Can you elaborate a bit on the following points:
>>>
>>>    - When putting the objects from the Java Flink side into the shared
>>> memory, you need to serialize them. How do you do that? Into a buffer,
>>> then
>>> copy that into the shared memory ByteBuffer? Directly?
>>>
>>>    - Shared memory access has to be somehow controlled. The pipes give you
>>> flow control for free (blocking write calls when the stream consumer is
>>> busy). What do you do for the shared memory? Usually, one uses semaphores,
>>> or, in java File(Range)Locks to coordinate access and block until memory
>>> regions are made available. Can you check if there are some busy waiting
>>> parts in you code?
>>>
>>>    - More general: The code is slower, but does it burn CPU cycles in its
>>> slowness or is it waiting for locks / monitors / conditions ?
>>>
>>> Stephan
>>>
>>>
>>>
>>> On Wed, Aug 27, 2014 at 8:34 PM, Chesnay Schepler <
>>> chesnay.schepler@fu-berlin.de> wrote:
>>>
>>>   Hello everyone,
>>>> This will be some kind of brainstorming question.
>>>>
>>>> As some of you may know I am currently working on the Python API. The
>>>> most
>>>> crucial part here is how the data is exchanged between Java and Python.
>>>> Up to this point we used pipes for this, but switched recently to memory
>>>> mapped files in hopes of increasing the (lacking) performance.
>>>>
>>>> Early (simplified) prototypes (outside of Flink) showed that this would
>>>> yield a significant increase. yet when i added the code to flink and ran
>>>> a
>>>> job, there was
>>>> no effect. like at all. two radically different schemes ran in /exactly/
>>>> the same time.
>>>>
>>>> my conclusion was that code already in place (and not part of the
>>>> prototypes) is responsible for this.
>>>> so i went ahead and modified the prototypes to use all relevant code from
>>>> the Python API in order to narrow down the culprit. but this time, the
>>>> performance increase was there.
>>>>
>>>> Now here's the question: How can the /very same code/ perform so much
>>>> worse when integrated into flink? if the code is not the problem, what
>>>> could be it?
>>>>
>>>> i spent a lot of time looking for that one line of code that cripples the
>>>> performance, but I'm pretty much out of places to look.
>>>>
>>>>
>>>>
#|#<CANC1h_udU32wt9ZrDR+LWa5Ez4DOwgEtXkg97ZrPncDZWN83Xw@mail.gmail.com>##//##<540DBA1C.5060700@fu-berlin.de>#|#2014-09-08-14:16:19#|#Chesnay Schepler <chesnay.schepler@fu-berlin.de>#|#Re: Python API - Weird Performance Issue#|#
--------------090804030806070103020607
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit

sorry for the late answer.

today i did a quick hack to replace the synchronization completely with
udp. its still synchronous and record based, but 25x slower.
regarding busy-loops i would propose the following:

 1. leave the python side as it is. its doing most of the heavy lifting
    anyway and will run at 100% regardless of the loops. (the loops only
    take up 5% of the total runtime)
 2. once we exchange buffers instead of single records the IO operations
    and synchronization will take a fairly constant time. we could then
    put the java process to sleep manually for that time instead of
    waiting. it may not be as good as a blocking operation, but it
    should keep the cpu consumption down to some extent.

On 1.9.2014 22:50, Ufuk Celebi wrote:
> Hey Chesnay,
>
> any progress on this today? Are you going for the UDP buffer availability
> notifications Stephan proposed instead of the busy loop?
>
> Ufuk
>
>
> On Thu, Aug 28, 2014 at 1:06 AM, Chesnay Schepler <
> chesnay.schepler@fu-berlin.de> wrote:
>
>> the performance differences occur on the same system (16GB, 4 cores +
>> HyperThreading) with a DOP of 1 for a plan consisting of a single operator.
>> plenty of resources :/
>>
>>
>> On 28.8.2014 0:50, Stephan Ewen wrote:
>>
>>> Hey Chesnay!
>>>
>>> Here are some thoughts:
>>>
>>>    - The repeated checking for 1 or 0 is indeed a busy loop. These may
>>> behave
>>> very different in different settings. If you run the code isolated, you
>>> have a spare core for the thread and it barely hurts. Run multiple
>>> parallel
>>> instances in a larger framework, and it eats away CPU cycles from the
>>> threads that do the work - it starts hurting badly.
>>>
>>>    - You may get around a copy into the shared memory (ByteBuffer into
>>> MemoryMappedFile) by creating an according DataOutputView - save one more
>>> data copy. That's the next step, though, first solve the other issue.
>>>
>>> The last time I implemented such an inter-process data pipe between
>>> languages, I had a similar issue: No support for system wide semaphores
>>> (or
>>> something similar) on both sides.
>>>
>>> I used Shared memory for the buffers, and a local network socket (UDP, but
>>> I guess TCP would be fine as well) for notifications when buffers are
>>> available. That worked pretty well, yielded high throughput, because the
>>> big buffers were not copied (unlike in streams), and the UDP notifications
>>> were very fast (fire and forget datagrams).
>>>
>>> Stephan
>>>
>>>
>>>
>>> On Wed, Aug 27, 2014 at 10:48 PM, Chesnay Schepler <
>>> chesnay.schepler@fu-berlin.de> wrote:
>>>
>>>   Hey Stephan,
>>>> I'd like to point out right away that the code related to your questions
>>>> is shared by both programs.
>>>>
>>>> regarding your first point: i have a byte[] into which i serialize the
>>>> data first using a ByteBuffer, and then write that data to a
>>>> MappedByteBuffer.
>>>>
>>>> regarding synchronization: i couldn't find a way to use elaborate things
>>>> like semaphores or similar that work between python and java alike.
>>>>
>>>> the data exchange is currently completely synchronous. java writes a
>>>> record, sets an "isWritten" bit and then repeatedly checks this bit
>>>> whether
>>>> it is 0. python repeatedly checks this bit whether it is 1. once that
>>>> happens, it reads the record, sets the bit to 0 which tells java that it
>>>> has read the record and can write the next one. this scheme works the
>>>> same
>>>> way the other way around.
>>>>
>>>> *NOW,* this may seem ... inefficient, to put it slightly. it is (or
>>>> rather
>>>> should be...) way faster (5x) that what we had so far though
>>>> (asynchronous
>>>> pipes).
>>>> (i also tried different schemes that all had no effect, so i decided to
>>>> stick with the easiest one)
>>>>
>>>> on to your last point: I'm gonna check for that tomorrow.
>>>>
>>>>
>>>>
>>>>
>>>> On 27.8.2014 20:45, Stephan Ewen wrote:
>>>>
>>>>   Hi Chesnay!
>>>>> That is an interesting problem, though hard to judge with the
>>>>> information
>>>>> we have.
>>>>>
>>>>> Can you elaborate a bit on the following points:
>>>>>
>>>>>     - When putting the objects from the Java Flink side into the shared
>>>>> memory, you need to serialize them. How do you do that? Into a buffer,
>>>>> then
>>>>> copy that into the shared memory ByteBuffer? Directly?
>>>>>
>>>>>     - Shared memory access has to be somehow controlled. The pipes give
>>>>> you
>>>>> flow control for free (blocking write calls when the stream consumer is
>>>>> busy). What do you do for the shared memory? Usually, one uses
>>>>> semaphores,
>>>>> or, in java File(Range)Locks to coordinate access and block until memory
>>>>> regions are made available. Can you check if there are some busy waiting
>>>>> parts in you code?
>>>>>
>>>>>     - More general: The code is slower, but does it burn CPU cycles in
>>>>> its
>>>>> slowness or is it waiting for locks / monitors / conditions ?
>>>>>
>>>>> Stephan
>>>>>
>>>>>
>>>>>
>>>>> On Wed, Aug 27, 2014 at 8:34 PM, Chesnay Schepler <
>>>>> chesnay.schepler@fu-berlin.de> wrote:
>>>>>
>>>>>    Hello everyone,
>>>>>
>>>>>> This will be some kind of brainstorming question.
>>>>>>
>>>>>> As some of you may know I am currently working on the Python API. The
>>>>>> most
>>>>>> crucial part here is how the data is exchanged between Java and Python.
>>>>>> Up to this point we used pipes for this, but switched recently to
>>>>>> memory
>>>>>> mapped files in hopes of increasing the (lacking) performance.
>>>>>>
>>>>>> Early (simplified) prototypes (outside of Flink) showed that this would
>>>>>> yield a significant increase. yet when i added the code to flink and
>>>>>> ran
>>>>>> a
>>>>>> job, there was
>>>>>> no effect. like at all. two radically different schemes ran in
>>>>>> /exactly/
>>>>>> the same time.
>>>>>>
>>>>>> my conclusion was that code already in place (and not part of the
>>>>>> prototypes) is responsible for this.
>>>>>> so i went ahead and modified the prototypes to use all relevant code
>>>>>> from
>>>>>> the Python API in order to narrow down the culprit. but this time, the
>>>>>> performance increase was there.
>>>>>>
>>>>>> Now here's the question: How can the /very same code/ perform so much
>>>>>> worse when integrated into flink? if the code is not the problem, what
>>>>>> could be it?
>>>>>>
>>>>>> i spent a lot of time looking for that one line of code that cripples
>>>>>> the
>>>>>> performance, but I'm pretty much out of places to look.
>>>>>>
>>>>>>
>>>>>>
>>>>>>


--------------090804030806070103020607--
#|#<CAKiyyaFVW_3MNPGgf=qS7dmEgS1qAO2hnBLXFKked9UaUxJeAQ@mail.gmail.com>##//##<540FA1E7.70402@fu-berlin.de>#|#2014-09-10-00:57:39#|#Chesnay Schepler <chesnay.schepler@fu-berlin.de>#|#Exception when running WC#|#
--------------090609020404010405040403
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit

Hello,

tonight i was running a WordCount job with the Python API, and halfway
through i got the exception below.
the issue did not occur again after ressubmitting the job.
DOP=160
taskslots=8
filesize=100GB

    org.apache.flink.client.program.ProgramInvocationException: The
    program execution failed: java.lang.RuntimeException: An error
    occurred while reading the next record: The channel is erroneous.
         at
    org.apache.flink.runtime.util.KeyGroupedIterator$ValuesIterator.hasNext(KeyGroupedIterator.java:202)
         at
    org.apache.flink.languagebinding.api.java.streaming.Sender.sendRecords(Sender.java:57)
         at
    org.apache.flink.languagebinding.api.java.streaming.Streamer.stream(Streamer.java:106)
         at
    org.apache.flink.languagebinding.api.java.python.functions.PythonGroupReduce.reduce(PythonGroupReduce.java:77)
         at
    org.apache.flink.runtime.operators.GroupReduceDriver.run(GroupReduceDriver.java:108)
         at
    org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:509)
         at
    org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:374)
         at
    org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:265)
         at java.lang.Thread.run(Thread.java:745)
    Caused by: java.io.IOException: The channel is erroneous.
         at
    org.apache.flink.runtime.io.disk.iomanager.ChannelAccess.checkErroneous(ChannelAccess.java:132)
         at
    org.apache.flink.runtime.io.disk.iomanager.BlockChannelReader.readBlock(BlockChannelReader.java:75)
         at
    org.apache.flink.runtime.io.disk.iomanager.ChannelReaderInputView.sendReadRequest(ChannelReaderInputView.java:263)
         at
    org.apache.flink.runtime.io.disk.iomanager.ChannelReaderInputView.nextSegment(ChannelReaderInputView.java:226)
         at
    org.apache.flink.runtime.memorymanager.AbstractPagedInputView.advance(AbstractPagedInputView.java:159)
         at
    org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readByte(AbstractPagedInputView.java:270)
         at
    org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readUnsignedByte(AbstractPagedInputView.java:277)
         at
    org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readInt(AbstractPagedInputView.java:340)
         at
    org.apache.flink.api.common.typeutils.base.IntSerializer.deserialize(IntSerializer.java:69)
         at
    org.apache.flink.api.common.typeutils.base.IntSerializer.deserialize(IntSerializer.java:28)
         at
    org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:115)
         at
    org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30)
         at
    org.apache.flink.runtime.io.disk.ChannelReaderInputViewIterator.next(ChannelReaderInputViewIterator.java:86)
         at
    org.apache.flink.runtime.operators.sort.MergeIterator$HeadStream.nextHead(MergeIterator.java:131)
         at
    org.apache.flink.runtime.operators.sort.MergeIterator.next(MergeIterator.java:89)
         at
    org.apache.flink.runtime.util.KeyGroupedIterator$ValuesIterator.hasNext(KeyGroupedIterator.java:177)
         ... 8 more
    Caused by: java.io.IOException: Input/output error
         at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
         at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
         at sun.nio.ch.IOUtil.read(IOUtil.java:197)
         at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:149)
         at
    org.apache.flink.runtime.io.disk.iomanager.SegmentReadRequest.read(BlockChannelAccess.java:221)
         at
    org.apache.flink.runtime.io.disk.iomanager.IOManager$ReaderThread.run(IOManager.java:551)
         at org.apache.flink.client.program.Client.run(Client.java:321)
         at org.apache.flink.client.program.Client.run(Client.java:287)
         at org.apache.flink.client.program.Client.run(Client.java:281)
         at
    org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:54)
         at
    org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:519)
         at
    org.apache.flink.languagebinding.api.java.python.PythonExecutor.main(PythonExecutor.java:119)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
         at
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:606)
         at
    org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:389)
         at
    org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:307)
         at org.apache.flink.client.program.Client.run(Client.java:240)
         at
    org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:332)
         at org.apache.flink.client.CliFrontend.run(CliFrontend.java:319)
         at
    org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:930)
         at org.apache.flink.client.CliFrontend.main(CliFrontend.java:954)


--------------090609020404010405040403--
#|#null##//##<5474F606.9020703@apache.org>#|#2014-11-25-21:35:54#|#Daniel Warneke <warneke@apache.org>#|#Re: Compression of network traffic#|#
Hi,

if you really want to add compression on the data path, I would
encourage you to choose something as lightweight as possible. 10 GBit
Ethernet is becoming pretty much commodity these days in the server
space and it is not easy to saturate such a link even without compression.

Snappy is not a bad choice, but the fastest algorithm Iâ€™ve seen so far
is LZ4:

https://code.google.com/p/lz4/

Best regards,

     Daniel


Am 25.11.2014 20:53, schrieb Stephan Ewen:
> I would start with a simple compression of network buffers as a blob.
>
> At some point, Flink's internal data layout may become columnar, which
> should also help the blob-style compression, because more similar strings
> will be within one window...
>
> On Tue, Nov 25, 2014 at 11:26 AM, Viktor Rosenfeld <
> viktor.rosenfeld@tu-berlin.de> wrote:
>
>> Hi,
>>
>> A codec like Snappy would work on an entire network buffer as one big blob,
>> right? I was more thinking along the lines of compressing individual tuples
>> fields by treating them as columns, e.g., using frame-of-reference encoding
>> and bit backing. Compression on tuple fields should yield much better
>> results than compressing the entire blob. Given that Flink controls the
>> serialization process this should be transparent to other layers in the
>> code. Not sure it is worth the effort though.
>>
>> Cheers,
>> Viktor
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Compression-of-network-traffic-tp2568p2607.html
>> Sent from the Apache Flink (Incubator) Mailing List archive. mailing list
>> archive at Nabble.com.
>>
#|#<CANC1h_tjGbBGK3s5ezYKebubkZYbGKvRgAfy6TVKznuZt0u94A@mail.gmail.com>##//##<54BE3BBE.5000508@apache.org>#|#2015-01-20-11:27:50#|#Timo Walther <twalthr@apache.org>#|#Re: Handling custom types with Kryo#|#
Are we talking about the types for the input/output of operators or also
types that are used inside UDFs?
Operator I/O type classes are known, so we don't need static code
analysis for that. For types inside UDFs I can add that requirement to
FLINK-1319.


On 20.01.2015 11:51, Alexander Alexandrov wrote:
> +1 for program analysis from me too...
>
> Should be doable also on a lower level (e.g. analysis of compiled *.class
> files) with some off-the-shelf libraries, right?
>
> 2015-01-20 11:39 GMT+01:00 Till Rohrmann <till.rohrmann@gmail.com>:
>
>> I like the idea to automatically figure out which types are used by a
>> program and to register them at Kryo. Thus, +1 for this idea.
>>
>> On Tue, Jan 20, 2015 at 11:34 AM, Robert Metzger <rmetzger@apache.org>
>> wrote:
>>
>>> @Stephan: Yes, you are summarizing it correctly.
>>> I'll assign FLINK-1417 to myself and implement it as discussed here
>> (once I
>>> have resolved the other issues assigned to me)
>>>
>>> There is one additional point we forgot in the discussion so far: We are
>>> initializing Kryo with twitter/chill's "ScalaKryoInstantiator()". I just
>>> checked and its registering 51 default serializers and 81 registered
>> types.
>>> I just talked to Till (who implemented the KryoSerializer originally) and
>>> he also suggests to pool kryo instances using thread-local variables.
>> I'll
>>> look into that as well once FLINK-1417 has been resolved. I think that
>>> helps to mitigate the heavy initialization of Kryo (which is inevitable)
>>>
>>>
>>> @Arvid: Our POJO/Types support does explicitly not require our users to
>>> implement any interfaces, so that option is not feasible.
>>> The bytecode analysis is not part of the main flink distribution because
>> it
>>> is using a library with an apache incompatible license.
>>>
>>>
>>>
>>> On Tue, Jan 20, 2015 at 9:58 AM, Arvid Heise <arvid.heise@gmail.com>
>>> wrote:
>>>
>>>> An alternative way that may not be applicable in your case:
>>>>
>>>> For Sopremo, all types implemented a common interface. When a package
>> is
>>>> loaded, the Sopremo package manager scans the jar and looks for classes
>>>> implementing the interfaces (quite fast, because not the entire class
>>> must
>>>> be loaded). All types implementing the interface are automatically
>> added
>>> to
>>>> Kryo with their respective annotated serializers.
>>>>
>>>> If you still have bytecode analysis of jobs, you can also statically
>>>> determine all types that are used, check for default serializers, and
>>>> maintain only a minimal set of serializers used for this specific job.
>>> You
>>>> could already warn for unregistered types before submitting jobs.
>>>>
>>>> On Tue, Jan 20, 2015 at 6:38 AM, Stephan Ewen <sewen@apache.org>
>> wrote:
>>>>> Yes, I agree that the Avro serializer should be available by default.
>>>> That
>>>>> is one case of a typical type that should work out of the box, given
>>> that
>>>>> we support Avro file formats.
>>>>>
>>>>> Let me summarize how I understood that suggestion:
>>>>>
>>>>>   - We make Avro available by default by registering a default
>>> serializer
>>>>> for the SpecificBase
>>>>>
>>>>>   - We create a library of serializers. We do not register them by
>>>> default.
>>>>>   - Via FLINK-1417, we analyze the types. For any (nested) type that
>> we
>>>>> encounter for which we have a serializer in the library, we register
>>> that
>>>>> serializer as the default serializer. Also, for every (nested) type
>> we
>>>>> encounter, we register a tag at Kryo.
>>>>>
>>>>> I like that, it should give a nice and smooth user experience.
>>>>>
>>>>> Greetings,
>>>>> Stephan
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Mon, Jan 19, 2015 at 12:32 PM, Robert Metzger <
>> rmetzger@apache.org>
>>>>> wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> thank you for putting our discussion to the mailing list. This is
>>>> indeed
>>>>>> where such discussions belong. For the others, we started
>> discussing
>>>>> here:
>>>>>> https://github.com/apache/flink/pull/304
>>>>>>
>>>>>> I think there is one additional approach, which is probably close
>> to
>>>> (1):
>>>>>> We only register those serializers by default which we don't see in
>>> the
>>>>>> pre-flight phase (I think right now thats only GenericData.Array
>> from
>>>>>> Avro).
>>>>>> We would come across all the other classes (Jodatime, Protobuf,
>> Avro,
>>>>>> Thrift, ...) when traversing the class hierarchy, as proposed in
>>>>>> FLINK-1417. With this approach, users get the best out-of-the box
>>>>>> experience and the number of registered classes / serializers is
>> kept
>>>> at
>>>>> a
>>>>>> minimum.
>>>>>> We can still offer means to register additional serializers (I
>> think
>>>>> thats
>>>>>> already merged to master).
>>>>>>
>>>>>> My main concern with this particular issue is a good out of the box
>>>> user
>>>>>> experience. If there is an issue with type serialization, users
>> will
>>>>> notice
>>>>>> it very early. (In my experience people often have their existing
>>>>> datatypes
>>>>>> they use with other systems, and they want to continue using them)
>>>>>> Therefore, I want to put some effort into making it as good as
>>>> possible.
>>>>> I
>>>>>> would actually sacrifice performance over stability/usability here.
>>> Our
>>>>>> system is flexible enough to replace it later with a more efficient
>>>>>> serialization if that becomes an issue. But maybe my suggestion
>> above
>>>> is
>>>>>> already sufficient.
>>>>>>
>>>>>> We could also think about introducing a configuration variable
>> which
>>>>> allows
>>>>>> users to disable the default serializers.
>>>>>>
>>>>>>
>>>>>> Regarding the second question: Is there a downside registering all
>>>> types
>>>>>> for tagging? We reduce the overall I/O which is good for
>> performance.
>>>>>> Best,
>>>>>> Robert
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Mon, Jan 19, 2015 at 8:24 PM, Stephan Ewen <sewen@apache.org>
>>>> wrote:
>>>>>>> Hi all!
>>>>>>>
>>>>>>> We have various pending pull requests that add support for
>> certain
>>>>> types
>>>>>> by
>>>>>>> adding extra kryo serializers.
>>>>>>>
>>>>>>> I think we need to decide how we want to handle the support for
>>> extra
>>>>>>> types, because more are certainly to come.
>>>>>>>
>>>>>>> As I understand it, we have three broad options:
>>>>>>>
>>>>>>> (1)
>>>>>>> Add as many serializers to Kryo by default as possible.
>>>>>>>   Pro:
>>>>>>>      - Many types work out of the box
>>>>>>>   Contra:
>>>>>>>      - We may eventually overload the kryo registry with
>> serializers
>>>>>>>        that are not needed for most cases and suffer in
>> performance
>>>>>>>      - It is hard to guess which types work out of the box
>>>>> (intransparent)
>>>>>>>
>>>>>>> (2)
>>>>>>> We create a collection of serializers and a registration util.
>>>>>>> --------
>>>>>>> val env = ExecutionEnvironemnt.getExecutionEnviroment()
>>>>>>>
>>>>>>> Serializers.registerProtoBufSerializers(env);
>>>>>>> Serializers.registerJavaUtilSerializers(env);
>>>>>>> ---------
>>>>>>> Pro:
>>>>>>>    - Easy for users
>>>>>>>    - We can grow the set of supported types very large without
>>>>> overloading
>>>>>>> Kryo
>>>>>>>    - It is transparent what gets registered
>>>>>>>
>>>>>>> Contra:
>>>>>>>    - Not quite as convenient as if things just run
>>>>>>>
>>>>>>>
>>>>>>> (3)
>>>>>>> We do nothing and let the user create and register whatever is
>>>> needed.
>>>>>>> We could have a library and utility for serializers for certain
>>>>>> libraries.
>>>>>>> Users could use this to conveniently add serializers for the
>>>> libraries
>>>>>> they
>>>>>>> use.
>>>>>>> Pro:
>>>>>>>    - Simple for us ;-)
>>>>>>> Contra:
>>>>>>>    - More repeated work for users
>>>>>>>
>>>>>>>
>>>>>>> =======================>>>>>>>
>>>>>>> For approach (1) and (2), there is an orthogonal question of
>>> whether
>>>> we
>>>>>>> want to simply register default serializers (that enable that
>> types
>>>>> work)
>>>>>>> or also register types for tags, to speed up the serialization of
>>>> those
>>>>>>> types.
>>>>>>>
>>>>>>>
>>>>>>> Greetings,
>>>>>>> Stephan
>>>>>>>
#|#<CAEXqXca2gxydJjOCseJ8R0JFos_jkWW4CJhbfxdGxWcwcyQvaA@mail.gmail.com>##//##<54D223DC.3070307@apache.org>#|#2015-02-04-13:51:23#|#Timo Walther <twalthr@apache.org>#|#Re: Sorting of fields#|#
Ok, I found an earlier discussion about it. Sorry for the mail.

However, I think this is a very important feature and I should be added
soon.

On 04.02.2015 14:38, Timo Walther wrote:
> Hey,
>
> is it correct that we currently do not support sorting without any
> grouping? I had this question by 2 users in the last weeks and now I
> also need this functionality.
>
>
> Is it possible to sort e.g. a word count result Tuple2<String,
> Integer> by count?
>
>
> Regards,
> Timo
>
#|#<54D220E3.3050305@apache.org>##//##<54F489B1.4040208@informatik.hu-berlin.de>#|#2015-03-02-16:05:48#|#"Matthias J. Sax" <mjsax@informatik.hu-berlin.de>#|#Re: Problem mvn install#|#
--avmGHVbN1aSpfKAFj074isV5FPnOjuFpG
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

I guess, Eclipse created those files. I delete them manually, what
resolved the problem. I added "bin" to my local .gitignore and thus "git
status" did not list the files and I was not aware the they are not part
of the repository.

As far as I know, "-Dmaven.test.skip=3Dtrue" is equal to "-DskipTests".

-Matthias


On 03/02/2015 04:54 PM, Stephan Ewen wrote:
> Matthias!
>=20
> The files should not exist. Has some IDE setup copied the files into the
> "bin" directory (as part of compiling it without maven) ? It looks like you
> are building it not through maven really...
>=20
> BTW: Does it make a difference whether you use "mvn -Dmaven.test.skip=3Dtrue
> clean install" or "mvn -DskipTests clean install"
>=20
> Stephan
>=20
>=20
>=20
>=20
> On Mon, Mar 2, 2015 at 3:38 PM, Fabian Hueske <fhueske@gmail.com> wrote:
>=20
>> Hi Matthias,
>>
>> I just checked and could not reproduce the error.
>> The files that Maven RAT complained about do not exist in Flink's master
>> branch.
>> I don't think they are put there as part of the build process.
>>
>> Best, Fabian
>>
>>
>>
>>
>>
>> 2015-03-02 15:09 GMT+01:00 Matthias J. Sax <mjsax@informatik.hu-berlin=2Ede
>>> :
>>
>>> Hi,
>>>
>>> if I start "mvn-Dmaven.test.skip=3Dtrue clean install", the goal fails and
>>> I get the following error:
>>>
>>>> Unapproved licenses:
>>>>
>>>>   flink-clients/bin/src/main/resources/web-docs/js/dagre-d3.js
>>>>   flink-clients/bin/src/main/resources/web-docs/js/d3.js
>>>>   flink-staging/flink-avro/bin/src/test/resources/avro/user.avsc
>>>
>>> It seems, that an APL2 compatible license is expected. Can anyone
>>> comment it this issue.
>>>
>>> -> I am on branch flink/master
>>>
>>>
>>> -Matthias
>>>
>>>
>>
>=20


--avmGHVbN1aSpfKAFj074isV5FPnOjuFpG
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1

iQIcBAEBAgAGBQJU9Im3AAoJEBXkotPFErDWbccQAKHPxByW4cFdgwhdo8c1THvh
zkLRDt3Mv0t1/sbcQQ7GiDbtyvi3/DfB1NzhOAf9kj8mUtZ0oTDWbS6aR6hL3ELa
GddH7vASPDK0guL7wcuRA6Vu52yxlEW5JbUNTMkDEJJDO4ObTqrXJ0+MoFIwy7KG
23C1dbnrXtgJ36QYK1gdKEBkHYz6itA6/r5PyoNBp8IuYoJfW4MrCI9WboggzQ+C
Ga24sKnRcdEXjezRHYsNL+Tnemz7Z/yzTL/wlqFWENuefXSykIP/L0S+NROJNWgx
mmJP4SSPQzZfcZ20TnI5GDhZk5EeiSVbuJC+eCbj5MoYVkkApxm9vXEH1nY8pw2X
mbkW2civZpK9wUdw05mCbMNfsC44WdDuqvRcyaiUUi1fXXhY/yRxGIXMgkaRyP14
xmhdy11BLGwmiTTfyZKqXXMHaZRmlAaRseZiEbbE6f8/tAnywOpYdbWZRhpwQA9k
FJKxN29AVpjY3cjSKoy8Eqh6YlDhSJPRpOWJ17JOo17YwP61Bp7lhhllHv5biRJ3
0sn0j9jaikvDIaNWX2J97a7Ud4uDTfVfrLbq2o0qeVqLQF3FTs2IwJB1f0k7/kb9
E/iyNJA5GSqNtSReLptJcQTV+UdGUibBbdPwo99vdUAMENn5spvIASvOri/k0T5c
H9umtcAJb+IhikXat6oZ
=C88h
-----END PGP SIGNATURE-----

--avmGHVbN1aSpfKAFj074isV5FPnOjuFpG--
#|#<CANC1h_ukVU7zisqx7wHgxKjeRe8Qi3TGDgPnto7dfzinAeq6RA@mail.gmail.com>##//##<54FD7E34.80603@apache.org>#|#2015-03-09-11:03:58#|#Timo Walther <twalthr@apache.org>#|#Re: Semantic Properties and Functions with Iterables#|#
Thanks for the clarification. If I have understood it correctly,
forwarded fields are only interesting for key fields, right? So I will
implement that key information is passed to the analyzer for consideration.

So if GroupReduce1 is grouped by f1, the result will be
@ForwardedFields("1") in this example and not "*":

     public static class GroupReduce1 implements
GroupReduceFunction<Tuple2<Long, Long>,Tuple2<Long, Long>> {
         @Override
         public void reduce(Iterable<Tuple2<Long, Long>> values,
                 Collector<Tuple2<Long, Long>> out) throws Exception {
             out.collect(values.iterator().next());
         }
     }


On 08.03.2015 23:21, Fabian Hueske wrote:
> I added you comment and an answer to FLINK-1656:
>
> "Right, that's a good point.
>
> +1 limiting to key fields. That's much easier to reason about for users.
>
> However, I am not sure how it is implemented right now.
> I guess secondary sort info is already removed by the property filtering,
> but I need to verify that."
>
> 2015-03-08 21:53 GMT+01:00 Stephan Ewen <sewen@apache.org>:
>
>> Any other thoughts in this?
>>
>> On Fri, Mar 6, 2015 at 12:12 PM, Stephan Ewen <sewen@apache.org> wrote:
>>
>>> I think the order of emitting elements is not part of the forward field
>>> properties, but would rather be a separate one that we do not have right
>>> now.
>>>
>>> At the moment, we would assume that all group operations destroy
>> secondary
>>> orders.
>>>
>>> In that sense, forward fields in group operations only make sense for
>>> fields where all fields are the same in the group (key fields).
>>>
>>> On Fri, Mar 6, 2015 at 11:25 AM, Fabian Hueske <fhueske@gmail.com>
>> wrote:
>>>> Hi Timo,
>>>>
>>>> there are several restrictions for forwarded fields of operators with
>>>> iterator input.
>>>> 1) forwarded fields must be emitted in the order in which they are
>>>> received
>>>> through the iterator
>>>> 2) all forwarded fields of a record must stick together, i.e., if your
>>>> function builds record from field 0 of the 1st, 3rd, 5th, ... and field
>> 1
>>>> of the 2nd, 4th, ... record coming through the iterator, these are not
>>>> valid forwarded fields.
>>>> 3) it is OK to completely filter out records coming through the
>> iterator.
>>>> The reason for these rules is, that the optimizer uses forwarded fields
>> to
>>>> reason about physical data properties such as order and grouping. If you
>>>> mix up the order of records or emit records which are composed from
>>>> different input records, you might destroy a (secondary) order or
>>>> grouping.
>>>>
>>>> Considering these rules, your second example is correct as well.
>>>> In case of the TriadBuilder, the information is correct (in the context
>> of
>>>> the Program) as well, because field 0 is used as key. It is however
>> true,
>>>> that there is a strange dependency between the function and the context
>> in
>>>> which it is used within the program. It would be better to remove the
>>>> class
>>>> annotation, and add this information through the
>> .withForwardedFields("0")
>>>> method in the program, to make that clear.
>>>>
>>>> It is very good that you raise this point.
>>>> This is currently not reflected in the documentation is should be made
>>>> clear very soon. I will open a JIRA for that.
>>>>
>>>> Thanks, Fabian
>>>>
>>>>
>>>>
>>>> 2015-03-06 10:19 GMT+01:00 Timo Walther <twalthr@apache.org>:
>>>>
>>>>> Hey all,
>>>>>
>>>>> I'm currently working a lot on the UDF static code analyzer. But I
>> have
>>>> a
>>>>> general question about Semantic Properties which might be also
>>>> interesting
>>>>> for other users.
>>>>>
>>>>> How is the ForwardedFields annotation interpreted for UDF functions
>> with
>>>>> Iterables?
>>>>>
>>>>> An example can be found in: org.apache.flink.examples.
>>>>> java.graph.EnumTrianglesBasic.TriadBuilder
>>>>>
>>>>> Does this mean that each call of "collect" must happen in the same
>> order
>>>>> than the call of "next"? But this is not the case in the example
>> above.
>>>> Or
>>>>> does the annotation only refer to the first iterator element?
>>>>>
>>>>> Other examples:
>>>>>
>>>>> @ForwardedFields("*") // CORRECT?
>>>>>      public static class GroupReduce1 implements
>>>> GroupReduceFunction<Tuple2<Long,
>>>>> Long>,Tuple2<Long, Long>> {
>>>>>          @Override
>>>>>          public void reduce(Iterable<Tuple2<Long, Long>> values,
>>>>>                  Collector<Tuple2<Long, Long>> out) throws Exception {
>>>>>              out.collect(values.iterator().next());
>>>>>          }
>>>>>      }
>>>>>
>>>>> @ForwardedFields("*") // NOT CORRECT?
>>>>>      public static class GroupReduce3 implements
>>>> GroupReduceFunction<Tuple2<Long,
>>>>> Long>,Tuple2<Long, Long>> {
>>>>>          @Override
>>>>>          public void reduce(Iterable<Tuple2<Long, Long>> values,
>>>>>                  Collector<Tuple2<Long, Long>> out) throws Exception {
>>>>>              Iterator<Tuple2<Long, Long>> it = values.iterator();
>>>>>              while (it.hasNext()) {
>>>>>                  Tuple2<Long,Long> t = it.next();
>>>>>                  if (t.f0 == 42) {
>>>>>                      out.collect(t);
>>>>>                  }
>>>>>              }
>>>>>          }
>>>>>      }
>>>>>
>>>>> Thanks in advance.
>>>>>
>>>>> Regards,
>>>>> Timo
>>>>>
>>>
#|#<CAAdrtT1iY2_q8H9Ed8Q4XDVU9st94L_t1ADqKNC=5Lz5rOv_Yg@mail.gmail.com>##//##<551dde2e.68cec20a.06d4.21ce@mx.google.com>#|#2015-04-03-00:27:40#|#<fhueske@gmail.com>#|#=?utf-8?Q?Re:_HBase_TableOutputFormat_fix_(Flink_0.8.1)?=#|#
--_4C25082E-1676-4389-B574-7B2D487745E1_
Content-Transfer-Encoding: base64
Content-Type: text/plain; charset="utf-8"
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--_4C25082E-1676-4389-B574-7B2D487745E1_--
#|#null##//##<5523A868.4050505@informatik.hu-berlin.de>#|#2015-04-07-09:57:09#|#"Matthias J. Sax" <mjsax@informatik.hu-berlin.de>#|#Re: Rework of the window-join semantics#|#
--Fmgh2GMaw7RwLJUo99SammSagQ7eNI28x
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi @all,

please keep me in the loop for this work. I am highly interested and I
want to help on it.

My initial thoughts are as follows:

 1) Currently, system timestamps are used and the suggested approach can
be seen as state-of-the-art (there is actually a research paper using
the exact same join semantic). Of course, the current approach is
inherently non-deterministic. The advantage is, that there is no
overhead in keeping track of the order of records and the latency should
be very low. (Additionally, state-recovery is simplified. Because, the
processing in inherently non-deterministic, recovery can be done with
relaxed guarantees).

  2) The user should be able to "switch on" deterministic processing,
ie, records are timestamped (either externally when generated, or
timestamped at the sources). Because deterministic processing adds some
overhead, the user should decide for it actively.
In this case, the order must be preserved in each re-distribution step
(merging is sufficient, if order is preserved within each incoming
channel). Furthermore, deterministic processing can be achieved by sound
window semantics (and there is a bunch of them). Even for
single-stream-windows it's a tricky problem; for join-windows it's even
harder. From my point of view, it is less important which semantics are
chosen; however, the user must be aware how it works. The most tricky
part for deterministic processing, is to deal with duplicate timestamps
(which cannot be avoided). The timestamping for (intermediate) result
tuples, is also an important question to be answered.


-Matthias


On 04/07/2015 11:37 AM, Gyula F=C3=B3ra wrote:
> Hey,
>=20
> I agree with Kostas, if we define the exact semantics how this works, this
> is not more ad-hoc than any other stateful operator with multiple inputs.
> (And I don't think any other system support something similar)
>=20
> We need to make some design choices that are similar to the issues we had
> for windowing. We need to chose how we want to evaluate the windowing
> policies (global or local) because that affects what kind of policies can
> be parallel, but I can work on these things.
>=20
> I think this is an amazing feature, so I wouldn't necessarily rush the
> implementation for 0.9 though.
>=20
> And thanks for helping writing these down.
>=20
> Gyula
>=20
> On Tue, Apr 7, 2015 at 11:11 AM, Kostas Tzoumas <ktzoumas@apache.org> wrote:
>=20
>> Yes, we should write these semantics down. I volunteer to help.
>>
>> I don't think that this is very ad-hoc. The semantics are basically the
>> following. Assuming an arriving element from the left side:
>> (1) We find the right-side matches
>> (2) We insert the left-side arrival into the left window
>> (3) We recompute the left window
>> We need to see whether right window re-computation needs to be triggered as
>> well. I think that this way of joining streams is also what the symmetric
>> hash join algorithms were meant to support.
>>
>> Kostas
>>
>>
>> On Tue, Apr 7, 2015 at 10:49 AM, Stephan Ewen <sewen@apache.org> wrote:
>>
>>> Is the approach of joining an element at a time from one input against a
>>> window on the other input not a bit arbitrary?
>>>
>>> This just joins whatever currently happens to be the window by the time
>> the
>>> single element arrives - that is a bit non-predictable, right?
>>>
>>> As a more general point: The whole semantics of windowing and when they
>> are
>>> triggered are a bit ad-hoc now. It would be really good to start
>>> formalizing that a bit and
>>> put it down somewhere. Users need to be able to clearly understand and
>> how
>>> to predict the output.
>>>
>>>
>>>
>>> On Fri, Apr 3, 2015 at 12:10 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com>
>> wrote:
>>>
>>>> I think it should be possible to make this compatible with the
>>>> .window().every() calls. Maybe if there is some trigger set in "every"
>> we
>>>> would not join that stream 1 by 1 but every so many elements. The
>> problem
>>>> here is that the window and every in this case are very-very different
>>> than
>>>> the normal windowing semantics. The window would define the join window
>>> for
>>>> each element of the other stream while every would define how often I
>>> join
>>>> This stream with the other one.
>>>>
>>>> We need to think to make this intuitive.
>>>>
>>>> On Fri, Apr 3, 2015 at 11:23 AM, M=C3=A1rton Balassi <
>>> balassi.marton@gmail.com>
>>>> wrote:
>>>>
>>>>> That would be really neat, the problem I see there, that we do not
>>>>> distinguish between dataStream.window() and
>> dataStream.window().every()
>>>>> currently, they both return WindowedDataStreams and TriggerPolicies
>> of
>>>> the
>>>>> every call do not make much sense in this setting (in fact
>> practically
>>>> the
>>>>> trigger is always set to count of one).
>>>>>
>>>>> But of course we could make it in a way, that we check that the
>>> eviction
>>>>> should be either null or count of 1, in every other case we throw an
>>>>> exception while building the JobGraph.
>>>>>
>>>>> On Fri, Apr 3, 2015 at 8:43 AM, Aljoscha Krettek <
>> aljoscha@apache.org>
>>>>> wrote:
>>>>>
>>>>>> Or you could define it like this:
>>>>>>
>>>>>> stream_A =3D a.window(...)
>>>>>> stream_B =3D b.window(...)
>>>>>>
>>>>>> stream_A.join(stream_B).where().equals().with()
>>>>>>
>>>>>> So a join would just be a join of two WindowedDataStreamS. This
>> would
>>>>>> neatly move the windowing stuff into one place.
>>>>>>
>>>>>> On Thu, Apr 2, 2015 at 9:54 PM, M=C3=A1rton Balassi <
>>>> balassi.marton@gmail.com
>>>>>>
>>>>>> wrote:
>>>>>>> Big +1 for the proposal for Peter and Gyula. I'm really for
>>> bringing
>>>>> the
>>>>>>> windowing and window join API in sync.
>>>>>>>
>>>>>>> On Thu, Apr 2, 2015 at 6:32 PM, Gyula F=C3=B3ra <gyfora@apache.org>
>>>> wrote:
>>>>>>>
>>>>>>>> Hey guys,
>>>>>>>>
>>>>>>>> As Aljoscha has highlighted earlier the current window join
>>>> semantics
>>>>> in
>>>>>>>> the streaming api doesn't follow the changes in the windowing
>> api.
>>>>> More
>>>>>>>> precisely, we currently only support joins over time windows of
>>>> equal
>>>>>> size
>>>>>>>> on both streams. The reason for this is that we now take a
>> window
>>> of
>>>>>> each
>>>>>>>> of the two streams and do joins over these pairs. This would be
>> a
>>>>>> blocking
>>>>>>>> operation if the windows are not closed at exactly the same time
>>>> (and
>>>>>> since
>>>>>>>> we dont want this we only allow time windows)
>>>>>>>>
>>>>>>>> I talked with Peter who came up with the initial idea of an
>>>>> alternative
>>>>>>>> approach for stream joins which works as follows:
>>>>>>>>
>>>>>>>> Instead of pairing windows for joins, we do element against
>> window
>>>>>> joins.
>>>>>>>> What this means is that whenever we receive an element from one
>> of
>>>> the
>>>>>>>> streams, we join this element with the current window(this
>> window
>>> is
>>>>>>>> constantly updated) of the other stream. This is non-blocking on
>>> any
>>>>>> window
>>>>>>>> definitions as we dont have to wait for windows to be completed
>>> and
>>>> we
>>>>>> can
>>>>>>>> use this with any of our predefined policies like Time.of(...),
>>>>>>>> Count.of(...), Delta.of(....).
>>>>>>>>
>>>>>>>> Additionally this also allows some very flexible way of defining
>>>>> window
>>>>>>>> joins. With this we could also define grouped windowing inside
>> if
>>> a
>>>>>> join.
>>>>>>>> An example of this would be: Join all elements of Stream1 with
>> the
>>>>> last
>>>>>> 5
>>>>>>>> elements by a given windowkey of Stream2 on some join key.
>>>>>>>>
>>>>>>>> This feature can be easily implemented over the current
>> operators,
>>>> so
>>>>> I
>>>>>>>> already have a working prototype for the simple non-grouped
>> case.
>>> My
>>>>>> only
>>>>>>>> concern is the API, the best thing I could come up with is
>>> something
>>>>>> like
>>>>>>>> this:
>>>>>>>>
>>>>>>>> stream_A.join(stream_B).onWindow(windowDefA,
>>>>> windowDefB).by(windowKey1,
>>>>>>>> windowKey2).where(...).equalTo(...).with(...)
>>>>>>>>
>>>>>>>> (the user can omit the "by" and "with" calls)
>>>>>>>>
>>>>>>>> I think this new approach would be worthy of our "flexible
>>>> windowing"
>>>>> in
>>>>>>>> contrast with the current approach.
>>>>>>>>
>>>>>>>> Regards,
>>>>>>>> Gyula
>>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>=20


--Fmgh2GMaw7RwLJUo99SammSagQ7eNI28x
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJVI6hrAAoJEBXkotPFErDWvuMP/3gqLUQcW4gdRTsy99FS3jMP
J3r+/iqrt98GD8DarPJElRqpsWKxvPake5nEDXfaSjcouucIuDGRz5zvGcy7VbqO
1ZpyEtyOiRMUnS6Y5S9GnpAcxKe04APp38c3Vhnsw9QDegUBC1KkHnbhNrVPWdwt
bn6G+CFjgrRl5Og39S3rV3+XkXj99U56QTyo8GrIAkGc/9gzEdwP8w4oVWJO5V5s
4yZl4QVEPzRsZ3exBF1WdH1tjk+JmR6lF+uV5H3A0RXp2VJf3Zak7PaMVAWnQyWP
VqFM8cLUy7CpU1LnT+uV5pQGY/QjmhwFuX4RiuSwvZ2CzCLBZBb6ZVkY0E4eqqpj
9pCR+wN8QlGcNz1AmeZ9+1GK8F5hHYKqjmCR7IB/YqKCcckFgna/efonO579vH0F
lxYvgc5M2AUfRLTX/gzeq5xzGiv8NV8tcieYzzxJJ6KDawHLU33J7PK5ks4Tl5Jl
SXvJTcWEMnog8q1vp/USvEGqTv6wHblVzOh6MUsEKTdZE6h4NfZ4TGCG8uP4o/i5
owbsskGLNDHPB5WllnZFiKxLAUBeL7umGRR0Sn3oOBIUtcl9XBytqiNHHDWry2Pp
Eh8WwKR97pLmmofCv96iWOBnfiR7ahAojiwRQHtq+QrEPKALqj17o3b2TDF0mweT
EJxaRZ9oa403BIRJ/aEv
=Vp1X
-----END PGP SIGNATURE-----

--Fmgh2GMaw7RwLJUo99SammSagQ7eNI28x--
#|#<CA+faj9z4J-nWYfOrAXsBkVgPkscRtKUV169_qi_yna=_RM+2NA@mail.gmail.com>##//##<552662B9.8050908@informatik.hu-berlin.de>#|#2015-04-09-11:37:10#|#"Matthias J. Sax" <mjsax@informatik.hu-berlin.de>#|#Re: Rework of the window-join semantics#|#
--nuS0cwlgqk0oE3FDfB4ver8ktVtGNLVRI
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi Paris,

thanks for the pointer to the Naiad paper. That is quite interesting.

The paper I mentioned [1], does not describe the semantics in detail; it
is more about the implementation for the stream-joins. However, it uses
the same semantics (from my understanding) as proposed by Gyula.

-Matthias

[1] Kang, Naughton, Viglas. "Evaluationg Window Joins over Unbounded
Streams". VLDB 2002.



On 04/07/2015 12:38 PM, Paris Carbone wrote:
> Hello Matthias,
>=20
> Sure, ordering guarantees are indeed a tricky thing, I recall having that discussion back in TU Berlin. Bear in mind thought that DataStream, our abstract data type, represents a *partitioned* unbounded sequence of events. There are no *global* ordering guarantees made whatsoever in that model across partitions. If you see it more generally there are many =E2=80=9Crace conditions=E2=80=9D in a distributed execution graph of vertices that process multiple inputs asynchronously, especially when you add joins and iterations into the mix (how do you deal with reprocessing =E2=80=9Cold=E2=80=9D tuples that iterate in the graph). Btw have you checked the Naiad paper [1]? Stephan cited a while ago and it is quite relevant to that discussion.
>=20
> Also, can you cite the paper with the joining semantics you are referring to? That would be of good help I think.
>=20
> Paris
>=20
> [1] https://users.soe.ucsc.edu/~abadi/Papers/naiad_final.pdf
>=20
> <https://users.soe.ucsc.edu/~abadi/Papers/naiad_final.pdf>
>=20
> <https://users.soe.ucsc.edu/~abadi/Papers/naiad_final.pdf>
> On 07 Apr 2015, at 11:50, Matthias J. Sax <mjsax@informatik.hu-berlin.de<mailto:mjsax@informatik.hu-berlin.de>> wrote:
>=20
> Hi @all,
>=20
> please keep me in the loop for this work. I am highly interested and I
> want to help on it.
>=20
> My initial thoughts are as follows:
>=20
> 1) Currently, system timestamps are used and the suggested approach can
> be seen as state-of-the-art (there is actually a research paper using
> the exact same join semantic). Of course, the current approach is
> inherently non-deterministic. The advantage is, that there is no
> overhead in keeping track of the order of records and the latency should
> be very low. (Additionally, state-recovery is simplified. Because, the
> processing in inherently non-deterministic, recovery can be done with
> relaxed guarantees).
>=20
>  2) The user should be able to "switch on" deterministic processing,
> ie, records are timestamped (either externally when generated, or
> timestamped at the sources). Because deterministic processing adds some
> overhead, the user should decide for it actively.
> In this case, the order must be preserved in each re-distribution step
> (merging is sufficient, if order is preserved within each incoming
> channel). Furthermore, deterministic processing can be achieved by sound
> window semantics (and there is a bunch of them). Even for
> single-stream-windows it's a tricky problem; for join-windows it's even
> harder. From my point of view, it is less important which semantics are
> chosen; however, the user must be aware how it works. The most tricky
> part for deterministic processing, is to deal with duplicate timestamps
> (which cannot be avoided). The timestamping for (intermediate) result
> tuples, is also an important question to be answered.
>=20
>=20
> -Matthias
>=20
>=20
> On 04/07/2015 11:37 AM, Gyula F=C3=B3ra wrote:
> Hey,
>=20
> I agree with Kostas, if we define the exact semantics how this works, this
> is not more ad-hoc than any other stateful operator with multiple inputs.
> (And I don't think any other system support something similar)
>=20
> We need to make some design choices that are similar to the issues we had
> for windowing. We need to chose how we want to evaluate the windowing
> policies (global or local) because that affects what kind of policies can
> be parallel, but I can work on these things.
>=20
> I think this is an amazing feature, so I wouldn't necessarily rush the
> implementation for 0.9 though.
>=20
> And thanks for helping writing these down.
>=20
> Gyula
>=20
> On Tue, Apr 7, 2015 at 11:11 AM, Kostas Tzoumas <ktzoumas@apache.org<mailto:ktzoumas@apache.org>> wrote:
>=20
> Yes, we should write these semantics down. I volunteer to help.
>=20
> I don't think that this is very ad-hoc. The semantics are basically the
> following. Assuming an arriving element from the left side:
> (1) We find the right-side matches
> (2) We insert the left-side arrival into the left window
> (3) We recompute the left window
> We need to see whether right window re-computation needs to be triggered as
> well. I think that this way of joining streams is also what the symmetric
> hash join algorithms were meant to support.
>=20
> Kostas
>=20
>=20
> On Tue, Apr 7, 2015 at 10:49 AM, Stephan Ewen <sewen@apache.org<mailto:sewen@apache.org>> wrote:
>=20
> Is the approach of joining an element at a time from one input against a
> window on the other input not a bit arbitrary?
>=20
> This just joins whatever currently happens to be the window by the time
> the
> single element arrives - that is a bit non-predictable, right?
>=20
> As a more general point: The whole semantics of windowing and when they
> are
> triggered are a bit ad-hoc now. It would be really good to start
> formalizing that a bit and
> put it down somewhere. Users need to be able to clearly understand and
> how
> to predict the output.
>=20
>=20
>=20
> On Fri, Apr 3, 2015 at 12:10 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com<mailto:gyula.fora@gmail.com>>
> wrote:
>=20
> I think it should be possible to make this compatible with the
> .window().every() calls. Maybe if there is some trigger set in "every"
> we
> would not join that stream 1 by 1 but every so many elements. The
> problem
> here is that the window and every in this case are very-very different
> than
> the normal windowing semantics. The window would define the join window
> for
> each element of the other stream while every would define how often I
> join
> This stream with the other one.
>=20
> We need to think to make this intuitive.
>=20
> On Fri, Apr 3, 2015 at 11:23 AM, M=C3=A1rton Balassi <
> balassi.marton@gmail.com<mailto:balassi.marton@gmail.com>>
> wrote:
>=20
> That would be really neat, the problem I see there, that we do not
> distinguish between dataStream.window() and
> dataStream.window().every()
> currently, they both return WindowedDataStreams and TriggerPolicies
> of
> the
> every call do not make much sense in this setting (in fact
> practically
> the
> trigger is always set to count of one).
>=20
> But of course we could make it in a way, that we check that the
> eviction
> should be either null or count of 1, in every other case we throw an
> exception while building the JobGraph.
>=20
> On Fri, Apr 3, 2015 at 8:43 AM, Aljoscha Krettek <
> aljoscha@apache.org<mailto:aljoscha@apache.org>>
> wrote:
>=20
> Or you could define it like this:
>=20
> stream_A =3D a.window(...)
> stream_B =3D b.window(...)
>=20
> stream_A.join(stream_B).where().equals().with()
>=20
> So a join would just be a join of two WindowedDataStreamS. This
> would
> neatly move the windowing stuff into one place.
>=20
> On Thu, Apr 2, 2015 at 9:54 PM, M=C3=A1rton Balassi <
> balassi.marton@gmail.com<mailto:balassi.marton@gmail.com>
>=20
> wrote:
> Big +1 for the proposal for Peter and Gyula. I'm really for
> bringing
> the
> windowing and window join API in sync.
>=20
> On Thu, Apr 2, 2015 at 6:32 PM, Gyula F=C3=B3ra <gyfora@apache.org<mailto:gyfora@apache.org>>
> wrote:
>=20
> Hey guys,
>=20
> As Aljoscha has highlighted earlier the current window join
> semantics
> in
> the streaming api doesn't follow the changes in the windowing
> api.
> More
> precisely, we currently only support joins over time windows of
> equal
> size
> on both streams. The reason for this is that we now take a
> window
> of
> each
> of the two streams and do joins over these pairs. This would be
> a
> blocking
> operation if the windows are not closed at exactly the same time
> (and
> since
> we dont want this we only allow time windows)
>=20
> I talked with Peter who came up with the initial idea of an
> alternative
> approach for stream joins which works as follows:
>=20
> Instead of pairing windows for joins, we do element against
> window
> joins.
> What this means is that whenever we receive an element from one
> of
> the
> streams, we join this element with the current window(this
> window
> is
> constantly updated) of the other stream. This is non-blocking on
> any
> window
> definitions as we dont have to wait for windows to be completed
> and
> we
> can
> use this with any of our predefined policies like Time.of(...),
> Count.of(...), Delta.of(....).
>=20
> Additionally this also allows some very flexible way of defining
> window
> joins. With this we could also define grouped windowing inside
> if
> a
> join.
> An example of this would be: Join all elements of Stream1 with
> the
> last
> 5
> elements by a given windowkey of Stream2 on some join key.
>=20
> This feature can be easily implemented over the current
> operators,
> so
> I
> already have a working prototype for the simple non-grouped
> case.
> My
> only
> concern is the API, the best thing I could come up with is
> something
> like
> this:
>=20
> stream_A.join(stream_B).onWindow(windowDefA,
> windowDefB).by(windowKey1,
> windowKey2).where(...).equalTo(...).with(...)
>=20
> (the user can omit the "by" and "with" calls)
>=20
> I think this new approach would be worthy of our "flexible
> windowing"
> in
> contrast with the current approach.
>=20
> Regards,
> Gyula
>=20
>=20
>=20
>=20
>=20
>=20
>=20
>=20
>=20


--nuS0cwlgqk0oE3FDfB4ver8ktVtGNLVRI
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJVJmK9AAoJEBXkotPFErDWBDwP/jPHanwaDScgV6I3YHdpxtvn
muUWDA3ey8b+h3tfIHhhiz6r7Ak1wcgLq6rVxF86YxfbfeET/h/+t5LovnCdmmGX
9uZxOnMN7jrgSDjr76AGfjg9nHPG3LcL7ysijbn1kybYjnwCKfZXMqGGawfNbDH6
ZwOsIR73MJY9qltFfW19fi3amwVW71jmXPyOFuHssoc8unL7JMnOB8AsviGwb4YI
eBgAHmLJ/B3nD4YccF2IGSf46zu67HfzDv5uI+T6RQyJWQ6SFQ0Z0jJfoxyHlRy0
oFKybnSUwmB9xg0j19F1RonVm6Y4GzdhhFVOnJsqbEMir6zpfSWAISFKSasDs0PB
8Ef8AM8V3ZaraimVive9KNMWFdWdDWVK9iAtrUavHI2By7nIQ+r2sQKBY2XNwGml
DrBOPw5vLfCLPDjDKZxCwKNCYfLIF8xXQbNybymDTT1sB9J1ypV1dwM+I7IxNGmQ
WEYH3sE/qcrvgYSGnXHhIEyTN2vX+d3BVtVLC+Q7jHocbeigwH0l3iv2pNh9zGUR
zjQeU1oqHyTpN1EClMzNGL5S5NWi76ZdYdplAyk6zVroC5pIlODP7iP2k5rYrOUl
QjfRfzvLbuCgXOwGJCBW7QKZrrU4lAUpm9vT2FYxNLuZuWM4AAHbkDjNlxwNuVBA
Mtxbq8mJA0tvQU/AQxMU
=mwEo
-----END PGP SIGNATURE-----

--nuS0cwlgqk0oE3FDfB4ver8ktVtGNLVRI--
#|#<8540F857-EFB8-4F78-8D83-2336AF4A419F@kth.se>##//##<5534F60B.2020508@apache.org>#|#2015-04-20-12:50:16#|#Timo Walther <twalthr@apache.org>#|#Re: Merge Python API#|#
+1

On 20.04.2015 14:49, Gyula FÃ³ra wrote:
> +1
>
> On Mon, Apr 20, 2015 at 2:41 PM, Fabian Hueske <fhueske@gmail.com> wrote:
>
>> +1
>>
>> 2015-04-20 14:39 GMT+02:00 Maximilian Michels <mxm@apache.org>:
>>
>>> +1 Let's merge it to flink-staging and get some people to use it.
>>>
>>> On Mon, Apr 20, 2015 at 2:21 PM, Kostas Tzoumas <ktzoumas@apache.org>
>>> wrote:
>>>
>>>> I'm +1 for this
>>>>
>>>> On Mon, Apr 20, 2015 at 11:03 AM, Robert Metzger <rmetzger@apache.org>
>>>> wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> The Python API pull request [1] has been open for quite some time
>> now.
>>>>> I was wondering whether we are planning to merge it or not.
>>>>> I took a closer look at the Python API a few weeks ago and I think we
>>>>> should merge it to expose it to our users to collect feedback.
>>>>> I hope by merging it, we'll find additional contributors for it and
>> we
>>>> get
>>>>> more feedback.
>>>>>
>>>>> Since it will be located in the "flink-staging" module and we'll mark
>>> it
>>>> as
>>>>> a beta component, there is not much risk that we break any existing
>>> code.
>>>>> Please give me some +1's if you want to merge the Python API PR.
>>>>> I'd like to merge it in the next 24 to 48 hours, depending on the
>>>> feedback
>>>>> I'm getting in this thread here.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> [1] https://github.com/apache/flink/pull/202
>>>>>
#|#<CA+faj9zxCRBngfrS+iZ4mpE1crwH=Tc24V24JP76p-Kod73OXA@mail.gmail.com>##//##<5551CA85.3090902@informatik.hu-berlin.de>#|#2015-05-12-09:43:18#|#"Matthias J. Sax" <mjsax@informatik.hu-berlin.de>#|#Re: [DISCUSS] Merging Storm compatibility to Flink-contrib#|#
--SJ4SenPK67O6GbQgPiORqqI42WLREJi5o
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi,

some UnsupportedOperationExceptions are required, because Storm
interfaces are implement but Flink cannot support those functionality.
Some other are "not yet implemented" once.

A few other of them could be removed (in case an interface in not
implemented, but only mimicked), by removing the whole method. I prefer
to mimic interfaced completely and through
UnsupportedOperationExceptions, because if a user wants to execute Storm
code on Flink, less changes to the original Storm code are necessary,
making the transition easier.

Of course, it is a valid argument to remove the methods completely to
raise incompatibilities directly are compile time.

The TODOs are points that might or might not be Flink compatible. This
must be checked and maybe discussed.

Any feedback is welcome.


-Matthias

On 05/12/2015 10:46 AM, Robert Metzger wrote:
> Hi,
>=20
> Thank you for starting the discussion Marton!
>=20
> I would really like to merge the storm compat to our source repo. I think
> that code which is not merged there will not get enough attention.
>=20
> I'm against splitting flink-contrib into small maven modules. I totally
> understand your reasoning (mixed dependencies), but "flink-staging" exists
> exactly for that purpose (one maven module per "beta"-module).
> For now, users depending on flink-contrib have to define exclusions to
> control the dependencies.
>=20
> So I'm +1 for merging it to "flink-contrib".
> I guess not all committers have time to look into the pull request,
> therefore, I want to remind you that the code contains a lot of TODOs and
> UnsupportedOperationExceptions.
>=20
> @Marton: Do you know anybody from the Budapest Flink Streaming crew with
> some Storm experience who could try out the code on a cluster and give some
> feedback?
>=20
>=20
> On Tue, May 12, 2015 at 9:52 AM, M=C3=A1rton Balassi <balassi.marton@gmail.com>
> wrote:
>=20
>> The purpose of flink-contrib currently is to hold contributions to the
>> project that we do not consider part of the core flink functionality, but
>> provide useful tools around it. In general code placed here has to meet
>> less requirements in terms of covering all corner cases if it provides a
>> nice solution for a set of well defined problems.
>>
>> As of today it has two small utilities, the TweetInputFormat (by Mustafa
>> Elbehery) and the collect functionality for the DataStream (by Gabor
>> Gevay).
>>
>> The pull request for the Storm compatibility layer (by Matthias J. Sax) [1]
>> raises the issue as it is way more code to maintain and is more complex in
>> general that how the community would like to handle these in terms of
>> distribution. Do we want to have it in the Flink repository or maybe in a
>> separate one.
>>
>> I am personally really for having the Storm compatibility layer under
>> flink-contrib as Matthias is very active on the mailing list and has also
>> expressed his interest of further developing the functionality of the
>> compatibility layer. To top that a couple of users got excited about the
>> new feature, so I see no risk in having this code in the main repository.
>>
>> As for the structure of flink-contrib I would have the contents separated
>> to slim as possible maven projects, to make sure that the users only get
>> the dependencies that they really need.
>>
>> [1] https://github.com/apache/flink/pull/573
>>
>=20


--SJ4SenPK67O6GbQgPiORqqI42WLREJi5o
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJVUcqFAAoJEBXkotPFErDWn88QAKrSAwzWognMBQSnzZTVEIY7
iwHrcBpZtqut3yuX2nVp2PgfcR1oFUUlxAyAGFNrBHGLtXNrTjf3WyJzbEVf5tWy
3L4cFK9fVRUkjhe25TxtG92gaCpeiBkAJ8XCyDrgV5L5Zp7jz0yylLeRp1m8DZEk
MuHvF9vk+AGd3RAMSVth8S2Cw8yI5wvmGJXzgs9+TZHhuhXoBg52GBtbkSc6Gfb9
ztH7nOb4uJtzvB6d7/8dSY5BqaTW1sM3JvjdWjIbvkoqoNKtcypsmXFG1eN87IGl
Awq8kEM1zN9xct1g40kCwiypBGNELC7kzqy1CAAvMbrFOMPbJtI6oVXPpm6IrmjX
MzCM8qgtYXJUS8TncIpNNxippxHcAndyxxMK8bM2lyMqwVBvp3ydJz1otbgd6bB/
QneEh5WWNqX62pFNJX65PLD6T/H3F450PuoTuGuyxZfiPOU5fRtLm5ZGXde8ahl9
5VhrLEt8zHUhNWY48CN89rUk+DjZTovkJn1dcp3eGWpVNz5IGMbTkXXZsaSDsERe
snbN3kK9cAQ5OHopXPFOrGFJicbidmbIc/+MEQkocdwL6wGAPWssEprsj6VgKlm/
KHRGoxZF1QvgSVhzSkST/RJD0Ig+KglCW+39Wr/R6sL+VMoow3wpAOMGn9SNyWgZ
ssshgmyTEENunMuje8b8
=0l4T
-----END PGP SIGNATURE-----

--SJ4SenPK67O6GbQgPiORqqI42WLREJi5o--
#|#<CAGr9p8Cpmq9+wTLdAycSex3g5gwEbkWxZJxDo3P+u63apPb+ww@mail.gmail.com>##//##<5558FC47.6060503@informatik.hu-berlin.de>#|#2015-05-17-20:48:07#|#"Matthias J. Sax" <mjsax@informatik.hu-berlin.de>#|#Re: Package multiple jobs in a single jar#|#
--6Tmc5nB79VOaIGfHHP2BI2E7xcbGGgl2r
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi,

I like the idea that Flink's WebClient can show different plans for
different jobs within a single jar file.

I prepared a prototype for this feature. You can find it here:
https://github.com/mjsax/flink/tree/multipleJobsWebUI

To test the feature, you need to prepare a jar file, that contains the
code of multiple programs and specify each entry class in the manifest
file as comma separated values in "program-class" line.

Feedback is welcome. :)


-Matthias


On 05/08/2015 03:08 PM, Flavio Pompermaier wrote:
> Thank you all for the support!
> It will be a really nice feature if the web client could be able to show
> me the list of Flink jobs within my jar..
> it should be sufficient to mark them with a special annotation and
> inspect the classes within the jar..
>=20
> On Fri, May 8, 2015 at 3:03 PM, Malte Schwarzer <ms@mieo.de
> <mailto:ms@mieo.de>> wrote:
>=20
>     Hi Flavio,
>=20
>     you also can put each job in a single class and use the =E2=80=93c parameter
>     to execute jobs separately:
>=20
>     /bin/flink run =E2=80=93c com.myflinkjobs.JobA /path/to/jar/multiplejobs.jar
>     /bin/flink run =E2=80=93c com.myflinkjobs.JobB /path/to/jar/multiplejobs.jar
>     =E2=80=A6
>=20
>     Cheers
>     Malte
>=20
>     Von: Robert Metzger <rmetzger@apache.org <mailto:rmetzger@apache.org>>
>     Antworten an: <user@flink.apache.org <mailto:user@flink.apache.org>>
>     Datum: Freitag, 8. Mai 2015 14:57
>     An: "user@flink.apache.org <mailto:user@flink.apache.org>"
>     <user@flink.apache.org <mailto:user@flink.apache.org>>
>     Betreff: Re: Package multiple jobs in a single jar
>=20
>     Hi Flavio,
>=20
>     the pom from our quickstart is a good
>     reference: https://github.com/apache/flink/blob/master/flink-quickstart/flink-quickstart-java/src/main/resources/archetype-resources/pom.xml
>=20
>=20
>=20
>=20
>     On Fri, May 8, 2015 at 2:53 PM, Flavio Pompermaier
>     <pompermaier@okkam.it <mailto:pompermaier@okkam.it>> wrote:
>=20
>         Ok, get it.
>         And is there a reference pom.xml for shading my application into
>         one fat-jar? which flink dependencies can I exclude?
>=20
>         On Fri, May 8, 2015 at 1:05 PM, Fabian Hueske <fhueske@gmail.com
>         <mailto:fhueske@gmail.com>> wrote:
>=20
>             I didn't say that the main should return the
>             ExecutionEnvironment.
>             You can define and execute as many programs in a main
>             function as you like.
>             The program can be defined somewhere else, e.g., in a
>             function that receives an ExecutionEnvironment and attaches
>             a program such as
>=20
>             public void buildMyProgram(ExecutionEnvironment env) {
>               DataSet<String> lines =3D env.readTextFile(...);
>               // do something
>               lines.writeAsText(...);
>             }
>=20
>             That method could be invoked from main():
>=20
>             psv main() {
>               ExecutionEnv env =3D ...
>=20
>               if(...) {
>                 buildMyProgram(env);
>               }
>               else {
>                 buildSomeOtherProg(env);
>               }
>=20
>               env.execute();
>=20
>               // run some more programs
>             }
>=20
>             2015-05-08 12:56 GMT+02:00 Flavio Pompermaier
>             <pompermaier@okkam.it <mailto:pompermaier@okkam.it>>:
>=20
>                 Hi Fabian,
>                 thanks for the response.
>                 So my mains should be converted in a method returning
>                 the ExecutionEnvironment.
>                 However it think that it will be very nice to have a
>                 syntax like the one of the Hadoop ProgramDriver to
>                 define jobs to invoke from a single root class.
>                 Do you think it could be useful?
>=20
>                 On Fri, May 8, 2015 at 12:42 PM, Fabian Hueske
>                 <fhueske@gmail.com <mailto:fhueske@gmail.com>> wrote:
>=20
>                     You easily have multiple Flink programs in a single
>                     JAR file.
>                     A program is defined using an ExecutionEnvironment
>                     and executed when you call
>                     ExecutionEnvironment.exeucte().
>                     Where and how you do that does not matter.
>=20
>                     You can for example implement a main function such as:
>=20
>                     public static void main(String... args) {
>=20
>                       if (today =3D=3D Monday) {
>                         ExecutionEnvironment env =3D ...
>                         // define Monday prog
>                         env.execute()
>                       }
>                       else {
>                         ExecutionEnvironment env =3D ...
>                         // define other prog
>                         env.execute()
>                       }
>                     }
>=20
>                     2015-05-08 11:41 GMT+02:00 Flavio Pompermaier
>                     <pompermaier@okkam.it <mailto:pompermaier@okkam.it>>:
>=20
>                         Hi to all,
>                         is there any way to keep multiple jobs in a jar
>                         and then choose at runtime the one to execute
>                         (like what ProgramDriver does in Hadoop)?
>=20
>                         Best,
>                         Flavio
>=20
>=20
>=20
>=20
>=20
>=20
>=20
>=20
>=20


--6Tmc5nB79VOaIGfHHP2BI2E7xcbGGgl2r
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJVWPxKAAoJEBXkotPFErDWLasQAKDu9jR39MCzdEbOyTpUSRbe
0ZOewaekXA0qoJTi34pImeQBD1f9E9v6ABPrpp775roZ4oq3F/lMsjbxxsLpu+Q9
csLRq4wgZjjLoYtQmcgISIznA+EkqmdShO4SJyYIvWfnTpu1ZeFjQqo9GSoZmjR/
k9/v0LIveINb7+Q3Mm7XeTqMMhiem9BiBwpDfmHb2GzImU9eB7MvuKCF9FRPXtgn
nmev7UgALGrywnmxUAA1KxYIKeIeH8NwB6RdieG942uLbIif16LSaPVK08WShWmL
VdNQr3HL5XIdQFcH0+xEdPygj7eZfBYFhvz551lbRq7m1sfmKP1SQqnTAwFJTwci
q7lD1mmwJ0y3Ejdr0sIZF8oUwoNA6+rAc2FlQZmTPgayCyfUNlRUWhGtDRU43sg6
vrIESM+szeNl10hMu3f0K4wkIkdyI63gbL2RB6LO0eaAzahkmh2WE8sPCM07UdiJ
QE3y/Iw9EUnj5hD8PEZl4A1Gj7UMeDHp9qBd2biKSCm3sQt6NASxEOkIpD9aEZIg
8H0Ju/vfTqPYhV/oh7iwwJ+YUAaTAS/CbiOPHFtsb7hJQlM3tFQrUy3v2tn7UUth
snTkBspO3ODKG0CWLyNa8+pBxWNEs6xHizQTWK8p27/5PE9soJkzTRhXijj/C+m0
ELIPFqDcpWAycHxVMXy2
=XWyW
-----END PGP SIGNATURE-----

--6Tmc5nB79VOaIGfHHP2BI2E7xcbGGgl2r--
#|#<CAELUF_DC=2ry-Px2Js9+duYwj1W=sGmsucPyCojucCCELE47ew@mail.gmail.com>##//##<55701FFB.6060606@informatik.hu-berlin.de>#|#2015-06-04-09:56:06#|#"Matthias J. Sax" <mjsax@informatik.hu-berlin.de>#|#Re: Failing tests policy#|#
--GsAJBlkIv5gqSi5Q6BE1qA0MpSJPE3s5J
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

I have another idea: the problem is, that some commit might de-stabilize
a former stable test. This in not detected, because the build was
("accidentally") green and the code in merged.

We could reduce the probability that this happens, if a pull request
must pass the test-run multiple times (maybe 5x). Of course, it takes
much time to run all test on Travis such often and increases the time
until something can be merged. But it might be worth the effort.

Options on that?

On 06/04/2015 11:35 AM, Ufuk Celebi wrote:
> Thanks for the feedback and the suggestions.
>=20
> As Stephan said, the "we have to fix it asap" usually does not work well. I think blocking master is not an option, exactly for the reasons that Fabian and Till outlined.
>=20
> From the comments so far, I don't feel like we are eager to adapt a disable policy.
>=20
> I still think it is a better policy. I think we actually don't decrease test coverage by disabling a flakey test, but increase it. For example the KafkaITCase is in one of the modules, which is tested in the middle of a build. If it fails (as it does sometimes), a lot of later tests don't run. I'm not sure if we have the time (or discipline) to trigger a 1hr build again when a known-to-fail test is failing and 4 of the other builds are succeeding.
>=20
> =96 Ufuk
>=20
> On 04 Jun 2015, at 09:25, Till Rohrmann <till.rohrmann@gmail.com> wrote:
>=20
>> I'm also in favour of quickly fixing the failing test cases but I think
>> that blocking the master is a kind of drastic measure. IMO this creates a
>> culture of blaming someone whereas I would prefer a more proactive
>> approach. When you see a failing test case and know that someone recently
>> worked on it, then ping him because maybe he can quickly fix it or knows
>> about it. If he's not available, e.g. holidays, busy with other stuff,
>> etc., then maybe one can investigate the problem oneself and fix it.
>>
>> But this is basically our current approach and I don't know how to enforce
>> this policy by some means. Maybe it's making people more aware of it and
>> motivating people to have a stable master.
>>
>> Cheers,
>> Till
>>
>> On Thu, Jun 4, 2015 at 9:06 AM, Matthias J. Sax <
>> mjsax@informatik.hu-berlin.de> wrote:
>>
>>> I think, people should be forced to fixed failing tests asap. One way to
>>> go, could be to lock the master branch until the test is fixed. If
>>> nobody can push to the master, pressure is very high for the responsible
>>> developer to get it done asap. Not sure if this is Apache compatible.
>>>
>>> Just a thought (from industry experience).
>>>
>>>
>>> On 06/04/2015 08:10 AM, Aljoscha Krettek wrote:
>>>> I tend to agree with Ufuk, although it would be nice to fix them very
>>> quickly.
>>>>
>>>> On Thu, Jun 4, 2015 at 1:26 AM, Stephan Ewen <sewen@apache.org> wrote:
>>>>> @matthias: That is the implicit policy right now. Seems not to work=2E..
>>>>>
>>>>> On Thu, Jun 4, 2015 at 12:40 AM, Matthias J. Sax <
>>>>> mjsax@informatik.hu-berlin.de> wrote:
>>>>>
>>>>>> I basically agree that the current policy on not optimal. However, I
>>>>>> would rather give failing tests "top priority" to get fixed (if
>>> possible
>>>>>> within one/a-few days) and not disable them.
>>>>>>
>>>>>> -Matthias
>>>>>>
>>>>>> On 06/04/2015 12:32 AM, Ufuk Celebi wrote:
>>>>>>> Hey all,
>>>>>>>
>>>>>>> we have certain test cases, which are failing regularly on Travis=2E In
>>> all
>>>>>>> cases I can think of we just keep the test activated.
>>>>>>>
>>>>>>> I think this makes it very hard for regular contributors to take these
>>>>>>> failures seriously. I think the following situation is not unrealistic
>>>>>> with
>>>>>>> the current policy: I know that test X is failing. I don't know that
>>>>>> person
>>>>>>> Y fixed this test. I see test X failing (again for a different reason)
>>>>>> and
>>>>>>> think that it is a "known issue".
>>>>>>>
>>>>>>> I think a better policy is to just disable the test, assign someone to
>>>>>> fix
>>>>>>> it, and then only enable it again after someone has fixed it.
>>>>>>>
>>>>>>> Is this reasonable? Or do we have good reasons to keep such tests
>>> (there
>>>>>>> are currently one or two) activated?
>>>>>>>
>>>>>>> =96 Ufuk
>>>>>>>
>>>>>>
>>>>>>
>>>>
>>>
>>>
>=20
>=20


--GsAJBlkIv5gqSi5Q6BE1qA0MpSJPE3s5J
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2

iQIcBAEBCAAGBQJVcB//AAoJEBXkotPFErDWlf0P/114Rj2FGCy8R8pxhPg9kvwo
gS/aOsA3s5RUhvvfLuxqatrymTUtDv/Vtef1EF2UBZIx4DKKQhH6Wv42onRRT42V
T5jzKQYgyWPK4+YBLoDcc9uWUJw/2ry7AO+hg27VyW2mFwQ8d5vkmRUrxi1bRZr9
RlWcqZy4L8nPS7G54c/Eq7QPPiEDKL/54IlkQOqEFdjnYeZLEo/mlQRd8SfiFCXA
T/Zdo6Lil3ATHI0n/itOQhfKf2uGpu5Ku9NTWn0CXyWtFqLq7jCZEIzCh6Xe7ZOC
xF6i/QHTwCEB1fiQF5bigiUxJnD0zwSH6UEN/xMc5hu4j6pT1vTV6O0BUo7eN5qa
zPohlLjvKOOWIUJyEV7p05ZRVDSlcx7KwBjoQnveNUWPZcMSvtO05SXwC1mKOAwh
ba4BObczNQ94+4qG+J0vk62oJTtN7tMi/2Liyw4jbf2x1YeclzrVV17YrP/TA2tH
EZuE9LAFhn5LaEGorGZK4CdSQW934bRK17w2VAjfnkqHCRkMG4rCuMHUVrlkdmAB
+S7gkbho8aHCVb53S76CWF1U4Ahpq0/f8wZK0HZ9vn4+BMB8T7J4ZwepKtJnYjKF
EqUEJ4gFEelvLVVfTwx0HYEaX5KcvYEH2Qp8j/giaejgSxwTFlKmCFgOeJmL+Azv
t+MhKi4BKsO021s4i+D/
=QPWE
-----END PGP SIGNATURE-----

--GsAJBlkIv5gqSi5Q6BE1qA0MpSJPE3s5J--
#|#<B7094EE3-51AD-46F3-852B-B6F6990B2ECA@apache.org>##//##<62A04662-F895-4CCF-A4C4-A40672BF9E3F@apache.org>#|#2015-04-10-10:13:08#|#Ufuk Celebi <uce@apache.org>#|#[DISCUSS] Spargel vs. Gelly#|#
Hey all,

currently we have two vertex-centric graph APIs: Spargel and Gelly. I want to discuss whether we shall
1) keep Spargel as a public API as it is, or
2) deprecate (and remove) it.

In my understanding, Spargel was a proof-of-concept, which stuck around. It is very stable, but limited in functionality. Gelly provides a superset of Spargel's functionality and a high-level library of graph algorithms. The vertex-centric iterations actually wrap Spargel.

I am in favour of 2):

+ Less confusing and less work to have two APIs for the same thing (we have to communicate this, document it etc.)
+ Gelly is actively maintained and getting a lot of contributions
- Spargel users will have to move to Gelly at some point in time (I think this will happen anyways and it should be straight forward)

The Spargel internal code will probably stick around as part of Gelly. The question is whether we want to have two public APIs for this.

=96 Ufuk#|#null##//##<7CC35317-5FEE-4AB3-9341-8CE9295F5221@kth.se>#|#2014-10-03-16:59:57#|#Paris Carbone <parisc@kth.se>#|#Re: Hackathon Stockholm - Agenda and Topics#|#
Hey sorry just saw the mail. I have a very unstable connection in the Baltic Sea :( . The rooms are:

8th : Knuth(sics) + Ada(KTH)
9-10th : Ada(KTH)

Both rooms are in the electrum building. You can reach SICS at the 6th floor preferably through elevator B where the reception can open for you. KTH Ada is at the 4th floor through elevator A, there I can open for you.

Paris

> On 03 Oct 2014, at 18:23, Stephan Ewen <sewen@apache.org> wrote:
>=20
> Yes please. Can you send the exact address/building/rooms also to the dev
> list? I don't know them ;-)
>=20
> On Fri, Oct 3, 2014 at 6:19 PM, Vasiliki Kalavri <vasilikikalavri@gmail.com>
> wrote:
>=20
>> Hi,
>>=20
>> looks good!
>> Shall we also send this to the list of participants (in case they're not
>> subscribed here)?
>> And also add location information, i.e. the rooms we have booked at
>> SICS/KTH?
>>=20
>> -V.
>>=20
>>> On 3 October 2014 17:09, Aljoscha Krettek <aljoscha@apache.org> wrote:
>>>=20
>>> I would also participate in the hacking, although from Germany and
>>> connected via Skype or some such thing. :D
>>>=20
>>>> On Fri, Oct 3, 2014 at 4:21 PM, Stephan Ewen <sewen@apache.org> wrote:
>>>> Hello everyone!
>>>>=20
>>>> For the hackathon Stockholm next week (at KTH / SICS, Oct 8th - 9th),
>>> here
>>>> is a suggestion for a rough agenda and a list of topics to work upon or
>>>> look into. Suggestions and more topics are welcome.
>>>>=20
>>>>=20
>>>> Wednesday (8th)
>>>> --------------------------
>>>>=20
>>>> 9:00 - 10:00  Introduction to Apache Flink, System overview, and Dev
>>>> environment (by Stephan)
>>>>=20
>>>> 10:15 - 11:00 Introduction to the topics (Streaming API and system by
>>> Gyula
>>>> & Marton), (Graphs by Vasia / Martin / Stephan)
>>>>=20
>>>>=20
>>>> 11:00 - 12:30 Happy hacking (part 1)
>>>>=20
>>>> 12:30 - Lunch (Food will be provided by KTH / SICS. A big thank you to
>>> them
>>>> and also to Paris, for organizing that)
>>>>=20
>>>> 13:xx - Happy hacking (part 2)
>>>>=20
>>>> 6:00 pm - Hadoop User Group Meetup @ Spotify , for those who are
>>>> interested. (http://www.meetup.com/stockholm-hug/events/207323222/)
>>>>=20
>>>>=20
>>>> Thursday (9th)
>>>> --------------------------
>>>>=20
>>>> Happy hacking (continued)
>>>>=20
>>>>=20
>>>>=20
>>>> Ideas for topics
>>>> -----------------------------
>>>>=20
>>>> Streaming:
>>>>=20
>>>>  - Sample streaming applications (e.g. continuous heavy hitters and
>>> topics
>>>> on the twitter stream)
>>>>=20
>>>>  - Implement a simple SQL to Streaming program parser. Possibly using
>>>> Apache Calcite (http://optiq.incubator.apache.org/)
>>>>=20
>>>>  - Implement different windowing methods (count-based, time-based,
>> ...)
>>>>=20
>>>>  - Implement different windowed operations (windowed-stream-join,
>>>> windowed-stream-co-group)
>>>>=20
>>>>  - Streaming state, and interaction with other programs (that access
>>> state
>>>> of a stream program)
>>>>=20
>>>>=20
>>>>=20
>>>> Graph Analysis
>>>> ----------------------------
>>>>=20
>>>>  - Prototype a Graph DSL (simple graph building, filters, graph
>>>> properties, some algorithms)
>>>>=20
>>>>  - Prototype abstractions different Graph processing paradigms
>>>> (vertex-centric, partition-centric).
>>>>=20
>>>>  - Generalize the delta iterations, allow flexible state access.
>>>>=20
>>>>=20
>>>> Feel free to comment!
>>>>=20
>>>>=20
>>>> Greetings,
>>>> Stephan
>>>=20
>>=20
#|#<CANC1h_ttLrkPuQix5get-z=ZGDPn_HGLOEJn81_2ng0EF_ih5w@mail.gmail.com>##//##<979DB9666496EA4AA881B4E447D3040001B2890E@MXMA2012.hpi.uni-potsdam.de>#|#2014-11-18-12:25:46#|#"Kruse, Sebastian" <Sebastian.Kruse@hpi.de>#|#RE: Heartbeat lost#|#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#|#<CANC1h_uoQHzZ09bGBrKg+pEiDvrz2PnMP=fk2LcTtzAyMbvXVw@mail.gmail.com>##//##<A7C1B78B-7002-420D-8EAB-18BE172CFEED@apache.org>#|#2015-06-03-00:11:05#|#Ufuk Celebi <uce@apache.org>#|#Re: Send events to parallel operator instances#|#

On 02 Jun 2015, at 22:45, Gyula F=F3ra <gyfora@apache.org> wrote:
> I am wondering, what is the suggested way to send some events directly to
> another parallel instance in a flink job? For example from one mapper to
> another mapper (of the same operator).
>=20
> Do we have any internal support for this? The first thing that we thought
> of is iterations but that is clearly an overkill.

There is no support for this at the moment. Any parallel instance? Or a subtask instance of the same task?

Can you provide more input on the use case? It is certainly possible to add support for this.

If the events don't need to be inline with the records, we can easily setup the TaskEventDispatcher as a separate actor (or extend the task manager) to process both backwards flowing events and in general any events that don't need to be inline with the records. The task deployment descriptors need to be extended with the extra parallel instance information.

=96 Ufuk#|#<CA+faj9zq6=iUW=-z7+zbXpg5bFvZusucp5UQA9Y7v78Yyb4BiQ@mail.gmail.com>##//##<A7CD463E-54CE-4E6F-A1AF-BB5E69770783@apache.org>#|#2015-06-05-10:51:44#|#Ufuk Celebi <uce@apache.org>#|#Re: Planning the 0.9 Release#|#

On 05 Jun 2015, at 11:46, Robert Metzger <rmetzger@apache.org> wrote:

> I'll address the remaining documentation issues today.

Thank you so much for doing this.

> What about
>   Sync Streaming Java/Scala API
>   - Consolidate names across batch/streaming (discussion)
>   - Merge static code analysis
>=20
> They seem both unresolved.

As I've said in the previous emails, I'm testing the code analysis. It looks good modulo some minor issues, which I will address and merge.

> Other than that it seems we are good to go.
> Maybe we can manage to get the first release candidate out today?

We need

1) to do some initial testing (with the recent streaming changes), and
2) I think the ExecutionGraph deadlock should be fixed (we know exactly what it is).

Regarding 1): it does not make a difference whether we do this on master or a RC. RC means more overhead to get this started.

Regarding 2): if more people agree that this should be fixed, then this is blocking the RC.
#|#<CAGr9p8DVH56sQVwSTXmUuyhd1hHa+u_C=ay=3tk3NXOs3Jcfog@mail.gmail.com>##//##<B3CE1943-35A9-44AB-A4C8-F2447E6EEEEB@icloud.com>#|#2015-03-22-00:56:52#|#Chiwan Park <chiwanpark@icloud.com>#|#Re: Subscription to mailing list#|#
--Apple-Mail=_4D60B2B8-C814-429D-867E-3164B56B42B5
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Hi,

You can subscribe to the mailing list by sending a email to dev-subscribe@flink.apache.org <mailto:dev-subscribe@flink.apache.org>.
Other mailing lists about Flink are in http://flink.apache.org/community.html#mailing-lists <http://flink.apache.org/community.html#mailing-lists>

Regards.
Chiwan Park (Sent with iPhone)



> On Mar 22, 2015, at 3:46 AM, Devesh Gade <deveshgade152003@gmail.com> wrote:
>=20
> Hi,
>=20
> I would like to subscribe to the Apache Flink developer mailing list.
>=20
> Regards,
> Devesh Gade.
>=20
> --=20
> Tough times dont last,Tough People Do.


--Apple-Mail=_4D60B2B8-C814-429D-867E-3164B56B42B5--
#|#null##//##<B668B5C4-4C6D-4899-8BA1-81976036B553@icloud.com>#|#2014-11-09-12:25:59#|#Chiwan Park <chiwanpark@icloud.com>#|#Re: beginner#|#
--Apple-Mail=_F84B0277-3D6C-4F27-ACDC-3BD00398D1E9
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Hello.

There is a help article for new contributor.
http://flink.incubator.apache.org/how-to-contribute.html <http://flink.incubator.apache.org/how-to-contribute.html>

If you read this first, you can learn contributing to Flink project.


=E2=80=94
Chiwan Park (Sent with iPhone)



> On Nov 9, 2014, at 9:12 PM, navya sri nizamkari <navyasri.tech@gmail.com> wrote:
>=20
> Hello,
>=20
> I want to learn contributing to Open SOURCE through your organisation.Can
> somebody please help me out?
>=20
> Thank you


--Apple-Mail=_F84B0277-3D6C-4F27-ACDC-3BD00398D1E9--
#|#null##//##<B738204C-EB87-4477-B60D-3C43D69265FB@apache.org>#|#2015-05-27-16:06:47#|#Ufuk Celebi <uce@apache.org>#|#Re: SQL on Flink#|#

On 27 May 2015, at 17:05, Timo Walther <twalthr@apache.org> wrote:

> It's rather passion for the future of the project than passion for SQL ;-)
>=20
> I always try to think like someone from the economy. And IMO the guys from economy are still thinking in SQL. If you want to persuade someone coming from the SQL world, you should offer a SQL interface to run legacy code first (similar to Hadoop operators). Rewriting old queries in Table API is not very convenient.
>=20
> I share Stephans opinion. Building both APIs concurrently would act as a good source to test and extend the Table API. Currently, the Table API is half-done, but I think the goal is to have SQL functionality. I can implement an SQL operator and extend the Table API if functionality is missing.

Very exiting! :-) +1

As suggested, I think the best thing is to do this hand-in-hand with the Table API. I don't think that there was any real disagreement. Everyone agrees that the SQL layer should be built on top of the Table API, which is great for both the Table API and the SQL layer. :-)
#|#<5565DD26.4040905@apache.org>##//##<BA5C40FE-3778-47A3-AFDC-B4F3F7298315@gmail.com>#|#2015-04-30-19:02:25#|#Robert Metzger <metrobert@gmail.com>#|#Re: Gzip support#|#
There is already support for inflate compressed files and I introduced logic to handle unsplittable formats.


Sent from my iPhone

> On 30.04.2015, at 19:39, Stephan Ewen <sewen@apache.org> wrote:
>=20
> I think that would be very worthwhile :-) Happy to hear that you want to
> contribute that!
>=20
> Decorating the input stream sounds like a great approach and would also
> work for other compression formats.
>=20
> The other thing that needs to be taken into account is that GZIP files are
> not splittable in the same way as uncompressed files. You may have to
> invent something clever there, or simply restrict the format to have one
> input split per file (rather than block).
>=20
> On Thu, Apr 30, 2015 at 5:41 PM, Kruse, Sebastian <Sebastian.Kruse@hpi.de>
> wrote:
>=20
>> Hi everyone,
>>=20
>> I just recently came across a use-case where I needed to read gzip files
>> and handle byte order marks transparently. I know that gzip can be read
>> with Hadoop input formats but that did not work for me since I wanted to
>> reuse my existing custom Flink input formats.
>>=20
>> It turned out that both requirements (and more) can be dealt with by
>> allowing the input formats to decorate the input stream. Do you think it is
>> worthwhile to include these changes in Flink? I could take care of it.
>>=20
>> Cheers,
>> Sebastian
>>=20
#|#<CANC1h_tOtXb1ek1gpEuvFAhYh+1F6eqOiXWMNQ8_RqnaOwr1Yg@mail.gmail.com>##//##<BDFF2CB5-F6C7-4D3E-A553-0DCF2D0D9A46@gmail.com>#|#2014-09-29-16:47:37#|#Gyula Fora <gyula.fora@gmail.com>#|#Re: Clean up dependencies in streaming connectors#|#
Thanks, I will look into this and try to figure it out, as you can see I am not a maven pro :)

On 29 Sep 2014, at 18:44, Stephan Ewen <sewen@apache.org> wrote:

> You may be able to solve this with careful exclusions.
>=20
> It seems kafka is monolithic, having no separation between connector and
> engine. If you know for example that zookeeper is not required by the
> connector (you have to be sure), you can exclude it as the dependency. We
> have done this for Hadoop1, where we only use the HDFS client functionality.
>=20
> On Mon, Sep 29, 2014 at 6:40 PM, Gyula F=F3ra <gyula.fora@gmail.com> wrote:
>=20
>> Yes, you are right, kafka and flume are the heavy ones.
>>=20
>> We always have the choice to take out them from the package and maybe have
>> a separate repo for all the different connectors and only keep 1-2 most
>> important ones. I don't think there's much else to do because we don't use
>> the packages you mentioned, but they get pulled by the kafka and flume
>> dependencies.
>>=20
>>=20
>>=20
>>=20
>> On Mon, Sep 29, 2014 at 6:24 PM, Stephan Ewen <sewen@apache.org> wrote:
>>=20
>>> The streaming connectors currently pull a massive amount of dependencies.
>>>=20
>>> For example, we transitively get the scala compiler/reflection/etc and
>>> ZooKeeper.
>>>=20
>>> A lot of stuff comes with flume and kafka. Are those required to make the
>>> connectors work? Otherwise, it might be good to exclude them, to prevent
>>> conflicts for users that actually depend on those components.
>>>=20
>>=20
#|#<CANC1h_s-kHEyjwDsaks9cp3s_742CPmeUv5fyEd8-5=3tMb4ww@mail.gmail.com>##//##<BLU436-SMTP23094D66CA4EC135CC7B36FDEF90@phx.gbl>#|#2015-04-11-16:02:17#|#Yi ZHOU <zhouyi0922@hotmail.com>#|#[Gelly] question about vertex-centric Iteration#|#
Hello,

In vertex-centric Iteration, in each iteration, the vertices sends and
recieves messages once.
However, to implement Affinity Propogation algortihm, I need to update
responsibility and availability in one iteration(superstep):
1, each vertex send and recieves messages from neigbor vertices to
update the responsibility.
2, each vertex sends and recieves messages from neigbor vertices to
update the availability.

That means, I need to send->update->send->update in each super step. How
can i achieve this ?  Does any have some suggestions?
Also, I saw the setNewVertexValue() in VertexUpdater , can I update the
value of adjacent edges in VertexUpdater?
#|#null##//##<C06C8DE5-7218-456E-8625-C5C8CC6E4DB5@gmail.com>#|#2014-11-27-10:02:44#|#Gyula Fora <gyula.fora@gmail.com>#|#Re: Channel indexing with pointwise connection pattern#|#
Thanks Stephan,

So I took a quick look at the ChannelSelectors the batch api uses and I see that for Forward strategy uses round-robin. My question was aimed exactly to avoid having to do this. Isn=E2=80=99t this sub-optimal?=20
Maybe we could pass the channel info to the channel selector, so it can make =E2=80=9Csmarter=E2=80=9D decision.

Gyula

> On 27 Nov 2014, at 10:45, Stephan Ewen <sewen@apache.org> wrote:
>=20
> This is a bit tricky, since the new scheduling is more flexible...
>=20
> Assume we have a PointWise connection with two receiving tasks per sending
> task: outgoing channels 0 and 1.
>=20
> When scheduling in the most basic mode, the receivers can go anywhere, but
> the schedule will try to give them a slot on the same instance, if
> possible. Both could be in-memory, but both could be remote.
>=20
> When using a SlotSharingGroup (we do that by default right now), one of the
> receivers can share the slot of the sender, but not both. Which one does
> depends wich one stays first. Currently that is the one which gets data
> first, but this is going to change soon with the new channels and
> deployment.
>=20
> When you use a CoLocationGroup, you are guaranteed that subtasks n of the
> sender is Co-located with subtask n of the receiver. But in the above
> PointWise model, you would rather want subtask n to be co-located with
> subtask 2*n.
>=20
> I don't think there is a reliable way to guarantee that, other than having
> a slightly modified version of the co-location constraint.
>=20
> Stephan
> Am 27.11.2014 00:45 schrieb "Gyula Fora" <gyula.fora@gmail.com>:
>=20
>> Hey,
>>=20
>> I was hoping that someone can answer this right away without me having to
>> dig through all the code :)
>>=20
>> How does the channel indexing go when more then one consumer subtask is
>> connected to an intermediate dataset in the pointwise pattern?
>> I am trying to figure out which one is the =E2=80=9Cin-memory=E2=80=9D channel to set up
>> proper partitioning for streams in this case. The idea would be to push the
>> majority to the in memory channel while still push some messages to the
>> network channel to leverage operator parallelism, but to implement this I
>> need to figure out the index of the in-memory channel.
>>=20
>> Thank you,
>> Gyula
#|#<CANC1h_ts4eGZ+VojXZgRrnp477k4syL+cZXkG6XDdRKCn+OwkQ@mail.gmail.com>##//##<CA+faj9wFD+=hJ+PH34MPmf5FuF-DOv8mvNXqw6n8Dsk=yiabyg@mail.gmail.com>#|#2015-01-05-09:46:52#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Streaming temporal operator (join, cross...) syntax#|#
--047d7b86d2ae11261b050be4900e
Content-Type: text/plain; charset=UTF-8

Hey guys,

We have been discussing the possible syntaxes for doing temporal operators
on DataStreams(join, corss, cogroup etc) with Paris and we have come up
with two alternatives.

1.

ds1.join(ds2).onWindow(5, seconds).every(2, seconds).where(...).equalTo(...)

2.

ds1.connect(ds2).onWindow(5, seconds).every(2,
seconds).join().where(...).equalTo(...)



Basically the difference is that in the second case we produce a binary
stream of 2 types by the connect method and we create a window on that
before join/cross. While in the first case the join/cross/etc is the method
of the DataStream itself and we define the window after calling
join/cross/etc

We currently have the first one.

Which one do you think is the more intuitive? (Or propose an alternative)

Cheers,
Gyula & Paris

--047d7b86d2ae11261b050be4900e--
#|#null##//##<CA+faj9wnhKtv2QXz=bEdBbbd6BNhH5O1pnCp+C_23B6r0R+zaQ@mail.gmail.com>#|#2015-03-24-11:01:44#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Re: [GSoc][flink-streaming] Interested in pursuing FLINK-1617 and FLINK-1534#|#
--f46d043c82561a6c50051206b5ca
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hey,

Give me an hour or so as I am in a meeting currently, but I will get back
to you afterwards.

Regards,
Gyula

On Tue, Mar 24, 2015 at 11:03 AM, Akshay Dixit <akshaydixi@gmail.com> wrote:

> Hi,
> It'd really help if I got a reply soon. It'll be helpful in writing the
> proposal since the deadline is on 27th. Thanks
> Regards,
> Akshay Dixit
>
> On Sun, Mar 22, 2015 at 1:17 AM, Akshay Dixit <akshaydixi@gmail.com>
> wrote:
>
> > Thanks for the explanation Marton. I've decided to try out for
> FLINK-1534.
> >
> > After reading through the thesis[4] and a few other papers[1][2][3], I
> > believe I've gathered a little context to ask more questions. But I'm
> still
> > not sure how Flink's internals work
> > so please bear with me. Although the ongoing effort to document the
> > architecture and internal is really helpful for newbies like me and would
> > greatly decrease the ramping up time.
> >
> > Detecting a pattern of events would comprise of a pipeline that accepts
> > the pattern query and
> > sources of DataStreams, and outputs detected matches of that pattern to a
> > sink or forwards it
> > along to another stream for further computation.
> >
> > As you said, a simple filter-join-aggregate query system could be
> > developed implementing using the existing Streaming windowing API.
> > But matching over complex events and decoding their pattern queries would
> > require implementing a DSL that transforms queries into an evaluation
> > model. For e.g,
> > in [1], the authors have implemented an NFA automaton with a shared
> > versioned buffer that models the queries. In [4], the authors
> > propose a new language that is much more expressive and compiles into a
> > topology graph for Storm.
> >
> > So in Flink's case, I believe the proposed DSL would generate operator
> > graphs for the Flink compiler to schedule Jobgraphs over TaskManagers.
> > If we don't depend on the Windowing API, would we need to create new
> > operators such as the Projection, Conjunction and Union operators defined
> > in [4] ?
> > Also I would like to hear your thoughts on how to approach scaling the
> > pattern matching query. Note all these techniques talk about scaling a
> > single query.
> > I've read various ways such as
> >
> > 1.  Merging equivalent runs[1] -: This seems a good way to squash
> multiple
> > instances of pattern matching forks into a single one if they have the
> same
> > state.
> > But I'm not sure how we would implement this in Flink since this is a
> > runtime optimization.
> >
> > 2.  Implementing a matched version buffer[1] -: This would involve
> sharing
> > state of a buffer datastructure across multiple candidate match instances
> > for the pattern.
> >
> > 3.  Splitting complex composite patterns into simpler sub-patterns[4] and
> > executing separate queries to detect those sub-patterns. This might
> > translate into different
> > tasks and duplicating the source datastreams to all the new generated
> > tasks.
> >
> > Also since I don't know how the Flink compiler behaves, would some of the
> > optimizations involve making changes to it too?
> >
> > Regards,
> > Akshay Dixit
> >
> > [1] : Efficient Pattern Matching over Event Streams
> > <http://people.cs.umass.edu/~yanlei/publications/sase-sigmod08-long.pdf>
> > [2] : On Supporting Kleene Closure over Event Streams
> > <http://people.cs.umass.edu/~yanlei/publications/sase-icde08.pdf>
> > [3] : Processing Flows of Information: From Data Stream to Complex Event
> > Processing
> > <
> http://citeseerx.ist.psu.edu/viewdoc/download?doi=3D10.1.1.396.1785&rep=3Drep1&type=3Dpdf
> >
> > [4] : Distributing Complex Event Detection
> > <http://www.doc.ic.ac.uk/teaching/distinguished-projects/2012/k.nagy.pdf
> >
> >
> > On Mon, Mar 16, 2015 at 3:22 PM, M=C3=A1rton Balassi <
> balassi.marton@gmail.com>
> > wrote:
> >
> >> Dear Akshay,
> >>
> >> Thanks again for your interest and for the recent contribution to
> >> streaming.
> >>
> >> Both of the projects mentioned wold be largely appreciated by the
> >> community, and you can also propose other project suggestions here for
> >> discussion.
> >>
> >> Regarding FLINK-1534, the thesis I mentioned serves as a starting point
> >> and
> >> indeed the basic solution can be implemented with filtering and
> >> windowing/mapping with some state storing whether the cause of an event
> >> has
> >> been already seen. Solely relying on the now existing windowing API this
> >> however might cause performance issues if the events also have an
> >> expiration timeout - some optimization there would be included. The
> >> further
> >> challenge is to try to further exploit the parallel job execution of
> Flink
> >> to possibly scale a pattern matching query.
> >>
> >> Best,
> >>
> >> Marton
> >>
> >> On Sun, Mar 15, 2015 at 3:22 PM, Akshay Dixit <akshaydixi@gmail.com>
> >> wrote:
> >>
> >> > Hi,
> >> > I'm Akshay Dixit[1], a 4th year undergrad at VIT Vellore, India. I'm
> >> > currently interested in distributed systems and stream processing and
> am
> >> > looking to delve deeper into the subject, and hope to get some insight
> >> by
> >> > contributing to Apache Flink. I've gathered some idea of the
> >> > flink-streaming codebase by recently working on a PR for
> FLINK-1450[2].
> >> >
> >> > Both FLINK-1617[3] and FLINK-1534[4] are interesting projects that I
> >> would
> >> > love to work on over the summer. I was wondering which amongst these
> >> would
> >> > be more appreciated by the community, so I can start working towards a
> >> > proposal for either one.
> >> >
> >> > Regarding FLINK-1534, I was wondering why would simply merging and
> >> > filtering the existing streams for events we want to detect not work?
> >> Also
> >> > on going through the document mentioned by @mbalassi in the JIRA
> >> > comment[5], the authors specify some Runtime Event Detection concepts
> in
> >> > Section 5.2. I'm assuming the project entails on building a similar
> >> analogy
> >> > using Flink and the deliverables would include working pattern
> matching
> >> > operators over Flink DataStreams as described in the report. If so,
> then
> >> > shouldn't it be trivial to implement the described the Binary operator
> >> > using a WindowedStream and a Filter?
> >> > I hope my questions don't seem misplaced here and I would appreciate
> >> links
> >> > to literature where I can learn more on the topic.
> >> >
> >> > Regards,
> >> > Akshay Dixit
> >> >
> >> > [1] : http://akshaydixi.me
> >> > [2] : https://github.com/apache/flink/pull/481
> >> > [3] : https://issues.apache.org/jira/browse/FLINK-1617
> >> > [4] : https://issues.apache.org/jira/browse/FLINK-1534
> >> > [5] :
> >> >
> http://www.doc.ic.ac.uk/teaching/distinguished-projects/2012/k.nagy.pdf
> >> >
> >>
> >
> >
>

--f46d043c82561a6c50051206b5ca--
#|#<CALsanEaVtL3xCFJnqTwFmxfkMYpXtmSa6NTsxcez9aUWGUd_aQ@mail.gmail.com>##//##<CA+faj9x8q5oLjaTCD7FJhc0Zkcv0CtHzhMd+o7wO5ZByt+oOsQ@mail.gmail.com>#|#2015-06-04-09:53:11#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Send events to parallel operator instances#|#
--001a11c37ac0a8d7b70517ae2341
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I am simply thinking about the best way to send data to different subtasks
of the same operator.

Can we go back to the original question? :D

Stephan Ewen <sewen@apache.org> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn. 3., Sze,
23:45):

> I think that it may be a bit pre-mature to invest heavily into the parallel
> delta-policy windows just yet.
> We have not even answered all questions on the key-local delta windows yet:
>
>  - How does it behave with non-monotonous changes? What does the delta
> refer to, the max interval in the window, the interval to the earliest
> element. The max difference between two consecutive elements?
>
>  - What about the order of records? Are deltas even interesting when
> records come in arbitrary order? What about the predictability of recovery
> runs?
>
>
> I would assume that a consistent version of the key-local delta windows
> will get us a long way, use-case wise.
>
> Let's learn more about how users use these policies in the "simple" case.
> Because that will impact the protocol for global coordination (for examplea
> concerning order and relative to what element are the deltas computed, the
> first or the min). Otherwise we invest a lot of effort into something where
> we have not yet a clear understanding about how we actually want it to
> behave, exactly.
>
> What do you think?
>
>
>
>
> On Wed, Jun 3, 2015 at 2:14 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com> wrote:
>
> > I am talking of course about global delta windows. On the full stream not
> > on a partition. Delta windows per partition happens as you said currently
> > as well.
> >
> > On Wednesday, June 3, 2015, Aljoscha Krettek <aljoscha@apache.org>
> wrote:
> >
> > > Yes, this is obvious, but if we simply partition the data on the
> > > attribute that we use for the delta policy this can be done purely on
> > > one machine. No need for complex communication/synchronization.
> > >
> > > On Wed, Jun 3, 2015 at 1:32 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com
> > > <javascript:;>> wrote:
> > > > Yes, we define a delta function from the first element to the last
> > > element
> > > > in a window. Now let's discretize the stream using this semantics in
> > > > parallel.
> > > >
> > > > Aljoscha Krettek <aljoscha@apache.org <javascript:;>> ezt =C3=ADrta
> > > (id=C5=91pont: 2015. j=C3=BAn. 3.,
> > > > Sze, 12:20):
> > > >
> > > >> Ah ok. And by distributed you mean that the element that starts the
> > > >> window can be processed on a different machine than the element that
> > > >> finishes the window?
> > > >>
> > > >> On Wed, Jun 3, 2015 at 12:11 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com
> > > <javascript:;>> wrote:
> > > >> > This is not connected to the current implementation. So lets not
> > talk
> > > >> about
> > > >> > that.
> > > >> >
> > > >> > This is about theoretical limits to support distributed delta
> > policies
> > > >> > which has far reaching implications for the windowing policies one
> > can
> > > >> > implement in a prallel way.
> > > >> >
> > > >> > But you are welcome to throw in any constructive ideas :)
> > > >> > On Wed, Jun 3, 2015 at 11:49 AM Aljoscha Krettek <
> > aljoscha@apache.org
> > > <javascript:;>>
> > > >> > wrote:
> > > >> >
> > > >> >> Part of the reason for my question is this:
> > > >> >> https://issues.apache.org/jira/browse/FLINK-1967. Especially my
> > > latest
> > > >> >> comment there. If we want this, I think we have to overhaul the
> > > >> >> windowing system anyways and then it doesn't make sense to
> explore
> > > >> >> complicated workarounds for the current system.
> > > >> >>
> > > >> >> On Wed, Jun 3, 2015 at 11:07 AM, Gyula F=C3=B3ra <
> gyula.fora@gmail.com
> > > <javascript:;>>
> > > >> wrote:
> > > >> >> > There are simple ways of implementing it in a non-distributed
> or
> > > >> >> > inconsistent fashion.
> > > >> >> > On Wed, Jun 3, 2015 at 8:55 AM Aljoscha Krettek <
> > > aljoscha@apache.org <javascript:;>>
> > > >> >> wrote:
> > > >> >> >
> > > >> >> >> This already sounds awfully complicated. Is there no other way
> > to
> > > >> >> >> implement the delta windows?
> > > >> >> >>
> > > >> >> >> On Wed, Jun 3, 2015 at 7:52 AM, Gyula F=C3=B3ra <
> > gyula.fora@gmail.com
> > > <javascript:;>>
> > > >> >> wrote:
> > > >> >> >> > Hi Ufuk,
> > > >> >> >> >
> > > >> >> >> > In the concrete use case I have in mind I only want to send
> > > events
> > > >> to
> > > >> >> >> > another subtask of the same task vertex.
> > > >> >> >> >
> > > >> >> >> > Specifically: if we want to do distributed delta based
> windows
> > > we
> > > >> >> need to
> > > >> >> >> > send after every trigger the element that has triggered the
> > > current
> > > >> >> >> window.
> > > >> >> >> > So practically I want to broadcast some event regularly to
> all
> > > >> >> subtasks
> > > >> >> >> of
> > > >> >> >> > the same operator.
> > > >> >> >> >
> > > >> >> >> > In this case the operators would wait until they receive
> this
> > > event
> > > >> >> so we
> > > >> >> >> > need to make sure that this event sending is not blocked by
> > the
> > > >> actual
> > > >> >> >> > records.
> > > >> >> >> >
> > > >> >> >> > Gyula
> > > >> >> >> >
> > > >> >> >> > On Tuesday, June 2, 2015, Ufuk Celebi <uce@apache.org
> > > <javascript:;>> wrote:
> > > >> >> >> >
> > > >> >> >> >>
> > > >> >> >> >> On 02 Jun 2015, at 22:45, Gyula F=C3=B3ra <gyfora@apache.org
> > > <javascript:;>
> > > >> >> <javascript:;>>
> > > >> >> >> >> wrote:
> > > >> >> >> >> > I am wondering, what is the suggested way to send some
> > events
> > > >> >> >> directly to
> > > >> >> >> >> > another parallel instance in a flink job? For example
> from
> > > one
> > > >> >> mapper
> > > >> >> >> to
> > > >> >> >> >> > another mapper (of the same operator).
> > > >> >> >> >> >
> > > >> >> >> >> > Do we have any internal support for this? The first thing
> > > that
> > > >> we
> > > >> >> >> thought
> > > >> >> >> >> > of is iterations but that is clearly an overkill.
> > > >> >> >> >>
> > > >> >> >> >> There is no support for this at the moment. Any parallel
> > > instance?
> > > >> >> Or a
> > > >> >> >> >> subtask instance of the same task?
> > > >> >> >> >>
> > > >> >> >> >> Can you provide more input on the use case? It is certainly
> > > >> possible
> > > >> >> to
> > > >> >> >> >> add support for this.
> > > >> >> >> >>
> > > >> >> >> >> If the events don't need to be inline with the records, we
> > can
> > > >> easily
> > > >> >> >> >> setup the TaskEventDispatcher as a separate actor (or
> extend
> > > the
> > > >> task
> > > >> >> >> >> manager) to process both backwards flowing events and in
> > > general
> > > >> any
> > > >> >> >> events
> > > >> >> >> >> that don't need to be inline with the records. The task
> > > deployment
> > > >> >> >> >> descriptors need to be extended with the extra parallel
> > > instance
> > > >> >> >> >> information.
> > > >> >> >> >>
> > > >> >> >> >> =E2=80=93 Ufuk
> > > >> >> >>
> > > >> >>
> > > >>
> > >
> >
>

--001a11c37ac0a8d7b70517ae2341--
#|#<CANC1h_u2D7vc8T1Qj9ekrmiCr6fFwnLxL7yc7ssmMLZUKayCeA@mail.gmail.com>##//##<CA+faj9xDsoG3DsF5ZsQx_ipHDN-ZYzqkX5LKM9KH3Kr1x2i=YA@mail.gmail.com>#|#2014-12-18-23:23:39#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Eclipse import errors after akka update#|#
--047d7b86d2ae2ae97f050a85e099
Content-Type: text/plain; charset=UTF-8

Hey guys,

Since the last Akka update pull request from Till, I am getting a lot of
import errors (AkkaUtils, and other related packages) in Eclipse and I
cannot build the project. With Eclipse Luna there is no chance it gives
like a 100 erros. With the eclipse scala ide based on Kepler I still get
scala compilation and import errors.

For instance:

in TypeInfomrationGen.scala:
"value q is not member of StringContext"


Any ideas what could cause these and how to fix it?
Is there anyone who can actually build this in eclipse?

Cheers,
Gyula

--047d7b86d2ae2ae97f050a85e099--
#|#null##//##<CA+faj9xSkZVDNQxD9Y8yWEw1WTAWiA5z9do_fs21WUZZ0r1RFA@mail.gmail.com>#|#2015-06-10-12:31:21#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Force enabling checkpoints for iterative streaming jobs#|#
--089e013c64709ac46005182908fa
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Max suggested that I add this feature slightly hidden to the execution
config instance.

The problem then is that I either make a public field in the config or once
again add a method.

Any ideas?

Aljoscha Krettek <aljoscha@apache.org> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn. 10.,
Sze, 14:07):

> Thanks :D, now I see. It makes sense because we don't have another way
> of keeping the cluster state synced/distributed across parallel
> instances of the operators.
>
> On Wed, Jun 10, 2015 at 12:52 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com> wrote:
> > Here is an example for you:
> >
> > Parallel streaming kmeans, the state we keep is the current cluster
> > centers, and we use iterations to sync the centers across parallel
> > instances.
> > We can afford lost model updated in the loop but we need the checkpoint
> the
> > models.
> >
> >
> https://github.com/gyfora/stream-clustering/blob/master/src/main/scala/stream/clustering/StreamClustering.scala
> >
> > (checkpointing is not turned on but you will get the point)
> >
> >
> >
> > Gyula F=C3=B3ra <gyula.fora@gmail.com> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn. 10.,
> Sze,
> > 12:47):
> >
> >> You are right, to have consistent results we would need to persist the
> >> records.
> >>
> >> But since we cannot do that right now, we can still checkpoint all
> >> operator states and understand that inflight records in the loop are
> lost
> >> on failure.
> >>
> >> This is acceptable for most the use-cases that we have developed so far
> >> for iterations (machine learning, graph updates, etc.) What is not
> >> acceptable is to not have checkpointing at all.
> >>
> >> Aljoscha Krettek <aljoscha@apache.org> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn.
> 10.,
> >> Sze, 12:43):
> >>
> >>> The elements that are in-flight in an iteration are also state of the
> >>> job. I'm wondering whether the state inside iterations still makes
> >>> sense without these in-flight elements. But I also don't know the King
> >>> use-case, that's why I though an example could be helpful.
> >>>
> >>> On Wed, Jun 10, 2015 at 12:37 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com>
> >>> wrote:
> >>> > I don't understand the question, I vote for checkpointing all state
> in
> >>> the
> >>> > job, even inside iterations (its more of a loop).
> >>> >
> >>> > Aljoscha Krettek <aljoscha@apache.org> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn.
> >>> 10.,
> >>> > Sze, 12:34):
> >>> >
> >>> >> I don't understand why having the state inside an iteration but not
> >>> >> the elements that correspond to this state or created this state is
> >>> >> desirable. Maybe an example could help understand this better?
> >>> >>
> >>> >> On Wed, Jun 10, 2015 at 11:27 AM, Gyula F=C3=B3ra <gyula.fora@gmail.com>
> >>> wrote:
> >>> >> > The other tests verify that the checkpointing algorithm runs
> >>> properly.
> >>> >> That
> >>> >> > also ensures that it runs for iterations because a loop is just an
> >>> extra
> >>> >> > source and sink in the jobgraph (so it is the same for the
> >>> algorithm).
> >>> >> >
> >>> >> > Fabian Hueske <fhueske@gmail.com> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn.
> 10.,
> >>> >> Sze,
> >>> >> > 11:19):
> >>> >> >
> >>> >> >> Without going into the details, how well tested is this feature?
> >>> The PR
> >>> >> >> only extends one test by a few lines.
> >>> >> >>
> >>> >> >> Is that really enough to ensure that
> >>> >> >> 1) the change does not cause trouble
> >>> >> >> 2) is working as expected
> >>> >> >>
> >>> >> >> If this feature should go into the release, it must be thoroughly
> >>> >> checked
> >>> >> >> and we must take the time for that.
> >>> >> >> Including code and hoping for the best because time is scarce is
> >>> not an
> >>> >> >> option IMO.
> >>> >> >>
> >>> >> >> Fabian
> >>> >> >>
> >>> >> >>
> >>> >> >> 2015-06-10 11:05 GMT+02:00 Gyula F=C3=B3ra <gyula.fora@gmail.com>:
> >>> >> >>
> >>> >> >> > And also I would like to remind everyone that any fault
> tolerance
> >>> we
> >>> >> >> > provide is only as good as the fault tolerance of the master
> node.
> >>> >> Which
> >>> >> >> is
> >>> >> >> > non existent at the moment.
> >>> >> >> >
> >>> >> >> > So I don't see a reason why a user should not be able to choose
> >>> >> whether
> >>> >> >> he
> >>> >> >> > wants state checkpoints for iterations as well.
> >>> >> >> >
> >>> >> >> > In any case this will be used by King for instance, so making
> it
> >>> part
> >>> >> of
> >>> >> >> > the release would save a lot of work for everyone.
> >>> >> >> >
> >>> >> >> > Paris Carbone <parisc@kth.se> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn.
> 10.,
> >>> Sze,
> >>> >> >> > 10:29):
> >>> >> >> >
> >>> >> >> > >
> >>> >> >> > > To continue Gyula's point, for consistent snapshots we need
> to
> >>> >> persist
> >>> >> >> > the
> >>> >> >> > > records in transit within the loop  and also slightly change
> the
> >>> >> >> current
> >>> >> >> > > protocol since it works only for DAGs. Before going into that
> >>> >> direction
> >>> >> >> > > though I would propose we first see whether there is a nice
> way
> >>> to
> >>> >> make
> >>> >> >> > > iterations more structured.
> >>> >> >> > >
> >>> >> >> > > Paris
> >>> >> >> > > ________________________________________
> >>> >> >> > > From: Gyula F=C3=B3ra <gyula.fora@gmail.com>
> >>> >> >> > > Sent: Wednesday, June 10, 2015 10:19 AM
> >>> >> >> > > To: dev@flink.apache.org
> >>> >> >> > > Subject: Re: Force enabling checkpoints for iterative
> streaming
> >>> jobs
> >>> >> >> > >
> >>> >> >> > > I disagree. Not having checkpointed operators inside the
> >>> iteration
> >>> >> >> still
> >>> >> >> > > breaks the guarantees.
> >>> >> >> > >
> >>> >> >> > > It is not about the states it is about the loop itself.
> >>> >> >> > > On Wed, Jun 10, 2015 at 10:12 AM Aljoscha Krettek <
> >>> >> aljoscha@apache.org
> >>> >> >> >
> >>> >> >> > > wrote:
> >>> >> >> > >
> >>> >> >> > > > This is the answer I gave on the PR (we should have one
> place
> >>> for
> >>> >> >> > > > discussing this, though):
> >>> >> >> > > >
> >>> >> >> > > > I would be against merging this in the current form. What I
> >>> >> propose
> >>> >> >> is
> >>> >> >> > > > to analyse the topology to verify that there are no
> >>> checkpointed
> >>> >> >> > > > operators inside iterations. Operators before and after
> >>> iterations
> >>> >> >> can
> >>> >> >> > > > be checkpointed and we can safely allow the user to enable
> >>> >> >> > > > checkpointing.
> >>> >> >> > > >
> >>> >> >> > > > If we have the code to analyse which operators are inside
> >>> >> iterations
> >>> >> >> > > > we could also disallow windows inside iterations. I think
> >>> windows
> >>> >> >> > > > inside iterations don't make sense since elements in
> different
> >>> >> >> > > > "iterations" would end up in the same window. Maybe I'm
> wrong
> >>> here
> >>> >> >> > > > though, then please correct me.
> >>> >> >> > > >
> >>> >> >> > > > On Wed, Jun 10, 2015 at 10:08 AM, M=C3=A1rton Balassi
> >>> >> >> > > > <balassi.marton@gmail.com> wrote:
> >>> >> >> > > > > I agree that for the sake of the above mentioned use
> cases
> >>> it is
> >>> >> >> > > > reasonable
> >>> >> >> > > > > to add this to the release with the right documentation,
> for
> >>> >> >> machine
> >>> >> >> > > > > learning potentially loosing one round of feedback data
> >>> should
> >>> >> not
> >>> >> >> > > > matter.
> >>> >> >> > > > >
> >>> >> >> > > > > Let us not block prominent users until the next release
> on
> >>> this.
> >>> >> >> > > > >
> >>> >> >> > > > > On Wed, Jun 10, 2015 at 8:09 AM, Gyula F=C3=B3ra <
> >>> >> gyula.fora@gmail.com>
> >>> >> >> > > > wrote:
> >>> >> >> > > > >
> >>> >> >> > > > >> As for people currently suffering from it:
> >>> >> >> > > > >>
> >>> >> >> > > > >> An application King is developing requires iterations,
> and
> >>> they
> >>> >> >> need
> >>> >> >> > > > >> checkpoints. Practically all SAMOA programs would need
> >>> this.
> >>> >> >> > > > >>
> >>> >> >> > > > >> It is very likely that the state interfaces will be
> changed
> >>> >> after
> >>> >> >> > the
> >>> >> >> > > > >> release, so this is not something that we can just add
> >>> later. I
> >>> >> >> > don't
> >>> >> >> > > > see a
> >>> >> >> > > > >> reason why we should not add it, as it is clearly
> >>> documented.
> >>> >> In
> >>> >> >> > this
> >>> >> >> > > > >> actual case not having guarantees at all means people
> will
> >>> >> never
> >>> >> >> use
> >>> >> >> > > it
> >>> >> >> > > > in
> >>> >> >> > > > >> any production system. Having limited guarantees means
> >>> that it
> >>> >> >> will
> >>> >> >> > > > depend
> >>> >> >> > > > >> on the application.
> >>> >> >> > > > >>
> >>> >> >> > > > >> On Wed, Jun 10, 2015 at 12:53 AM, Ufuk Celebi <
> >>> uce@apache.org>
> >>> >> >> > wrote:
> >>> >> >> > > > >>
> >>> >> >> > > > >> > Hey Gyula,
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > I understand your reasoning, but I don't think its
> worth
> >>> to
> >>> >> rush
> >>> >> >> > > this
> >>> >> >> > > > >> into
> >>> >> >> > > > >> > the release.
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > As you've said, we cannot give precise guarantees. But
> >>> this
> >>> >> is
> >>> >> >> > > > arguably
> >>> >> >> > > > >> > one of the key requirements for any fault tolerance
> >>> >> mechanism.
> >>> >> >> > > > Therefore
> >>> >> >> > > > >> I
> >>> >> >> > > > >> > disagree that this is better than not having anything
> at
> >>> >> all. I
> >>> >> >> > > think
> >>> >> >> > > > it
> >>> >> >> > > > >> > will already go a long way to have the non-iterative
> case
> >>> >> >> working
> >>> >> >> > > > >> reliably.
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > And as far as I know there are no users really
> suffering
> >>> from
> >>> >> >> this
> >>> >> >> > > at
> >>> >> >> > > > the
> >>> >> >> > > > >> > moment (in the sense that someone has complained on
> the
> >>> >> mailing
> >>> >> >> > > list).
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > Hence, I vote to postpone this.
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > =E2=80=93 Ufuk
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > On 10 Jun 2015, at 00:19, Gyula F=C3=B3ra <
> gyfora@apache.org>
> >>> >> wrote:
> >>> >> >> > > > >> >
> >>> >> >> > > > >> > > Hey all,
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > > It is currently impossible to enable state
> >>> checkpointing
> >>> >> for
> >>> >> >> > > > iterative
> >>> >> >> > > > >> > > jobs, because en exception is thrown when creating
> the
> >>> >> >> jobgraph.
> >>> >> >> > > > This
> >>> >> >> > > > >> > > behaviour is motivated by the lack of precise
> >>> guarantees
> >>> >> that
> >>> >> >> we
> >>> >> >> > > can
> >>> >> >> > > > >> give
> >>> >> >> > > > >> > > with the current fault-tolerance implementations for
> >>> cyclic
> >>> >> >> > > graphs.
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > > This PR <https://github.com/apache/flink/pull/812>
> >>> adds an
> >>> >> >> > > optional
> >>> >> >> > > > >> > flag to
> >>> >> >> > > > >> > > force checkpoints even in case of iterations. The
> >>> algorithm
> >>> >> >> will
> >>> >> >> > > > take
> >>> >> >> > > > >> > > checkpoints periodically as before, but records in
> >>> transit
> >>> >> >> > inside
> >>> >> >> > > > the
> >>> >> >> > > > >> > loop
> >>> >> >> > > > >> > > will be lost.
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > > However even this guarantee is enough for most
> >>> applications
> >>> >> >> > > (Machine
> >>> >> >> > > > >> > > Learning for instance) and certainly much better
> than
> >>> not
> >>> >> >> having
> >>> >> >> > > > >> anything
> >>> >> >> > > > >> > > at all.
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > > I suggest we add this to the 0.9 release as
> currently
> >>> many
> >>> >> >> > > > applications
> >>> >> >> > > > >> > > suffer from this limitation (SAMOA, ML pipelines,
> graph
> >>> >> >> > streaming
> >>> >> >> > > > etc.)
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > > Cheers,
> >>> >> >> > > > >> > >
> >>> >> >> > > > >> > > Gyula
> >>> >> >> > > > >> >
> >>> >> >> > > > >> >
> >>> >> >> > > > >>
> >>> >> >> > > >
> >>> >> >> > >
> >>> >> >> >
> >>> >> >>
> >>> >>
> >>>
> >>
>

--089e013c64709ac46005182908fa--
#|#<CANMXwW0gtFcf5VC0uGPGKLqsikAc+jrQVaTQCZC99ADnRL0nxw@mail.gmail.com>##//##<CA+faj9xTo0zo9wnWHY8LUAFyCU34b63gjzTf2uYBZJWGvQJBpQ@mail.gmail.com>#|#2015-03-24-18:46:32#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Question about Flink Streaming#|#
--001a11c2270c4c814205120d2ffe
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The setup and open methods could be called together, but they do different
tasks, and therefore I dont see any reason why they should be in a same
method. This is a critical part of the code so better keep things clean and
separate.

The RuntimeContext refers to the operator while the TaskContext refers to
the task itself. We dont want to expose everything in the TaskContext to
the user, because that could lead to serious problems.

Ok I will talk with him.

On Tue, Mar 24, 2015 at 5:54 PM, Matthias J. Sax <
mjsax@informatik.hu-berlin.de> wrote:

> Hi Gyula,
>
> thank a lot. I still don't understand why setup() and open() can not be
> unified? I also don't know, what the difference between RuntimeContext
> and StreamTaskContext is (or to be more precise, why not using a single
> context class that unifies both)?
>
> About the renaming of Timestamp class: Marton told me, that he thinks
> the name is fine and should not be changed. Before opening a JIRA for
> it, you two should get in sync and decide what to do.
>
>
> -Matthias
>
>
>
> On 03/24/2015 05:38 PM, Gyula F=C3=B3ra wrote:
> > Hey Matthias,
> >
> > Let's see if I get these things for you :)
> >
> > 1) The difference between setup and open is that, setup to set things
> like
> > collectors, runtimecontext and everything that will be used by the
> > implemented invokable, and also by the rich functions. Open is called
> after
> > setup, to actually open the execution of the UDF operator.
> >
> > 2) Close is always called, even when the task is cancelled. In addition
> > when a task is failing (maybe because other tasks are failing) the cancel
> > method is called and the main thread is interrupted. The point of having
> a
> > cancel method that some invokables might require different shutdown logic
> > in case of failure.
> >
> > 3) This I need to look into...
> >
> > 4) You are right about the unintuitive name here, if you could open a
> JIRA
> > for this I would appreciate that :)
> >
> > 5) You are absolutely right on this point, we need to spend more effort
> on
> > writing proper docs.
> >
> > I hope I could clarify some stuff.
> >
> > Cheers,
> > Gyula
> >
> > On Tue, Mar 24, 2015 at 5:05 PM, Matthias J. Sax <
> > mjsax@informatik.hu-berlin.de> wrote:
> >
> >> Hi,
> >>
> >> as I get more familiar with Flink streaming and do some coding, I hit a
> >> few points which I want do discuss about because I find them
> >> contra-intuitive. Please tell me, what you think about it or clarify
> >> what I misunderstood.
> >>
> >> 1) In class StreamInvokable has two methods .setup(...) and .open(...)
> >>    -> what is the difference between both? When is each of both called
> >> exactly? It seems to be, that both are used to setup an operator. Why
> >> can't they be unified?
> >>
> >> 2) The same question about .close() and .cancel() ?
> >>
> >> 3) There is an class/interface hierarchy for user defined functions. The
> >> top level interface is 'Function' and there is an interface
> >> 'RichFunction' and abstract class 'AbstractRichFunction'. For each
> >> different type, there are user functions derived from. So far so good.
> >>         However, the StreamInvokable class only takes a constructor
> >> argument
> >> Function, indicating that RichFunctions are not supported. Internally,
> >> the given function is tested to be a RichFunction (using instanceof) at
> >> certain places. This in contra-intuitive from a API point of view.
> >>         From my OO understanding it would be better to replace Function
> by
> >> RichFunction everywhere. However, I was told that the (empty) Function
> >> interface is necessary for lambda expressions. Thus, I would suggest to
> >> extend the API with methods taking a RichFunction a parameter so it is
> >> clear that those are supported, too.
> >>
> >> 4) There is the interface Timestamp that is used to extract a time stamp
> >> for a record on order to create windows on a record attribute. I think
> >> the name "Timestamp" is miss leading, because the class does not
> >> represent a time stamp. I would rather call the interface
> >> "TimestampExtractor" or something similar.
> >>
> >> 5) Stefan started the discussion about more tests for the streaming
> >> component. I would additionally suggest to improve the Javadoc
> >> documentation. The are many classes an method with missing or very brief
> >> documentation and it is ofter hard to guess what they are used for. I
> >> would also suggest to describe the interaction of components/classes and
> >> WHY some thing are implemented in a certain way. As I have background
> >> knowledge from Stratosphere, I personally can work around it and make
> >> sense out of it (at least most times). However, for new contributers it
> >> might be very hard to make sense out of it and to get started
> >> implementing new features.
> >>
> >>
> >> Cheers,
> >>   Matthias
> >>
> >>
> >>
> >
>
>

--001a11c2270c4c814205120d2ffe--
#|#<551196D1.8090403@informatik.hu-berlin.de>##//##<CA+faj9xVjxrTUgmHRAVuK7yv6wuD8hEt+u_dOe8ccv=NSZ6osQ@mail.gmail.com>#|#2014-07-14-13:22:56#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Ongoing Projects in stratosphere#|#
--089e0115ea30cabb8304fe272e59
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Generally you can just simply import it as a maven project using the
Eclipse maven plugin and it should work.
2014.07.14. 15:18 ezt =C3=ADrta ("Sudheer Sana" <sudheer050.iitm@gmail.com>):

> Hi
>
> When I am clicking the  set up the Eclipse IDE for development
> <https://github.com/apache/incubator-flink/#eclipse-setup-and-debugging>,
> and how to build the code
> <https://github.com/apache/incubator-flink/#build-stratosphere>. in the
> link http://flink.incubator.apache.org/how-to-contribute.html, it is going
> to the mirror of apache flink, but i couldn't find the steps for setting up
> the eclipse
>
> -- Thanks & Regards
> *Sudheer*
>

--089e0115ea30cabb8304fe272e59--
#|#<CABHJ70wovppbotKGN4xC3DWxhb-6VMNysb8yR+grhzChH=F-7w@mail.gmail.com>##//##<CA+faj9xdk1bwex3vKMNDhd7987hJP7VsPpoq3JZWQnceRY26Rg@mail.gmail.com>#|#2015-05-12-12:53:47#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: About Interplay of Merged Streams, Output Selectors and#|#
--001a11c3474eb210460515e1fbf5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Its actually a very different mechanism as watermarks will not block the
computations

On Tue, May 12, 2015 at 2:48 PM, Matthias J. Sax <
mjsax@informatik.hu-berlin.de> wrote:

> Hi,
>
> I don't understand why we need the same machnism twice in the code...
> Could checkpoing barrieres and low watermarks be unified (or one build
> on-top/by-using the other)
>
> -Matthias
>
>
> On 05/12/2015 02:47 PM, Gyula F=C3=B3ra wrote:
> > Hi,
> >
> > Checkpoint barriers are handled directly on top of the network layer and
> > you are right they work similarly, by blocking input channels until it
> gets
> > the barrier from all of them.
> >
> > A way of implementing this on the operator level would be by adding a way
> > to ask the inputreader the channel index of the last record. This way the
> > operator could keep track of the channels from which it has received
> > records and execute the watermark logic. The IndexedReaders have
> > implemented the necessarry funcionality but were patched away
> accidentally
> > buy some earlier changes (as they were not used anyway)
> >
> > Adding a union operator is probably an overkill and would pose the same
> > difficulties when implementing it.
> >
> > Cheers,
> > Gyula
> >
> > On Tue, May 12, 2015 at 2:40 PM, Aljoscha Krettek <aljoscha@apache.org>
> > wrote:
> >
> >> Hi Folks,
> >> as I said in the subject. How will this work? I'm in the process about
> >> thinking how to implement low watermarks in Streaming. I'm thinking
> >> that the implementation should be quite similar to how the
> >> checkpointing barriers will be implemented since they also flush out
> >> stuff.
> >>
> >> Now I'm wondering how this will work with merged Streams and the
> >> output selectors (split streams). It seems to me that there are a lot
> >> of paths that elements can take to arrive at operators. The problem I
> >> have is that an operator can only emit a low watermark itself if it
> >> knows that all input operators have sent him a low watermark with that
> >> value (the low watermark is the minimum of the low watermarks of all
> >> upstream operators). I imagine that the checkpoint barriers exhibit
> >> the same behaviour.
> >>
> >> Do we maybe have to add an explicit union (merge) operator and change
> >> how split streams are implemented?
> >>
> >> What are your thoughts?
> >>
> >> Cheers,
> >> Aljoscha
> >>
> >
>
>

--001a11c3474eb210460515e1fbf5--
#|#<5551F6A3.6060607@informatik.hu-berlin.de>##//##<CA+faj9xgHvVq-f9pH=7CaC4cVezrvSYHp5=RwPeCTDmvfs7xAQ@mail.gmail.com>#|#2015-05-12-12:47:28#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: About Interplay of Merged Streams, Output Selectors and#|#
--047d7bfcf7a62713bf0515e1e53f
Content-Type: text/plain; charset=UTF-8

Hi,

Checkpoint barriers are handled directly on top of the network layer and
you are right they work similarly, by blocking input channels until it gets
the barrier from all of them.

A way of implementing this on the operator level would be by adding a way
to ask the inputreader the channel index of the last record. This way the
operator could keep track of the channels from which it has received
records and execute the watermark logic. The IndexedReaders have
implemented the necessarry funcionality but were patched away accidentally
buy some earlier changes (as they were not used anyway)

Adding a union operator is probably an overkill and would pose the same
difficulties when implementing it.

Cheers,
Gyula

On Tue, May 12, 2015 at 2:40 PM, Aljoscha Krettek <aljoscha@apache.org>
wrote:

> Hi Folks,
> as I said in the subject. How will this work? I'm in the process about
> thinking how to implement low watermarks in Streaming. I'm thinking
> that the implementation should be quite similar to how the
> checkpointing barriers will be implemented since they also flush out
> stuff.
>
> Now I'm wondering how this will work with merged Streams and the
> output selectors (split streams). It seems to me that there are a lot
> of paths that elements can take to arrive at operators. The problem I
> have is that an operator can only emit a low watermark itself if it
> knows that all input operators have sent him a low watermark with that
> value (the low watermark is the minimum of the low watermarks of all
> upstream operators). I imagine that the checkpoint barriers exhibit
> the same behaviour.
>
> Do we maybe have to add an explicit union (merge) operator and change
> how split streams are implemented?
>
> What are your thoughts?
>
> Cheers,
> Aljoscha
>

--047d7bfcf7a62713bf0515e1e53f--
#|#<CANMXwW0u9ijY4zNiaSxzKhCeUcJe1gyu_pajXqHMGsrnA+nGMw@mail.gmail.com>##//##<CA+faj9xganaqx9F8=ke_SETkVF=S4JHFXQzhS3ecAc9ZSrwSiw@mail.gmail.com>#|#2015-01-05-23:10:10#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Problems with Eclipse IDE#|#
--f46d043894e5032cc6050befc90b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sorry Stephan you are right, I cannot seem to find the links in a proper
website but could dig it up from some git repo:

https://github.com/scala-ide/scala-ide.github.com/blob/master/_includes/sdk-download-box-2-10.txt

So the actual download links:

http://downloads.typesafe.com/scalaide-pack/3.0.3.vfinal-210-20140327/scala-SDK-3.0.3-2.10-linux.gtk.x86_64.tar.gz

http://downloads.typesafe.com/scalaide-pack/3.0.3.vfinal-210-20140327/scala-SDK-3.0.3-2.10
-linux.gtk.x86.tar.gz

http://downloads.typesafe.com/scalaide-pack/3.0.3.vfinal-210-20140327/scala-SDK-3.0.3-2.10
-macosx.cocoa.x86_64.zip

http://downloads.typesafe.com/scalaide-pack/3.0.3.vfinal-210-20140327/scala-SDK-3.0.3-2.10
-macosx.cocoa.x86.zip

http://downloads.typesafe.com/scalaide-pack/3.0.3.vfinal-210-20140327/scala-SDK-3.0.3-2.10
-win32.win32.x86_64.zip

http://downloads.typesafe.com/scalaide-pack/3.0.3.vfinal-210-20140327/scala-SDK-3.0.3-2.10
-win32.win32.x86.zip

Cheers,
Gyula


On Mon, Jan 5, 2015 at 11:49 PM, Stephan Ewen <sewen@apache.org> wrote:

> Hi Gyula!
>
> Thank you for the reply. The links you sent me refer only to update sites
> to download plugin updates.
>
> Do you still have the link for the pre-packaged download?
>
> Stephan
>
> On Mon, Jan 5, 2015 at 11:39 PM, Gyula F=C3=B3ra <gyfora@apache.org> wrote:
>
> > Hey,
> >
> > Marton, Paris and I could get Eclipse working by using a previous stable
> > version if the eclipse scala-ide. Not by installing the plugin to Kepler
> > afterwards but directly downloading the prepackaged version from:
> >
> > http://scala-ide.org/download/prev-stable.html
> >
> > for scala 2.10.4
> > <http://download.scala-ide.org/sdk/helium/e38/scala210/stable/site>
> >
> > We have tried with both the plugins and 4.0.0 but it didnt work at all.
> >
> > For the import errors, sometimes it is needed to manually rebuild the
> > project with maven. Usually "mvn clean package" does the trick for random
> > import errors.
> >
> > Of course the compiler plugin needs to be set as mentioned in previous
> > discussions for the flink-scala project.
> >
> > Hope this helps,
> >
> > Gyula
> >
> > On Mon, Jan 5, 2015 at 11:32 PM, Stephan Ewen <sewen@apache.org> wrote:
> >
> > > Hi all!
> > >
> > > Since the last pull requests that split the "flink-runtime" in a part
> > Java
> > > part Scala project, it seems impossible to develop it with Eclipse.
> > >
> > > I have tried
> > >  - Eclipse Kepler with Scala IDE plugin
> > >  - Scala IDE 4.0.0
> > >
> > > The later does an even worse job than Kepler with the plugin.
> > >
> > > I am seeing all of those errors in mixed order:
> > >  - Scala files unable to find java classes from the same project
> > >  - Java files unable to find scala classes from the same project
> > >  - Java files being unable to resolve imports which are clearly present
> > in
> > > the maven dependencies.
> > >
> > > Has anyone had luck in getting this to run?
> > >
> > >
> > > In the current state, we effectively block all new contributors that
> are
> > > used to Eclipse as their default IDE and are hesitant to switch to a
> new
> > > tool only for trying out an open source project.
> > >
> > >
> > > Greetings,
> > > Stephan
> > >
> >
>

--f46d043894e5032cc6050befc90b--
#|#<CANC1h_u4EYTOS_CK9qhHQXYDGhYkSGreGW_kCsNf8PbWfkWp_w@mail.gmail.com>##//##<CA+faj9xit8J2hv1=nDOhArLBbcqZYr1jNy1CnL80Q+LaaoKDRA@mail.gmail.com>#|#2015-06-01-15:31:28#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Re: [DISCUSS] Consolidate method naming between the batch and#|#
--089e013c66a8e4a8b50517768483
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1 for the changes proposed by Marton (before the release)

Aljoscha Krettek <aljoscha@apache.org> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn. 1., H,
16:32):

> Yes, these renamings make sense. The partitionBy() is not yet in the
> master for streaming, though.
>
> On Mon, Jun 1, 2015 at 4:10 PM, M=C3=A1rton Balassi <balassi.marton@gmail.com>
> wrote:
> > Looking at the DataSet and DataStream APIs we have come to the conclusion
> > with Aljoscha that there are a few methods that although providing the
> same
> > functionality are named differently. These are the following:
> >
> >    1.  rebalance (batch) / distribute (streaming): Rebalances the data
> sent
> >    to the downstream operators thus equally distributing it.
> >    2. partitionByHash, partitionCustom (batch) / partitionBy (streaming):
> >    Partitioning has just recently been exposed in the streaming API and
> is not
> >    as refined as the batch one. The streaming partitionBy is actually
> >    partitionByHash.
> >    3. Union (batch) / merge, connect (streaming): The streaming merge
> does
> >    a union of two streams with the same type. Connect is conceptually
> >    different, it provides a way of sharing state between two streams with
> >    potentially different types without mapping them to a common type and
> then
> >    merging them. This saves latency and an ugly mapping. The former
> advantage
> >    can be offset by proper operator chaining, the second one would
> remain if
> >    we did not have connect.
> >
> > To consolidate the naming I would suggest the following:
> >
> >    1. Rename streaming distribute to rebalance.
> >    2. Rename streaming partitionBy to partitionByHash and file JIRA for
> >    custom partitioning support for streaming.
> >    3. Rename streaming merge to union, leave streaming connect as it is.
>

--089e013c66a8e4a8b50517768483--
#|#<CANMXwW12T8oAmwBY2mUsfQkLFspy0dEDR+vscddgVXe5ys+qOg@mail.gmail.com>##//##<CA+faj9xj3XAcePC5XaTqsTAY4ziQcyDqO1H_nJzEiaL8vgMAug@mail.gmail.com>#|#2015-05-21-20:30:13#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Re: [DISCUSS] Dedicated streaming mode#|#
--001a1133b06603dc9c05169d6965
Content-Type: text/plain; charset=UTF-8

Huge +1 from my side :)

Sorry for the late response.

On Thu, May 21, 2015 at 9:54 PM, Aljoscha Krettek <aljoscha@apache.org>
wrote:

> This sounds very reasonable.
> On May 21, 2015 9:34 PM, "Stephan Ewen" <sewen@apache.org> wrote:
>
> > I discussed a bit via Skype with Gyula and Paris.
> >
> >
> > We thought about the following way to do it:
> >
> >  - We add a dedicated streaming mode for now. The streaming mode
> supersedes
> > the batch mode, so it can run both type of programs.
> >
> >  - The streaming mode sets the memory manager to "lazy allocation".
> >     -> So long as it runs pure streaming jobs, the full heap will be
> > available to window buffers and UDFs.
> >     -> Batch programs can still run, so mixed workloads are not
> prevented.
> > Batch programs are a bit less robust there, because the memory manager
> does
> > not pre-allocate memory. UDFs can eat into Flink's memory portion.
> >
> >  - The streaming mode starts the necessary configured components/services
> > for state backups
> >
> >
> >
> > Over the next versions, we want to bring these things together:
> >   - use the managed memory for window buffers
> >   - on-demand starting of the state backend
> >
> > Then, we deprecate the streaming mode, let both modes start the cluster
> in
> > the same way.
> >
> >
> >
> >
> >
> > On Thu, May 21, 2015 at 4:01 PM, Aljoscha Krettek <aljoscha@apache.org>
> > wrote:
> >
> > > Would it not be possible to start the snapshot service once the user
> > > starts the first streaming job? About 2) with checkpointing coming up,
> > > would it not make sense to shift to managed memory rather sooner than
> > > later. Then this point would become moot.
> > >
> > > On Thu, May 21, 2015 at 3:47 PM, Matthias J. Sax
> > > <mjsax@informatik.hu-berlin.de> wrote:
> > > > What would be the consequences on "mixed" programs? (If there is any
> > > > plan to support those?)
> > > >
> > > > Would it be necessary to have a third mode? Or would those programs
> > > > simple run in streaming mode?
> > > >
> > > > -Matthias
> > > >
> > > > On 05/21/2015 03:12 PM, Stephan Ewen wrote:
> > > >> Hi all!
> > > >>
> > > >> We discussed a while back about introducing a dedicated streaming
> mode
> > > for
> > > >> Flink. I would like to take a go at this and implement the changes,
> > but
> > > >> discuss them before.
> > > >>
> > > >>
> > > >> Here is a brief summary why we wanted to introduce the dedicated
> > > streaming
> > > >> mode:
> > > >> Even though both batch and streaming are executed by the same
> > execution
> > > >> engine,
> > > >> a streaming setup of Flink varies a bit from a batch setup:
> > > >>
> > > >> 1) The streaming cluster starts an additional service to store the
> > > >> distributed state snapshots.
> > > >>
> > > >> 2) Streaming mode uses memory a bit different, so we should
> configure
> > > the
> > > >> memory manager differently. This difference may eventually go away.
> > > >>
> > > >>
> > > >>
> > > >> Concretely, to implement this, I was thinking about introducing the
> > > >> following externally visible changes
> > > >>
> > > >>  - Additional scripts "start-streaming-cluster.sh" and
> > > >> "start-streaming-local.sh"
> > > >>
> > > >>  - An execution mode parameter for the TaskManager ("batch /
> > streaming")
> > > >>
> > > >>  - An execution mode parameter for the JobManager TaskManager
> ("batch
> > /
> > > >> streaming")
> > > >>
> > > >>  - All local executors and mini clusters need a flag that specifies
> > > whether
> > > >> they will start
> > > >>    a streaming cluster, or a pure batch cluster.
> > > >>
> > > >>
> > > >> Anything else that comes to your minds?
> > > >>
> > > >>
> > > >> Greetings,
> > > >> Stephan
> > > >>
> > > >
> > >
> >
>

--001a1133b06603dc9c05169d6965--
#|#<CANMXwW2g7=jRrP3nQQKH9QVGn7joVoGn4p2PG3TGZP98bJKWhg@mail.gmail.com>##//##<CA+faj9xoWgewAkKh+qc2=3AEZFxvnfiD7aDdSS7PtFF2Vcu_0A@mail.gmail.com>#|#2015-04-21-13:20:40#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Periodic full stream aggregations#|#
--f46d04451809c3c84205143be239
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

You are right, but you should never try to compute full stream median,
thats the point :D

On Tue, Apr 21, 2015 at 2:52 PM, Bruno Cadonna <
cadonna@informatik.hu-berlin.de> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi Gyula,
>
> I read your comments of your PR.
>
> I have a question to this comment:
>
> "It only allows aggregations so we dont need to keep the full history
> in a buffer."
>
> What if the user implements an aggregation function like a median?
>
> For a median you need the full history, don't you?
>
> Am I missing something?
>
> Cheers,
> Bruno
>
> On 21.04.2015 14:31, Gyula F=C3=B3ra wrote:
> > I have opened a PR for this feature:
> >
> > https://github.com/apache/flink/pull/614
> >
> > Cheers, Gyula
> >
> > On Tue, Apr 21, 2015 at 1:10 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com>
> > wrote:
> >
> >> Thats a good idea, I will modify my PR to that :)
> >>
> >> Gyula
> >>
> >> On Tue, Apr 21, 2015 at 12:09 PM, Fabian Hueske
> >> <fhueske@gmail.com> wrote:
> >>
> >>> Is it possible to switch the order of the statements, i.e.,
> >>>
> >>> dataStream.every(Time.of(4,sec)).reduce(...) instead of
> >>> dataStream.reduce(...).every(Time.of(4,sec))
> >>>
> >>> I think that would be more consistent with the structure of the
> >>> remaining API.
> >>>
> >>> Cheers, Fabian
> >>>
> >>> 2015-04-21 10:57 GMT+02:00 Gyula F=C3=B3ra <gyfora@apache.org>:
> >>>
> >>>> Hi Bruno,
> >>>>
> >>>> Of course you can do that as well. (That's the good part :p
> >>>> )
> >>>>
> >>>> I will open a PR soon with the proposed changes (first
> >>>> without breaking
> >>> the
> >>>> current Api) and I will post it here.
> >>>>
> >>>> Cheers, Gyula
> >>>>
> >>>> On Tuesday, April 21, 2015, Bruno Cadonna <
> >>> cadonna@informatik.hu-berlin.de
> >>>>>
> >>>> wrote:
> >>>>
> > Hi Gyula,
> >
> > I have a question regarding your suggestion.
> >
> > Can the current continuous aggregation be also specified with your
> > proposed periodic aggregation?
> >
> > I am thinking about something like
> >
> > dataStream.reduce(...).every(Count.of(1))
> >
> > Cheers, Bruno
> >
> > On 20.04.2015 22:32, Gyula F=C3=B3ra wrote:
> >>>>>>> Hey all,
> >>>>>>>
> >>>>>>> I think we are missing a quite useful feature that
> >>>>>>> could be implemented (with some slight modifications)
> >>>>>>> on top of the current windowing api.
> >>>>>>>
> >>>>>>> We currently provide 2 ways of aggregating (or
> >>>>>>> reducing) over streams: doing a continuous aggregation
> >>>>>>> and always output the aggregated value (which cannot be
> >>>>>>> done properly in parallel) or doing aggregation in a
> >>>>>>> window periodically.
> >>>>>>>
> >>>>>>> What we don't have at the moment is periodic
> >>>>>>> aggregations on the whole stream. I would even go as
> >>>>>>> far as to remove the continuous outputting
> >>>>>>> reduce/aggregate it and replace it with this version
> >>>>>>> as this in return can be done properly in parallel.
> >>>>>>>
> >>>>>>> My suggestion would be that a call:
> >>>>>>>
> >>>>>>> dataStream.reduce(..) dataStream.sum(..)
> >>>>>>>
> >>>>>>> would return a windowed data stream where the window is
> >>>>>>> the whole record history, and the user would need to
> >>>>>>> define a trigger to get the actual reduced values
> >>>>>>> like:
> >>>>>>>
> >>>>>>> dataStream.reduce(...).every(Time.of(4,sec)) to get the
> >>>>>>> actual reduced results. dataStream.sum(...).every(...)
> >>>>>>>
> >>>>>>> I think the current data stream reduce/aggregation is
> >>>>>>> very confusing without being practical for any normal
> >>>>>>> use-case.
> >>>>>>>
> >>>>>>> Also this would be a very api breaking change (but I
> >>>>>>> would still make this change as it is much more
> >>>>>>> intuitive than the current behaviour) so I would try to
> >>>>>>> push it before the release if we can agree.
> >>>>>>>
> >>>>>>> Cheers, Gyula
> >>>>>>>
> >
> >>>>>
> >>>>
> >>>
> >>
> >>
> >
>
> - --
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
>   Dr. Bruno Cadonna
>   Postdoctoral Researcher
>
>   Databases and Information Systems
>   Department of Computer Science
>   Humboldt-Universit=C3=A4t zu Berlin
>
>   http://www.informatik.hu-berlin.de/~cadonnab
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.11 (GNU/Linux)
>
> iQEcBAEBAgAGBQJVNkgYAAoJEKdCIJx7flKw7rUIAMmu80ZuMvfA/BvQemkEo7As
> bU3iWre+e3OUWNRLuf2JfG9CHMKFSjBJG6Jax/pWZBXTYh8oaYDrYixq7e+vljqf
> P9ypurhd1h8In71aSUyUPIsrTg6aJ5xo/beUxA6LFbB2LpVqawNDe0gjn3ZRMobM
> zmn962kqp0oHAVipYI2mzEU6RNl1Kh0PoaLaZRLRh+dlgKofqDFcBiB3hhG/VEoF
> sCsCAsC1bXtpToPRZ29cRcEfpHcnE3zCgivPeG83JsWYr4mIEj7gp+smFUz0PjoI
> 1wHv/pnZJS4Onk38HH1GcP95/uYpqm4gz3OBCuE7v+3b1bI852bIvnUZrCGLOew=3D
> =3Du1R0
> -----END PGP SIGNATURE-----
>

--f46d04451809c3c84205143be239--
#|#<55364818.5080205@informatik.hu-berlin.de>##//##<CA+faj9y16aKtdFyk2PP8A+xKszqUKm=Cv40n5bwF6LU-TKz7Pg@mail.gmail.com>#|#2015-01-22-13:32:02#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#ClassLoader issue when submitting Flink Streaming programs through#|#
--047d7bfcfd10d625ca050d3db1ba
Content-Type: text/plain; charset=UTF-8

Hey,

While trying to add support for plan visualisation I also realised that
streaming programs cannot be run through the flink web client.

It seems to be a ClassLoader issue, which is interesting because we use the
classloader set in the environment to deserialize user defined objects.
This works for submitting jobs through the command line client though. I
dont see why it should be different when you submit something through the
commandline or the web interface.

Thanks,
Gyula

--047d7bfcfd10d625ca050d3db1ba--
#|#null##//##<CA+faj9y7AjHZfp52WRv3Ztgo9sZrcF=8iTzQdCoeKtxRiBW8mA@mail.gmail.com>#|#2014-07-16-12:06:15#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: A wiki for Flink#|#
--047d7b41cc744e840704fe4e589d
Content-Type: text/plain; charset=UTF-8

gyfora

Thanks :)


On Wed, Jul 16, 2014 at 11:49 AM, Fabian Hueske <fhueske@apache.org> wrote:

> fhueske
>
> thanks, Henry!
>
>
> 2014-07-16 11:36 GMT+02:00 Robert Metzger <rmetzger@apache.org>:
>
> > I filed a JIRA, now we have to wait:
> > https://issues.apache.org/jira/browse/INFRA-8039
> > (I'm going to add further usernames posted here into the JIRA)
> >
> >
> > On Wed, Jul 16, 2014 at 11:26 AM, Till Rohrmann <till.rohrmann@gmail.com
> >
> > wrote:
> >
> > > trohrmann
> > >
> > >
> > > On Wed, Jul 16, 2014 at 9:30 AM, Henry Saputra <
> henry.saputra@gmail.com>
> > > wrote:
> > >
> > > > LOL =)
> > > >
> > > > On Wed, Jul 16, 2014 at 12:26 AM, Aljoscha Krettek
> > > > <aljoscha.krettek@gmail.com> wrote:
> > > > > Dammit, gmail seems to be hiding it when i send just my name, so
> just
> > > to
> > > > be
> > > > > sure, my confluence name: aljoscha
> > > > >
> > > > >
> > > > > On Wed, Jul 16, 2014 at 9:26 AM, Aljoscha Krettek <
> > > > > aljoscha.krettek@gmail.com> wrote:
> > > > >
> > > > >> aljoscha
> > > > >>
> > > > >>
> > > > >> On Wed, Jul 16, 2014 at 9:19 AM, Ufuk Celebi <
> u.celebi@fu-berlin.de
> > >
> > > > >> wrote:
> > > > >>
> > > > >>>
> > > > >>> On 16 Jul 2014, at 09:16, Robert Metzger <rmetzger@apache.org>
> > > wrote:
> > > > >>>
> > > > >>> > Okay, I will request a Confluence Wiki for Flink at Infra.
> > > > >>> > Therefore, I need your _Confluence_ usernames. You can create
> an
> > > > account
> > > > >>> > here: https://cwiki.apache.org/.
> > > > >>>
> > > > >>> uce
> > > > >>>
> > > > >>
> > > > >>
> > > >
> > >
> >
>

--047d7b41cc744e840704fe4e589d--
#|#<CAAdrtT1+NcOu275jZhoVYNRXuki0ZHC4bFYQimNLJV0Yr-h6FQ@mail.gmail.com>##//##<CA+faj9yAOW1nxD=AKJgXX7sVghYVqfRGbgxBy_HYA41n-f4Fkg@mail.gmail.com>#|#2014-07-10-13:00:14#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Exception in Inputgate#|#
--089e013d09603c696204fdd666e8
Content-Type: text/plain; charset=UTF-8

Hey,

Until now, after every emit to the outputs we flushed them using the
.flush() method of the recordwriter. Now we removed this flush() call and
we have two interesting observations:

First of all we dont send enough records the source finishes but the output
buffer never gets flushed.

Secondly if we generate a simple datastream from lets say the first 1500
<https://github.com/stratosphere/stratosphere-streaming/blob/output-flush/stratosphere-streaming-core/src/test/java/eu/stratosphere/streaming/api/PrintTest.java#L48>
numbers we get an exception in the InputGates (after lets say a hundred
records): java.lang.IllegalStateException: Channel received an event before
completing the current partial record.

java.lang.IllegalStateException: Channel received an event before
completing the current partial record.
at
eu.stratosphere.runtime.io.channels.InputChannel.readRecord(InputChannel.java:177)
at eu.stratosphere.runtime.io.gates.InputGate.readRecord(InputGate.java:173)
at
eu.stratosphere.streaming.api.streamcomponent.StreamRecordReader.hasNext(StreamRecordReader.java:96)
at
eu.stratosphere.streaming.api.streamcomponent.AbstractStreamComponent.invokeRecords(AbstractStreamComponent.java:255)
at
eu.stratosphere.streaming.api.streamcomponent.StreamSink.invoke(StreamSink.java:74)
at
eu.stratosphere.nephele.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:260)
at java.lang.Thread.run(Unknown Source)



This works perfectly if we flush the outputs after the emits.

Any ideas what might cause this problem?

Regards,
Gyula

--089e013d09603c696204fdd666e8--
#|#null##//##<CA+faj9yBwzyxf5LZXjVtPvst+UrQkq1iJwjWSu52TNYutf24Fg@mail.gmail.com>#|#2014-10-15-15:24:15#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Possible race condition in StreamRecordWriter#|#
--001a113fc1d4064c6a050577b8a3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

So both the emit and flush methods have been modified to make sure that
only one accesses the serializer at the same time. (with the synchronized
blocks) So since the output flusher only flushes every so many millis, then
this worked for us so far. (and I think it should work) We haven't had any
issues with this either locally or on the cluster.



On Wed, Oct 15, 2014 at 5:18 PM, Ufuk Celebi <u.celebi@fu-berlin.de> wrote:

> Hey all (but mostly the streaming folks),
>
> while refactoring the writers for the runtime changes (FLINK-986) I
> discovered a possible race condition in StreamRecordWriter [1].
>
> The problem is that the record writer is not thread-safe, but both the
> streaming task vertex and the OutputFlusher thread use it concurrently.
>
> Am I overlooking something and is it safe to use it this way? If not:
> should I ensure that record writer is thread-safe with my upcoming changes?
>
> =E2=80=93 Ufuk
>
> [1]
> https://github.com/apache/incubator-flink/blob/b904b0041cf97b2c6181b1985afc457ed01cf626/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/io/StreamRecordWriter.java

--001a113fc1d4064c6a050577b8a3--
#|#<ADC0C05C-5C05-436A-85D9-BEB28EC23FBB@fu-berlin.de>##//##<CA+faj9yDkSyeS7XUz=kxftkrCCu9S1Me021GO_c3hGOCAnZJxg@mail.gmail.com>#|#2014-12-23-23:57:34#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Eclipse import errors after akka update#|#
--f46d043894e5f74ba7050aeaebf7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hey,

The easiest fix for the eclipse issues is to avoid using luna and instead
use the eclipse scala-ide (which you can also use for java development,
this is what I do). This is basically an eclipse kepler with preinstalled
scala plugins which are actually working.

You can download it from here:
http://scala-ide.org/download/prev-stable.html

Make sure you download the version for Scala 2.10
<http://download.scala-ide.org/sdk/helium/e38/scala210/stable/site>.

When you imported the project the last thing to do is to set the compiler
plugin for the flink-scala project as described by till:
" You have to specify it in Properties (Module) -> Scala compiler ->
Advanced -> Xplugin.
You should find the paradise jar in your local maven repository, if you
built the project at least once with maven: Something like
~/.m2/repository/org/scalamacros/paradise_2.10.4/2.
0.1/paradise_2.10.4-2.0.1.jar."

I hope this helps, worked fine for me and others.

Cheers,
Gyula


On Wed, Dec 24, 2014 at 12:48 AM, Chen Xu <chenxuecnu@gmail.com> wrote:

> Hi,
>
> I have the same import errors so that I can not build successfully in
> Eclipse Luna.
>
> For example, some class
> e.g.org.apache.flink.runtime.messages.ExecutionGraphMessages,
> org.apache.flink.runtime.akka.AkkaUtils
> can not be found.
>
> So, how to fix it?
>
> Cheers!
> -Chen
>
> 2014-12-19 6:58 GMT+01:00 Gyula Fora <gyula.fora@gmail.com>:
>
> > Hey,
> >
> > Thanks Till for the description, I actually thought that I had the things
> > set already what you described since I have been working on the scala
> code
> > anyways.
> >
> > It turned out that the xplugin path self updated itself by pasting my
> > workspace path in front of path. So when I checked if I have everything
> set
> > it looked good since the beginning of the path is same but was actually
> an
> > invalid=E2=80=A6
> >
> > Sorry I missed that somehow
> >
> > Gyula
> >
> > > On 19 Dec 2014, at 02:38, Till Rohrmann <trohrmann@apache.org> wrote:
> > >
> > > Hi Gyula,
> > >
> > > I just tried to build the current master with Eclipse Luna and the
> > ScalaIDE
> > > 4.0.0 and it worked. What you have to make sure is that the Scala
> > compiler
> > > for the sub modules requiring Scala is set to Scala 2.10. This is
> > > configured under Properties (Module) -> Scala Compiler. Otherwise you
> get
> > > errors saying that a library is build for a wrong Scala version (namely
> > > 2.10). The sub modules requiring a Scala nature are flink-runtime,
> > > flink-scala, flink-scala-examples, flink-test-utils and flink-tests, if
> > I'm
> > > not mistaken.
> > >
> > > The second thing are the quasi quotes in the flink-scala module. Either
> > you
> > > simply close the project and everything should work or you add the
> > required
> > > Scala macro plugin paradise_2.10.4-2.0.1.jar to the compiler. You have
> to
> > > specify it in Properties (Module) -> Scala compiler -> Advanced ->
> > Xplugin.
> > > You should find the paradise jar in your local maven repository, if you
> > > built the project at least once with maven: Something like
> > >
> >
> ~/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar.
> > > That is basically the same thing you have to do in IntelliJ as well to
> be
> > > able to compile the flink-scala module out of IntelliJ.
> > >
> > > I hope this solves your problems with Eclipse and sorry for the
> trouble I
> > > caused you with my changes.
> > >
> > > Greets,
> > >
> > > Till
> > >
> > > On Fri, Dec 19, 2014 at 1:34 AM, Stephan Ewen <sewen@apache.org>
> wrote:
> > >>
> > >> I will try and look into this tomorrow. I suspect it is something
> either
> > >> about Scala versions or compiler plugins...
> > >>
> > >> Greetings,
> > >> Stephan
> > >> Am 19.12.2014 00:23 schrieb "Gyula F=C3=B3ra" <gyula.fora@gmail.com>:
> > >>
> > >>> Hey guys,
> > >>>
> > >>> Since the last Akka update pull request from Till, I am getting a lot
> > of
> > >>> import errors (AkkaUtils, and other related packages) in Eclipse and
> I
> > >>> cannot build the project. With Eclipse Luna there is no chance it
> gives
> > >>> like a 100 erros. With the eclipse scala ide based on Kepler I still
> > get
> > >>> scala compilation and import errors.
> > >>>
> > >>> For instance:
> > >>>
> > >>> in TypeInfomrationGen.scala:
> > >>> "value q is not member of StringContext"
> > >>>
> > >>>
> > >>> Any ideas what could cause these and how to fix it?
> > >>> Is there anyone who can actually build this in eclipse?
> > >>>
> > >>> Cheers,
> > >>> Gyula
> > >>>
> > >>
> >
> >
>

--f46d043894e5f74ba7050aeaebf7--
#|#<CAKos+Mjxi4JBuxfJWiDv1VHK4XDOt6x0SAuwOK+-TK=2=rhfqw@mail.gmail.com>##//##<CA+faj9yGG2r3bhTk6kKR6WCwvE0fr1=6-TR31VSLFquNsGMsDQ@mail.gmail.com>#|#2015-04-18-12:01:26#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Rework of the window-join semantics#|#
--e89a8f2345570b35850513fe708d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hey all,

We have spent some time with Asterios, Paris and Jonas to finalize the
windowing semantics (both the current features and the window join), and I
think we made very have come up with a very clear picture.

We will write down the proposed semantics and publish it to the wiki next
week.

Cheers,
Gyula

On Thu, Apr 16, 2015 at 5:50 PM, Asterios Katsifodimos <
asterios.katsifodimos@tu-berlin.de> wrote:

> As far as I see in [1], Peter's/Gyula's suggestion is what Infosphere
> Streams does: symmetric hash join.
>
> From [1]:
> "When a tuple is received on an input port, it is inserted into the window
> corresponding to the input port, which causes the window to trigger. As
> part of the trigger processing, the tuple is compared against all tuples
> inside the window of the opposing input port. If the tuples match, then an
> output tuple will be produced for each match. If at least one output was
> generated, a window punctuation will be generated after all the outputs."
>
> Cheers,
> Asterios
>
> [1]
>
> http://www-01.ibm.com/support/knowledgecenter/#!/SSCRJU_3.2.1/com.ibm.swg.im.infosphere.streams.spl-standard-toolkit-reference.doc/doc/join.html
>
>
>
> On Thu, Apr 9, 2015 at 1:30 PM, Matthias J. Sax <
> mjsax@informatik.hu-berlin.de> wrote:
>
> > Hi Paris,
> >
> > thanks for the pointer to the Naiad paper. That is quite interesting.
> >
> > The paper I mentioned [1], does not describe the semantics in detail; it
> > is more about the implementation for the stream-joins. However, it uses
> > the same semantics (from my understanding) as proposed by Gyula.
> >
> > -Matthias
> >
> > [1] Kang, Naughton, Viglas. "Evaluationg Window Joins over Unbounded
> > Streams". VLDB 2002.
> >
> >
> >
> > On 04/07/2015 12:38 PM, Paris Carbone wrote:
> > > Hello Matthias,
> > >
> > > Sure, ordering guarantees are indeed a tricky thing, I recall having
> > that discussion back in TU Berlin. Bear in mind thought that DataStream,
> > our abstract data type, represents a *partitioned* unbounded sequence of
> > events. There are no *global* ordering guarantees made whatsoever in that
> > model across partitions. If you see it more generally there are many
> =E2=80=9Crace
> > conditions=E2=80=9D in a distributed execution graph of vertices that process
> > multiple inputs asynchronously, especially when you add joins and
> > iterations into the mix (how do you deal with reprocessing =E2=80=9Cold=E2=80=9D tuples
> > that iterate in the graph). Btw have you checked the Naiad paper [1]?
> > Stephan cited a while ago and it is quite relevant to that discussion.
> > >
> > > Also, can you cite the paper with the joining semantics you are
> > referring to? That would be of good help I think.
> > >
> > > Paris
> > >
> > > [1] https://users.soe.ucsc.edu/~abadi/Papers/naiad_final.pdf
> > >
> > > <https://users.soe.ucsc.edu/~abadi/Papers/naiad_final.pdf>
> > >
> > > <https://users.soe.ucsc.edu/~abadi/Papers/naiad_final.pdf>
> > > On 07 Apr 2015, at 11:50, Matthias J. Sax <
> mjsax@informatik.hu-berlin.de
> > <mailto:mjsax@informatik.hu-berlin.de>> wrote:
> > >
> > > Hi @all,
> > >
> > > please keep me in the loop for this work. I am highly interested and I
> > > want to help on it.
> > >
> > > My initial thoughts are as follows:
> > >
> > > 1) Currently, system timestamps are used and the suggested approach can
> > > be seen as state-of-the-art (there is actually a research paper using
> > > the exact same join semantic). Of course, the current approach is
> > > inherently non-deterministic. The advantage is, that there is no
> > > overhead in keeping track of the order of records and the latency
> should
> > > be very low. (Additionally, state-recovery is simplified. Because, the
> > > processing in inherently non-deterministic, recovery can be done with
> > > relaxed guarantees).
> > >
> > >  2) The user should be able to "switch on" deterministic processing,
> > > ie, records are timestamped (either externally when generated, or
> > > timestamped at the sources). Because deterministic processing adds some
> > > overhead, the user should decide for it actively.
> > > In this case, the order must be preserved in each re-distribution step
> > > (merging is sufficient, if order is preserved within each incoming
> > > channel). Furthermore, deterministic processing can be achieved by
> sound
> > > window semantics (and there is a bunch of them). Even for
> > > single-stream-windows it's a tricky problem; for join-windows it's even
> > > harder. From my point of view, it is less important which semantics are
> > > chosen; however, the user must be aware how it works. The most tricky
> > > part for deterministic processing, is to deal with duplicate timestamps
> > > (which cannot be avoided). The timestamping for (intermediate) result
> > > tuples, is also an important question to be answered.
> > >
> > >
> > > -Matthias
> > >
> > >
> > > On 04/07/2015 11:37 AM, Gyula F=C3=B3ra wrote:
> > > Hey,
> > >
> > > I agree with Kostas, if we define the exact semantics how this works,
> > this
> > > is not more ad-hoc than any other stateful operator with multiple
> inputs.
> > > (And I don't think any other system support something similar)
> > >
> > > We need to make some design choices that are similar to the issues we
> had
> > > for windowing. We need to chose how we want to evaluate the windowing
> > > policies (global or local) because that affects what kind of policies
> can
> > > be parallel, but I can work on these things.
> > >
> > > I think this is an amazing feature, so I wouldn't necessarily rush the
> > > implementation for 0.9 though.
> > >
> > > And thanks for helping writing these down.
> > >
> > > Gyula
> > >
> > > On Tue, Apr 7, 2015 at 11:11 AM, Kostas Tzoumas <ktzoumas@apache.org
> > <mailto:ktzoumas@apache.org>> wrote:
> > >
> > > Yes, we should write these semantics down. I volunteer to help.
> > >
> > > I don't think that this is very ad-hoc. The semantics are basically the
> > > following. Assuming an arriving element from the left side:
> > > (1) We find the right-side matches
> > > (2) We insert the left-side arrival into the left window
> > > (3) We recompute the left window
> > > We need to see whether right window re-computation needs to be
> triggered
> > as
> > > well. I think that this way of joining streams is also what the
> symmetric
> > > hash join algorithms were meant to support.
> > >
> > > Kostas
> > >
> > >
> > > On Tue, Apr 7, 2015 at 10:49 AM, Stephan Ewen <sewen@apache.org
> <mailto:
> > sewen@apache.org>> wrote:
> > >
> > > Is the approach of joining an element at a time from one input against
> a
> > > window on the other input not a bit arbitrary?
> > >
> > > This just joins whatever currently happens to be the window by the time
> > > the
> > > single element arrives - that is a bit non-predictable, right?
> > >
> > > As a more general point: The whole semantics of windowing and when they
> > > are
> > > triggered are a bit ad-hoc now. It would be really good to start
> > > formalizing that a bit and
> > > put it down somewhere. Users need to be able to clearly understand and
> > > how
> > > to predict the output.
> > >
> > >
> > >
> > > On Fri, Apr 3, 2015 at 12:10 PM, Gyula F=C3=B3ra <gyula.fora@gmail.com
> > <mailto:gyula.fora@gmail.com>>
> > > wrote:
> > >
> > > I think it should be possible to make this compatible with the
> > > .window().every() calls. Maybe if there is some trigger set in "every"
> > > we
> > > would not join that stream 1 by 1 but every so many elements. The
> > > problem
> > > here is that the window and every in this case are very-very different
> > > than
> > > the normal windowing semantics. The window would define the join window
> > > for
> > > each element of the other stream while every would define how often I
> > > join
> > > This stream with the other one.
> > >
> > > We need to think to make this intuitive.
> > >
> > > On Fri, Apr 3, 2015 at 11:23 AM, M=C3=A1rton Balassi <
> > > balassi.marton@gmail.com<mailto:balassi.marton@gmail.com>>
> > > wrote:
> > >
> > > That would be really neat, the problem I see there, that we do not
> > > distinguish between dataStream.window() and
> > > dataStream.window().every()
> > > currently, they both return WindowedDataStreams and TriggerPolicies
> > > of
> > > the
> > > every call do not make much sense in this setting (in fact
> > > practically
> > > the
> > > trigger is always set to count of one).
> > >
> > > But of course we could make it in a way, that we check that the
> > > eviction
> > > should be either null or count of 1, in every other case we throw an
> > > exception while building the JobGraph.
> > >
> > > On Fri, Apr 3, 2015 at 8:43 AM, Aljoscha Krettek <
> > > aljoscha@apache.org<mailto:aljoscha@apache.org>>
> > > wrote:
> > >
> > > Or you could define it like this:
> > >
> > > stream_A =3D a.window(...)
> > > stream_B =3D b.window(...)
> > >
> > > stream_A.join(stream_B).where().equals().with()
> > >
> > > So a join would just be a join of two WindowedDataStreamS. This
> > > would
> > > neatly move the windowing stuff into one place.
> > >
> > > On Thu, Apr 2, 2015 at 9:54 PM, M=C3=A1rton Balassi <
> > > balassi.marton@gmail.com<mailto:balassi.marton@gmail.com>
> > >
> > > wrote:
> > > Big +1 for the proposal for Peter and Gyula. I'm really for
> > > bringing
> > > the
> > > windowing and window join API in sync.
> > >
> > > On Thu, Apr 2, 2015 at 6:32 PM, Gyula F=C3=B3ra <gyfora@apache.org<mailto:
> > gyfora@apache.org>>
> > > wrote:
> > >
> > > Hey guys,
> > >
> > > As Aljoscha has highlighted earlier the current window join
> > > semantics
> > > in
> > > the streaming api doesn't follow the changes in the windowing
> > > api.
> > > More
> > > precisely, we currently only support joins over time windows of
> > > equal
> > > size
> > > on both streams. The reason for this is that we now take a
> > > window
> > > of
> > > each
> > > of the two streams and do joins over these pairs. This would be
> > > a
> > > blocking
> > > operation if the windows are not closed at exactly the same time
> > > (and
> > > since
> > > we dont want this we only allow time windows)
> > >
> > > I talked with Peter who came up with the initial idea of an
> > > alternative
> > > approach for stream joins which works as follows:
> > >
> > > Instead of pairing windows for joins, we do element against
> > > window
> > > joins.
> > > What this means is that whenever we receive an element from one
> > > of
> > > the
> > > streams, we join this element with the current window(this
> > > window
> > > is
> > > constantly updated) of the other stream. This is non-blocking on
> > > any
> > > window
> > > definitions as we dont have to wait for windows to be completed
> > > and
> > > we
> > > can
> > > use this with any of our predefined policies like Time.of(...),
> > > Count.of(...), Delta.of(....).
> > >
> > > Additionally this also allows some very flexible way of defining
> > > window
> > > joins. With this we could also define grouped windowing inside
> > > if
> > > a
> > > join.
> > > An example of this would be: Join all elements of Stream1 with
> > > the
> > > last
> > > 5
> > > elements by a given windowkey of Stream2 on some join key.
> > >
> > > This feature can be easily implemented over the current
> > > operators,
> > > so
> > > I
> > > already have a working prototype for the simple non-grouped
> > > case.
> > > My
> > > only
> > > concern is the API, the best thing I could come up with is
> > > something
> > > like
> > > this:
> > >
> > > stream_A.join(stream_B).onWindow(windowDefA,
> > > windowDefB).by(windowKey1,
> > > windowKey2).where(...).equalTo(...).with(...)
> > >
> > > (the user can omit the "by" and "with" calls)
> > >
> > > I think this new approach would be worthy of our "flexible
> > > windowing"
> > > in
> > > contrast with the current approach.
> > >
> > > Regards,
> > > Gyula
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> >
> >
>

--e89a8f2345570b35850513fe708d--
#|#<CAADvThdyMBBCszoSb+n1Pq+JSue5PACPciq73WHAgwCR2Br3Tw@mail.gmail.com>##//##<CA+faj9zHD87Vk4dkWLY36UKHh5BPOtViaZz_uXa5yHtWtXqesg@mail.gmail.com>#|#2015-06-10-08:54:37#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: Testing Apache Flink 0.9.0-rc1#|#
--089e0118279c4d64c405182605e2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

This feature needs to be included in the release, it has been tested and
used extensively. And many applciations depend on it.

Maximilian Michels <mxm@apache.org> ezt =C3=ADrta (id=C5=91pont: 2015. j=C3=BAn. 10., Sze,
10:47):

> With all the issues discovered, it looks like we'll have another release
> candidate. Right now, we have discovered the following problems:
>
> 1 YARN ITCase fails [fixed via 2eb5cfe]
> 2 No Jar for SessionWindowing example [fixed in #809]
> 3 Wrong description of the input format for the graph examples (eg.
> ConnectedComponents) [fixed in #809]
> 4 TaskManagerFailsWithSlotSharingITCase fails
> 5 ComplexIntegrationTest.complexIntegrationTest1() (FLINK-2192) fails
> 6 Submitting KMeans example to Web Submission Client does not work on
> Firefox.
> 7 Zooming is buggy in Web Submission Client (Firefox)
>
> Do we have someone familiar with the web interface who could take a look at
> the Firefox issues?
>
> One more important thing: The release-0.9 branch should only be used for
> bug fixes or prior discussed feature changes. Adding new features defies
> the purpose of carefully testing in advance and can have unforeseeable
> consequences. In particular, I'm referring to #810 pull request:
> https://github.com/apache/flink/pull/810
>
> IMHO, this one shouldn't have been cherry-picked onto the release-0.9
> branch. I would like to remove it from there if no objections are raised.
>
>
> https://github.com/apache/flink/commit/e0e6f59f309170e5217bdfbf5d30db87c947f8ce
>
> On Wed, Jun 10, 2015 at 8:52 AM, Aljoscha Krettek <aljoscha@apache.org>
> wrote:
>
> > This doesn't look good, yes.
> >
> > On Wed, Jun 10, 2015 at 1:32 AM, Ufuk Celebi <uce@apache.org> wrote:
> >
> > > While looking into FLINK-2188 (HBase input) I've discovered that Hadoop
> > > input formats implementing Configurable (like
> mapreduce.TableInputFormat)
> > > don't have the Hadoop configuration set via setConf(Configuration).
> > >
> > > I have a small fix for this, which I have to clean up. First, I wanted
> to
> > > check what you think about this issue wrt the release. Personally, I
> > think
> > > this is a release blocker, because it essentially means that no Hadoop
> > > input format, which relies on the Configuration instance to be set this
> > way
> > > will work (this is to some extent a bug of the respective input
> formats)
> > =E2=80=93
> > > most notably the HBase TableInputFormat.
> > >
> > > =E2=80=93 Ufuk
> > >
> > > On 09 Jun 2015, at 18:07, Chiwan Park <chiwanpark@icloud.com> wrote:
> > >
> > > > I attached jps and jstack log about hanging
> > > TaskManagerFailsWithSlotSharingITCase to JIRA FLINK-2183.
> > > >
> > > > Regards,
> > > > Chiwan Park
> > > >
> > > >> On Jun 10, 2015, at 12:28 AM, Aljoscha Krettek <aljoscha@apache.org
> >
> > > wrote:
> > > >>
> > > >> I discovered something that might be a feature, rather than a bug.
> > When
> > > you
> > > >> submit an example using the web client without giving parameters the
> > > >> program fails with this:
> > > >>
> > > >> org.apache.flink.client.program.ProgramInvocationException: The main
> > > method
> > > >> caused an error.
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:452)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:353)
> > > >>
> > > >> at org.apache.flink.client.program.Client.run(Client.java:315)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.apache.flink.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:302)
> > > >>
> > > >> at javax.servlet.http.HttpServlet.service(HttpServlet.java:668)
> > > >>
> > > >> at javax.servlet.http.HttpServlet.service(HttpServlet.java:770)
> > > >>
> > > >> at
> > > org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)
> > > >>
> > > >> at
> > >
> org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
> > > >>
> > > >> at
> > >
> org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)
> > > >>
> > > >> at org.eclipse.jetty.server.Server.handle(Server.java:352)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
> > > >>
> > > >> at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)
> > > >>
> > > >> at
> > org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)
> > > >>
> > > >> at
> > > org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)
> > > >>
> > > >> at java.lang.Thread.run(Thread.java:745)
> > > >>
> > > >> Caused by: java.lang.NullPointerException
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.apache.flink.api.common.JobExecutionResult.getAccumulatorResult(JobExecutionResult.java:78)
> > > >>
> > > >> at org.apache.flink.api.java.DataSet.collect(DataSet.java:409)
> > > >>
> > > >> at org.apache.flink.api.java.DataSet.print(DataSet.java:1345)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.apache.flink.examples.java.wordcount.WordCount.main(WordCount.java:80)
> > > >>
> > > >> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > > >>
> > > >> at
> > > >>
> > >
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
> > > >>
> > > >> at
> > > >>
> > >
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> > > >>
> > > >> at java.lang.reflect.Method.invoke(Method.java:497)
> > > >>
> > > >> at
> > > >>
> > >
> >
> org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:437)
> > > >>
> > > >> ... 24 more
> > > >>
> > > >>
> > > >> This also only occurs when you uncheck the "suspend execution while
> > > showing
> > > >> plan".
> > > >>
> > > >> I think this arises because the new print() uses collect() which
> tries
> > > to
> > > >> get the job execution result. I guess the result is Null since the
> job
> > > is
> > > >> submitted asynchronously when the checkbox is unchecked.
> > > >>
> > > >>
> > > >> Other than that, the new print() is pretty sweet when you run the
> > > builtin
> > > >> examples from the CLI. You get all the state changes and also the
> > > result,
> > > >> even when running in cluster mode on several task managers. :D
> > > >>
> > > >>
> > > >> On Tue, Jun 9, 2015 at 3:41 PM, Aljoscha Krettek <
> aljoscha@apache.org
> > >
> > > >> wrote:
> > > >>
> > > >>> I discovered another problem:
> > > >>> https://issues.apache.org/jira/browse/FLINK-2191 The closure
> cleaner
> > > >>> cannot be disabled in part of the Streaming Java API and all of the
> > > >>> Streaming Scala API. I think this is a release blocker (in addition
> > > >>> the the other bugs found so far.)
> > > >>>
> > > >>> On Tue, Jun 9, 2015 at 2:35 PM, Aljoscha Krettek <
> > aljoscha@apache.org>
> > > >>> wrote:
> > > >>>> I found the bug in the failing YARNSessionFIFOITCase: It was
> > comparing
> > > >>>> the hostname to a hostname in some yarn config. In one case it was
> > > >>>> capitalised, in the other case it wasn't.
> > > >>>>
> > > >>>> Pushing fix to master and release-0.9 branch.
> > > >>>>
> > > >>>> On Tue, Jun 9, 2015 at 2:18 PM, Sachin Goel <
> > sachingoel0101@gmail.com
> > > >
> > > >>> wrote:
> > > >>>>> A re-ran lead to reproducibility of 11 failures again.
> > > >>>>> TaskManagerTest.testSubmitAndExecuteTask was failing with a
> > time-out
> > > but
> > > >>>>> managed to succeed in a re-run. Here is the log output again:
> > > >>>>> http://pastebin.com/raw.php?i=3DN4cm1J18
> > > >>>>>
> > > >>>>> Setup: JDK 1.8.0_40 on windows 8.1
> > > >>>>> System memory: 8GB, quad-core with maximum 8 threads.
> > > >>>>>
> > > >>>>> Regards
> > > >>>>> Sachin Goel
> > > >>>>>
> > > >>>>> On Tue, Jun 9, 2015 at 5:34 PM, Ufuk Celebi <uce@apache.org>
> > wrote:
> > > >>>>>
> > > >>>>>>
> > > >>>>>> On 09 Jun 2015, at 13:58, Sachin Goel <sachingoel0101@gmail.com
> >
> > > >>> wrote:
> > > >>>>>>
> > > >>>>>>> On my local machine, several flink runtime tests are failing on
> > > "mvn
> > > >>>>>> clean
> > > >>>>>>> verify". Here is the log output:
> > > >>> http://pastebin.com/raw.php?i=3DVWbx2ppf
> > > >>>>>>
> > > >>>>>> Thanks for reporting this. Have you tried it multiple times? Is
> it
> > > >>> failing
> > > >>>>>> reproducibly with the same tests? What's your setup?
> > > >>>>>>
> > > >>>>>> =E2=80=93 Ufuk
> > > >>>
> > > >
> > > >
> > > >
> > >
> > >
> >
>

--089e0118279c4d64c405182605e2--
#|#<CAGco--ayEZNB1y6gEpbqGsc_vX+16SkFY=eym0o9WyPNLVnrmg@mail.gmail.com>##//##<CA+faj9zHm-HHQDHwhqVP3TBRFT63Dno3Gh_C5HgXngpX3_VW_Q@mail.gmail.com>#|#2015-05-05-13:12:31#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Re: [DISCUSS] Change Streaming Operators to be Push-Only#|#
--047d7bb042723b60fd0515556e84
Content-Type: text/plain; charset=UTF-8

I think this a good idea in general. I would try to minimize the methods we
include and make the ones that we keep very concrete. For instance i would
not have the receive barrier method as that is handled on a totally
different level already. And instead of punctuation I would directly add a
method to work on watermarks.

On Tuesday, May 5, 2015, Aljoscha Krettek <aljoscha@apache.org> wrote:

> What do you mean by "losing iterations"?
>
> For the pros and cons:
>
> Cons: I can't think of any, since most of the operators are chainable
> already and already behave like a collector.
>
> Pros:
>  - Unified model for operators, chainable operators don't have to
> worry about input iterators and the collect interface.
>  - Enables features that we want in the future, such as barriers and
> punctuations because they don't work with the
>    simple Collector interface.
>  - The while-loop is moved outside of the operators, now the Task (the
> thing that runs Operators) can control the flow of data better and
> deal with
>    stuff like barriers and punctuations. If we want to keep the
> main-loop inside each operator, then they all have to manage input
> readers and inline events manually.
>
> On Tue, May 5, 2015 at 2:41 PM, Kostas Tzoumas <ktzoumas@apache.org
> <javascript:;>> wrote:
> > Can you give us a rough idea of the pros and cons? Do we lose some
> > functionality by getting rid of iterations?
> >
> > Kostas
> >
> > On Tue, May 5, 2015 at 1:37 PM, Aljoscha Krettek <aljoscha@apache.org
> <javascript:;>>
> > wrote:
> >
> >> Hi Folks,
> >> while working on introducing source-assigned timestamps into streaming
> >> (https://issues.apache.org/jira/browse/FLINK-1967) I thought about how
> >> the punctuations (low watermarks) can be pushed through the system.
> >> The problem is, that operators can have two ways of getting input: 1.
> >> They read directly from input iterators, and 2. They act as a
> >> Collector and get elements via collect() from the previous operator in
> >> a chain.
> >>
> >> This makes it hard to push things through a chain that are not
> >> elements, such as barriers and/or punctuations.
> >>
> >> I propose to change all streaming operators to be push based, with a
> >> slightly improved interface: In addition to collect(), which I would
> >> call receiveElement() I would add receivePunctuation() and
> >> receiveBarrier(). The first operator in the chain would also get data
> >> from the outside invokable that reads from the input iterator and
> >> calls receiveElement() for the first operator in a chain.
> >>
> >> What do you think? I would of course be willing to implement this
> myself.
> >>
> >> Cheers,
> >> Aljoscha
> >>
>

--047d7bb042723b60fd0515556e84--
#|#<CANMXwW1g2p9KciNiv2GhJgs5VHEWfBLLek0nC2fgXB9G6pFkkA@mail.gmail.com>##//##<CA+faj9zRuvMShHuG9uK3RqSEhg4arV=25y=hHHjSLf7Hm_e11g@mail.gmail.com>#|#2015-01-21-14:41:32#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Turn lazy operator execution off for streaming jobs#|#
--089e013d0f0e8f5a75050d2a8c5f
Content-Type: text/plain; charset=UTF-8

Hey Guys,

I think it would make sense to turn lazy operator execution off for
streaming programs because it would make life simpler for windowing.  I
also created a JIRA issue here
<https://issues.apache.org/jira/browse/FLINK-1425>.

Can anyone give me some quick pointers how to do this? Its probably simple,
I am just not familiar with that part of the code. (Or maybe its so easy
that someone could pick this up :) )

By the way, do you see any reasons why we should not do this?

Thank you!
Gyula

--089e013d0f0e8f5a75050d2a8c5f--
#|#null##//##<CA+faj9zX4Cc4eRROpBeT7TZMv8sBPu0XhamRGEbe6xMRZxA-GA@mail.gmail.com>#|#2015-05-20-13:41:28#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyula.fora@gmail.com>#|#Re: [DISCUSS] Re-add record copy to chained operator calls#|#
--089e01419b1c05323d05168395f7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I would go for the Failsafe option as a default behaviour with a clearly
documented lightweight (no-copy) setting, but I think having a Vote on this
would be the proper way of settling this question.

On Wed, May 20, 2015 at 3:37 PM, Aljoscha Krettek <aljoscha@apache.org>
wrote:

> I think that in the long run (maybe not too long) we will have to
> change our stateful operators (windows, basically) to use managed
> memory and spill to disk. (Think jobs that have sliding windows over
> days or weeks) Then then the internal operators will take care of
> copying anyways. The problem Gyula mentioned we cannot tackle other
> than by defining how user code must behave.
>
> On Wed, May 20, 2015 at 3:30 PM, Stephan Ewen <sewen@apache.org> wrote:
> > It does not mean we have to behave the same way, it is just an indication
> > that well-defined behavior can allow you to mess things up.
> >
> > The question is now what is the default mode:
> >  - Failsafe/Heavy (always copy)
> >  - Performance/Lightweight (do not copy)
> >
> >
> > On Wed, May 20, 2015 at 3:29 PM, Stephan Ewen <sewen@apache.org> wrote:
> >
> >> This is something that we can clearly define as "should not be done".
> >> Systems do that.
> >> I think if you repeatedly emit (or mutate) the same object for example
> in
> >> Spark, you get an RDD with completely messed up contents.
> >>
> >> On Wed, May 20, 2015 at 3:27 PM, Gyula F=C3=B3ra <gyfora@apache.org> wrote:
> >>
> >>> If the preceding operator is emitting a mutated object, or does
> something
> >>> with the output object afterwards then its a problem.
> >>>
> >>> Emitting the same object is a special case of this.
> >>>
> >>> On Wed, May 20, 2015 at 3:09 PM, Stephan Ewen <sewen@apache.org>
> wrote:
> >>>
> >>> > The case you are making is if a preceding operator in a chain is
> >>> repeatedly
> >>> > emitting the same object, and the succeeding operator is gathering
> the
> >>> > objects, then it is a problem
> >>> >
> >>> > Or are there cases where the system itself repeatedly emits the same
> >>> > objects?
> >>> >
> >>> > On Wed, May 20, 2015 at 3:07 PM, Gyula F=C3=B3ra <gyfora@apache.org>
> wrote:
> >>> >
> >>> > > We are designing a system for stateful stream computations,
> assuming
> >>> long
> >>> > > standing operators that gather and store data as the stream evolves
> >>> > (unlike
> >>> > > in the dataset api). Many programs, like windowing, sampling etc
> hold
> >>> the
> >>> > > state in the form of past data. And without careful understanding
> of
> >>> the
> >>> > > runtime these programs will break or have unnecessary copies.
> >>> > >
> >>> > > This is why I think immutability should be the default so we can
> have
> >>> a
> >>> > > clear dataflow model with immutable streams.
> >>> > >
> >>> > > I see absolutely no reason why we cant have the non-copy version
> as an
> >>> > > optional setting for the users.
> >>> > >
> >>> > >
> >>> > > On Wed, May 20, 2015 at 2:21 PM, Paris Carbone <parisc@kth.se>
> wrote:
> >>> > >
> >>> > > > @stephan I see your point. If we assume that operators do not
> hold
> >>> > > > references in their state to any transmitted records it works
> fine.
> >>> We
> >>> > > > therefore need to make this clear to the users. I need to check
> if
> >>> that
> >>> > > > would break semantics in SAMOA or other integrations as well that
> >>> > assume
> >>> > > > immutability. For example in SAMOA there are often local metric
> >>> objects
> >>> > > > that are being constantly mutated and simply forwarded
> periodically
> >>> to
> >>> > > > other (possibly chained) operators that need to evaluate them.
> >>> > > >
> >>> > > > ________________________________________
> >>> > > > From: Gyula F=C3=B3ra <gyfora@apache.org>
> >>> > > > Sent: Wednesday, May 20, 2015 2:06 PM
> >>> > > > To: dev@flink.apache.org
> >>> > > > Subject: Re: [DISCUSS] Re-add record copy to chained operator
> calls
> >>> > > >
> >>> > > > "Copy before putting it into a window buffer and any other group
> >>> > buffer."
> >>> > > >
> >>> > > > Exactly my point. Any stateful operator should be able to
> implement
> >>> > > > something like this without having to worry about copying the
> object
> >>> > (and
> >>> > > > at this point the user would need to know whether it comes from
> the
> >>> > > network
> >>> > > > to avoid unnecessary copies), so I don't agree with leaving the
> copy
> >>> > off.
> >>> > > >
> >>> > > > The user can of course specify that the operator is mutable if he
> >>> wants
> >>> > > > (and he is worried about the performance), But I still think the
> >>> > default
> >>> > > > behaviour should be immutable.
> >>> > > > We cannot force users to not hold object references and also it
> is a
> >>> > > quite
> >>> > > > unnatural way of programming in a language like java.
> >>> > > >
> >>> > > >
> >>> > > > On Wed, May 20, 2015 at 1:39 PM, Stephan Ewen <sewen@apache.org>
> >>> > wrote:
> >>> > > >
> >>> > > > > I am curious why the copying is actually needed.
> >>> > > > >
> >>> > > > > In the batch API, we chain and do not copy and it is rather
> >>> > > predictable.
> >>> > > > >
> >>> > > > > The cornerpoints of that design is to follow these rules:
> >>> > > > >
> >>> > > > >  1) Objects read from the network or any buffer are always new
> >>> > objects.
> >>> > > > > That comes naturally when they are deserialized as part of that
> >>> (all
> >>> > > > > buffers store serialized)
> >>> > > > >
> >>> > > > >  2) After a function returned a record (or gives one to the
> >>> > collector),
> >>> > > > it
> >>> > > > > if given to the chain of chained operators, but after it is
> >>> through
> >>> > the
> >>> > > > > chain, no one else holds a reference to that object.
> >>> > > > >      For that, it is crucial that objects are not stored by
> >>> > reference,
> >>> > > > but
> >>> > > > > either stored serialized, or a copy is stored.
> >>> > > > >
> >>> > > > > This is quite solid in the batch API. How about we follow the
> same
> >>> > > > paradigm
> >>> > > > > in the streaming API. We would need to adjust the following:
> >>> > > > >
> >>> > > > > 1) Do not copy between operators (I think this is the case
> right
> >>> now)
> >>> > > > >
> >>> > > > > 2) Copy before putting it into a window buffer and any other
> group
> >>> > > > buffer.
> >>> > > > >
> >>> > > > >
> >>> > > > >
> >>> > > > >
> >>> > > > >
> >>> > > > >
> >>> > > > >
> >>> > > > >
> >>> > > > > On Wed, May 20, 2015 at 1:22 PM, Aljoscha Krettek <
> >>> > aljoscha@apache.org
> >>> > > >
> >>> > > > > wrote:
> >>> > > > >
> >>> > > > > > Yes, in fact I anticipated this. There is one central place
> >>> where
> >>> > we
> >>> > > > > > can insert a copy step, in OperatorCollector in
> OutputHandler.
> >>> > > > > >
> >>> > > > > > On Wed, May 20, 2015 at 11:17 AM, Paris Carbone <
> parisc@kth.se>
> >>> > > wrote:
> >>> > > > > > > I guess it was not intended ^^.
> >>> > > > > > >
> >>> > > > > > > Chaining should be transparent and not break the
> >>> correct/expected
> >>> > > > > > behaviour.
> >>> > > > > > >
> >>> > > > > > >
> >>> > > > > > > Paris?
> >>> > > > > > >
> >>> > > > > > > On 20 May 2015, at 11:02, M=C3=A1rton Balassi <
> mbalassi@apache.org
> >>> >
> >>> > > > wrote:
> >>> > > > > > >
> >>> > > > > > > +1 for copying.
> >>> > > > > > > On May 20, 2015 10:50 AM, "Gyula F=C3=B3ra" <gyfora@apache.org>
> >>> > wrote:
> >>> > > > > > >
> >>> > > > > > > Hey,
> >>> > > > > > >
> >>> > > > > > > The latest streaming operator rework removed the copying of
> >>> the
> >>> > > > outputs
> >>> > > > > > > before passing them to chained operators. This is a major
> >>> break
> >>> > for
> >>> > > > the
> >>> > > > > > > previous operator semantics which guaranteed immutability.
> >>> > > > > > >
> >>> > > > > > > I think this change leads to very indeterministic program
> >>> > behaviour
> >>> > > > > from
> >>> > > > > > > the user's perspective as only non-chained outputs/inputs
> >>> will be
> >>> > > > > > mutable.
> >>> > > > > > > If we allow this to happen, users will start disabling
> >>> chaining
> >>> > to
> >>> > > > get
> >>> > > > > > > immutability which defeats the purpose. (chaining should
> not
> >>> > affect
> >>> > > > > > program
> >>> > > > > > > behaviour just increase performance)
> >>> > > > > > >
> >>> > > > > > > In my opinion the default setting for each operator should
> be
> >>> > > > > > immutability
> >>> > > > > > > and the user could override this manually if he/she wants.
> >>> > > > > > >
> >>> > > > > > > What do you think?
> >>> > > > > > >
> >>> > > > > > > Regards,
> >>> > > > > > > Gyula
> >>> > > > > > >
> >>> > > > > > >
> >>> > > > > >
> >>> > > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> >>
> >>
>

--089e01419b1c05323d05168395f7--
#|#<CANMXwW2io3ChXr-=0vt7Z9CgkJ4KwDuJQ2YYPr79kULAJ=Mz_A@mail.gmail.com>##//##<CA+faj9zeZsYzz09pfM8Kv33WrnUja0UEEFqmOr0L+HgDFAZd8w@mail.gmail.com>#|#2015-05-20-13:07:27#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Re: [DISCUSS] Re-add record copy to chained operator calls#|#
--001a1133b066b9543c0516831b4e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

We are designing a system for stateful stream computations, assuming long
standing operators that gather and store data as the stream evolves (unlike
in the dataset api). Many programs, like windowing, sampling etc hold the
state in the form of past data. And without careful understanding of the
runtime these programs will break or have unnecessary copies.

This is why I think immutability should be the default so we can have a
clear dataflow model with immutable streams.

I see absolutely no reason why we cant have the non-copy version as an
optional setting for the users.


On Wed, May 20, 2015 at 2:21 PM, Paris Carbone <parisc@kth.se> wrote:

> @stephan I see your point. If we assume that operators do not hold
> references in their state to any transmitted records it works fine. We
> therefore need to make this clear to the users. I need to check if that
> would break semantics in SAMOA or other integrations as well that assume
> immutability. For example in SAMOA there are often local metric objects
> that are being constantly mutated and simply forwarded periodically to
> other (possibly chained) operators that need to evaluate them.
>
> ________________________________________
> From: Gyula F=C3=B3ra <gyfora@apache.org>
> Sent: Wednesday, May 20, 2015 2:06 PM
> To: dev@flink.apache.org
> Subject: Re: [DISCUSS] Re-add record copy to chained operator calls
>
> "Copy before putting it into a window buffer and any other group buffer."
>
> Exactly my point. Any stateful operator should be able to implement
> something like this without having to worry about copying the object (and
> at this point the user would need to know whether it comes from the network
> to avoid unnecessary copies), so I don't agree with leaving the copy off.
>
> The user can of course specify that the operator is mutable if he wants
> (and he is worried about the performance), But I still think the default
> behaviour should be immutable.
> We cannot force users to not hold object references and also it is a quite
> unnatural way of programming in a language like java.
>
>
> On Wed, May 20, 2015 at 1:39 PM, Stephan Ewen <sewen@apache.org> wrote:
>
> > I am curious why the copying is actually needed.
> >
> > In the batch API, we chain and do not copy and it is rather predictable.
> >
> > The cornerpoints of that design is to follow these rules:
> >
> >  1) Objects read from the network or any buffer are always new objects.
> > That comes naturally when they are deserialized as part of that (all
> > buffers store serialized)
> >
> >  2) After a function returned a record (or gives one to the collector),
> it
> > if given to the chain of chained operators, but after it is through the
> > chain, no one else holds a reference to that object.
> >      For that, it is crucial that objects are not stored by reference,
> but
> > either stored serialized, or a copy is stored.
> >
> > This is quite solid in the batch API. How about we follow the same
> paradigm
> > in the streaming API. We would need to adjust the following:
> >
> > 1) Do not copy between operators (I think this is the case right now)
> >
> > 2) Copy before putting it into a window buffer and any other group
> buffer.
> >
> >
> >
> >
> >
> >
> >
> >
> > On Wed, May 20, 2015 at 1:22 PM, Aljoscha Krettek <aljoscha@apache.org>
> > wrote:
> >
> > > Yes, in fact I anticipated this. There is one central place where we
> > > can insert a copy step, in OperatorCollector in OutputHandler.
> > >
> > > On Wed, May 20, 2015 at 11:17 AM, Paris Carbone <parisc@kth.se> wrote:
> > > > I guess it was not intended ^^.
> > > >
> > > > Chaining should be transparent and not break the correct/expected
> > > behaviour.
> > > >
> > > >
> > > > Paris?
> > > >
> > > > On 20 May 2015, at 11:02, M=C3=A1rton Balassi <mbalassi@apache.org>
> wrote:
> > > >
> > > > +1 for copying.
> > > > On May 20, 2015 10:50 AM, "Gyula F=C3=B3ra" <gyfora@apache.org> wrote:
> > > >
> > > > Hey,
> > > >
> > > > The latest streaming operator rework removed the copying of the
> outputs
> > > > before passing them to chained operators. This is a major break for
> the
> > > > previous operator semantics which guaranteed immutability.
> > > >
> > > > I think this change leads to very indeterministic program behaviour
> > from
> > > > the user's perspective as only non-chained outputs/inputs will be
> > > mutable.
> > > > If we allow this to happen, users will start disabling chaining to
> get
> > > > immutability which defeats the purpose. (chaining should not affect
> > > program
> > > > behaviour just increase performance)
> > > >
> > > > In my opinion the default setting for each operator should be
> > > immutability
> > > > and the user could override this manually if he/she wants.
> > > >
> > > > What do you think?
> > > >
> > > > Regards,
> > > > Gyula
> > > >
> > > >
> > >
> >
>

--001a1133b066b9543c0516831b4e--
#|#<1432124473181.93862@kth.se>##//##<CA+faj9zq6=iUW=-z7+zbXpg5bFvZusucp5UQA9Y7v78Yyb4BiQ@mail.gmail.com>#|#2015-06-03-00:45:04#|#=?UTF-8?Q?Gyula_F=C3=B3ra?= <gyfora@apache.org>#|#Send events to parallel operator instances#|#
--001a11c37ac03bffc105178f04dc
Content-Type: text/plain; charset=UTF-8

Hi,

I am wondering, what is the suggested way to send some events directly to
another parallel instance in a flink job? For example from one mapper to
another mapper (of the same operator).

Do we have any internal support for this? The first thing that we thought
of is iterations but that is clearly an overkill.

Cheers,
Gyula

--001a11c37ac03bffc105178f04dc--
#|#null##//##<CAADvThdOY6hH=fX8Q=FO4fvk7-v1Qha87A6eie8CLtKrSua-5w@mail.gmail.com>#|#2014-08-29-09:34:38#|#Asterios Katsifodimos <asterios.katsifodimos@tu-berlin.de>#|#Re: Replacing JobManager with Scala implementation#|#
--089e0158b05ec6776d0501c15a98
Content-Type: text/plain; charset=UTF-8

I agree that using Akka's actors from Java results in very ugly code.
Hiding the internals of Akka behind Java reflection looks better but breaks
the principles of actors. For me it is kind of a deal breaker for using
Akka from Java.  I think that Till has more reasons to believe that Scala
would be a more appropriate for building a new Job/Task Manager.

I think that this discussion should focus on 4 main aspects:
1. Performance
2. Implementability
3. Maintainability
4. Available Tools

1. Performance: Since that the job of the JobManager and the TaskManager is
to 1) exchange messages in order to maintain a distributed state machine
and 2) setup connections between task managers, 3) detect failures etc.. In
these basic operations, performance should not be an issue. Akka was proven
to scale quite well with very low latency. I guess that the low level
"plumbing" (serialization, connections, etc.) will continue in Java in
order to guarantee high performance. I have no clue on what's happening
with memory management and whether this will be implemented in Java or
Scala and the respective consequences. Please comment.

2. Since the Job/Task Manager is going to be essentially implemented from
scratch, given the power of Akka, it seems to me that the implementation
will be   easier, shorter and less verbose in Scala, given that Till is
comfortable enough with Scala.

3. Given #2, maintaining the code and trying out new ideas in Scala would
take less time and effort. But maintaining low level plumbing in Java and
high level logic in Scala scares me. Anyone that has done this before could
comment on this?

4. Tools: Robert has raised some issues already but I think that tools will
get better with time.

Given the above, I would focus on #3 to be honest. Apart from this, going
the Scala way sounds like a great idea. I really second Kostas' opinion
that if large changes are going to happen, this is the best moment.

Cheers,
Asterios



On Fri, Aug 29, 2014 at 1:02 AM, Till Rohrmann <till.rohrmann@gmail.com>
wrote:

> I also agree with Robert and Kostas that it has to be a community decision.
> I understand the problems with Eclipse and the Scala IDE which is a pain in
> the ass. But eventually these things will be fixed. Maybe we could also
> talk to the typesafe guy and tell him that this problem bothers us a lot.
>
> I also believe that the project is not about a specific programming
> language but a problem we want to tackle with Flink. From time to time it
> might be necessary to adapt the tools in order to reach the goal. In fact,
> I don't believe that Scala parts would drive people away from the project.
> Instead, Scala enthusiasts would be motivated to join us.
>
> Actually I stumbled across a quote of Leibniz which put's my point of view
> quite accurately in a nutshell:
>
> In symbols one observes an advantage in discovery which is greatest when
> they express the exact nature of a thing briefly and, as it were, picture
> it; then indeed the labor of thought is wonderfully diminished -- Gottfried
> Wilhelm Leibniz
>
>
> On Thu, Aug 28, 2014 at 12:57 PM, Kostas Tzoumas <ktzoumas@apache.org>
> wrote:
>
> > On Thu, Aug 28, 2014 at 11:49 AM, Robert Metzger <rmetzger@apache.org>
> > wrote:
> >
> > >
> > > Changing the programming language of a very important system component
> is
> > > something we should carefully discuss.
> > >
> >
> > Definitely agree, I think the community should discuss this very
> carefully.
> >
> >
> > > I understand that Akka is written in Scala and that it will be much
> more
> > > natural to implement the actor based system using Scala.
> > > I see the following issues that we should consider:
> > > Until now, Flink is clearly a project implemented only in Java. The
> Scala
> > > API basically sits on top of the Java-based runtime. We do not really
> > > depend on Scala (we could easily remove the Scala API if we want to).
> > > Having code written in Scala in the main system will add a hard
> > dependency
> > > on a scala version.
> > > Being a pure Java project has some advantages: I think its a fact that
> > > there are more Java programmers than Scala programmers. So our chances
> of
> > > attracting new contributors are higher when being a Java project.
> > > On the other hand, we could maybe attract Scala developers to our
> > project.
> > > But that has not happened (for contributors, not users!) so far for our
> > > Scala API, so I don't see any reason for that to happen.
> > >
> > >
> > This is definitely an issue to consider. We need to carefully weight how
> > important this issue is. If we want to break things, incubation is the
> > right time to do it. Below are some arguments in favor of breaking
> things,
> > but do keep in mind that I am undecided, and I would really like to see
> the
> > community weighing in.
> >
> > First, I would dare say that the primary reason for someone to contribute
> > to Flink so far has not been that the code is written in Java, but more
> the
> > content and nature of the project. Most contributors are Big Data
> > enthusiasts in some way or another.
> >
> > Second, Scala projects have attracted contributors in the past.
> >
> > Third, it should not be too hard for someone that does not know Scala to
> > contribute to a different component if the interfaces are clear.
> >
> >
> > > Another issue is tooling: There are a lot of problems with Scala and
> > > Eclipse: I've recently switched to Eclipse Luna. It seems to be
> > impossible
> > > to compile Scala code with Luna because ScalaIDE does not properly cope
> > > with it.
> > > Even with Eclipse versions that are supported by ScalaIDE, you have to
> > > manually install 3 plugins, some of them are not available in the
> Eclipse
> > > Marketplace. So with a JobManager written in Scala, users can not just
> > > import our project as a Maven project into Eclipse and start
> developing.
> > > The support for Maven is probably also limited. For example, I don't
> know
> > > if there is a checkstyle plugin for Scala.
> > >
> > > I'm looking forward to hearing other opinions on this issue. As I said
> in
> > > the beginning, we should exchange arguments on this and think about it
> > for
> > > some time before we decide on this.
> > >
> > Best,
> > > Robert
> > >
> > >
> > >
> > > On Thu, Aug 28, 2014 at 1:08 AM, Till Rohrmann <trohrmann@apache.org>
> > > wrote:
> > >
> > > > Hi guys,
> > > >
> > > > I currently working on replacing the old rpc infrastructure with an
> > akka
> > > > based actor system. In the wake of this change I will reimplement the
> > > > JobManager and TaskManager which will then be actors. Akka offers a
> > Java
> > > > API but the implementation turns out to be very verbose and
> laborious,
> > > > because Java 6 and 7 do not support lambdas and pattern matching.
> Using
> > > > Scala instead, would allow a far more succinct and clear
> implementation
> > > of
> > > > the JobManager and TaskManager. Instead of a lot of if statements
> using
> > > > instanceof to figure out the message type, we could simply use
> pattern
> > > > matching. Furthermore, the callback functions could simply be Scala's
> > > > anonymous functions. Therefore I would propose to use Scala for these
> > two
> > > > systems.
> > > >
> > > > The Akka system uses the slf4j library as logging interface.
> Therefore
> > I
> > > > would also propose to replace the jcl logging system with the slf4j
> > > logging
> > > > system. Since we want to use Akka in many parts of the runtime system
> > and
> > > > it recommends using logback as logging backend, I would also like to
> > > > replace log4j with logback. But this change should inflict only few
> > > changes
> > > > once we established the slf4j logging interface everywhere.
> > > >
> > > > What do you guys think of that idea?
> > > >
> > > > Best regards,
> > > >
> > > > Till
> > > >
> > >
> >
>

--089e0158b05ec6776d0501c15a98--
#|#<fdeb1f7c16d6415c839129b241f9d97a@EX-MBX01.tubit.win.tu-berlin.de>##//##<CAADvTheSg_cWiy4LGLydiUBk3UucKt=LHw7oV9wWmDvyMepMUw@mail.gmail.com>#|#2014-09-18-10:36:44#|#Asterios Katsifodimos <asterios.katsifodimos@tu-berlin.de>#|#Re: [VOTE] Style of Flink squirrel logo for website and public accounts#|#
--bcaec5014c9fd28dcf0503548dad
Content-Type: text/plain; charset=UTF-8

+1 Colored

On Thu, Sep 18, 2014 at 12:34 PM, Kostas Tzoumas <ktzoumas@apache.org>
wrote:

> +1 for COLORED
>
> On Thu, Sep 18, 2014 at 12:31 PM, Ufuk Celebi <uce@apache.org> wrote:
>
> > +1 for COLORED
> >
> > On Thu, Sep 18, 2014 at 12:30 PM, Gyula Fora <gyfora@apache.org> wrote:
> >
> > > +COLORED
> > >
> > > On 18 Sep 2014, at 12:28, Fabian Hueske <fhueske@apache.org> wrote:
> > >
> > > > Please vote on which style of Flink's squirrel logo should be used
> for
> > > > Flink's website and other public accounts (e.g., Twitter).
> > > >
> > > > The logo comes in three styles:
> > > > - COLORED (in Apache colors)
> > > > - WHITE (white logo on dark backgrounds)
> > > > - OUTLINED (black outlines)
> > > >
> > > > The alternative styles can be checked out here:
> > > > -->
> > > >
> > >
> >
> https://www.dropbox.com/sh/gmc943qco092p0i/AABgfMmoVCH-dVivWPuTdBJ3a/flink_contest2.pdf?dl=0
> > > >
> > > > The vote is open for the next 72 hours.
> > > > The style with the most PPMC votes is chosen for Flink's website and
> > > public
> > > > accounts.
> > > > If the vote ends with no single style having the most votes, a new
> vote
> > > > with an alternative voting scheme will be done.
> > > >
> > > > [ ] +COLORED The colored logo should be used for Flink's website and
> > > public
> > > > accounts
> > > > [ ] +WHITE The white logo should be used for Flink's website and
> public
> > > > accounts
> > > > [ ] +OUTLINED The outlined  logo should be used for Flink's website
> and
> > > > public accounts
> > > > [ ] -1 None of the proposed logo styles should be used for Flink's
> > > website
> > > > because...
> > >
> > >
> >
>

--bcaec5014c9fd28dcf0503548dad--
#|#<90815e13c4954e48a5f2021b78b0ee9c@EX-MBX01.tubit.win.tu-berlin.de>##//##<CAAS6=7iuaqba0TTdvFycvJM-NpqEAkum=0BOtyQTHEtFdarVxQ@mail.gmail.com>#|#2014-06-26-03:32:12#|#Marvin Humphrey <marvin@rectangular.com>#|#Fwd: Incubator PMC/Board report for Jul 2014 ([ppmc])#|#
(bcc to dev@brooklyn, dev@fleece, dev@flink, dev@slider)

Greetings,

Four podlings who are still in the initial "monthly" reporting phase did not
receive emailed report reminders today because of various bookkeeping
glitches:

*   Brooklyn
*   Fleece
*   Flink
*   Slider

Please file your reports this month, per the instructions below.

Best,

Marvin Humphrey
Incubator PMC

---------- Forwarded message ----------
From: Marvin <no-reply@apache.org>
Date: Wed, Jun 25, 2014 at 7:15 AM
Subject: Incubator PMC/Board report for Jul 2014 ([ppmc])
To: vxquery-dev@incubator.apache.org


Dear podling,

This email was sent by an automated system on behalf of the Apache Incubator
PMC.  It is an initial reminder to give you plenty of time to prepare your
quarterly board report.

The board meeting is scheduled for Wed, 16 July 2014, 10:30:30:00 PST. The
report for your podling will form a part of the Incubator PMC report. The
Incubator PMC requires your report to be submitted 2 weeks before the board
meeting, to allow sufficient time for review and submission (Wed, Jul 2nd).

Please submit your report with sufficient time to allow the incubator PMC, and
subsequently board members to review and digest. Again, the very latest you
should submit your report is 2 weeks prior to the board meeting.

Thanks,

The Apache Incubator PMC

Submitting your Report
----------------------

Your report should contain the following:

 * Your project name
 * A brief description of your project, which assumes no knowledge of the
   project or necessarily of its field
 * A list of the three most important issues to address in the move towards
   graduation.
 * Any issues that the Incubator PMC or ASF Board might wish/need to be aware
   of
 * How has the community developed since the last report
 * How has the project developed since the last report.

This should be appended to the Incubator Wiki page at:

  http://wiki.apache.org/incubator/July2014

Note: This manually populated. You may need to wait a little before this page
      is created from a template.

Mentors
-------
Mentors should review reports for their project(s) and sign them off on the
Incubator wiki page. Signing off reports shows that you are following the
project - projects that are not signed may raise alarms for the Incubator PMC.

Incubator PMC
#|#<20140625141504.2A2F72388D7D@eris.apache.org>##//##<CAAdrtT05Z+rwAMi7AtZTjwPiK9DaTT7Mdx5rMdt=1Jdh5L55Xw@mail.gmail.com>#|#2014-08-19-09:34:19#|#Fabian Hueske <fhueske@apache.org>#|#Re: Adding the streaming project to the main repository#|#
--20cf3011d96fbdd5b60500f82fc1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for the explanation!
Very nice set of features. Looking forward to check it out myself :-)


2014-08-18 21:38 GMT+02:00 Gyula F=C3=B3ra <gyula.fora@gmail.com>:

> Hey,
>
> The simple reduce is like what you said yes. But there are also grouped
> reduce which you can use by calling .groupBy(keyposition) and then reduce.
>
> Also there is reduce for windows: batchReduce and windowReduce batch gives
> you a sliding window over a predefined number of records, and window reduce
> gices you the same but by time. (also there are grouped versions of these)
>
> Cheers,
> Gyula
>
>
> On Mon, Aug 18, 2014 at 9:19 PM, Fabian Hueske <fhueske@apache.org> wrote:
>
> > Hi folks,
> >
> > great work!
> >
> > Looking at the example I have a quick question. What's the semantics of
> the
> > Reduce operator? I guess its not a window reduce.
> > Is it backed by a hash table and every input tuple updates the hash table
> > and returns the updated value?
> >
> > Cheers, Fabian
> >
> >
> > 2014-08-18 20:53 GMT+02:00 Stephan Ewen <sewen@apache.org>:
> >
> > > The streaming code is in "flink-addons", for new/experimental code.
> > >
> > > Documents should come over the next days/weeks, definitely before we
> make
> > > this part of the core.
> > >
> > > Right now, I would suggest to have a look at some of the examples, to
> > get a
> > > feeling for the addon, check for example this here:
> > >
> > >
> >
> https://github.com/apache/incubator-flink/tree/master/flink-addons/flink-streaming/flink-streaming-examples/src/main/java/org/apache/flink/streaming/examples/wordcount
> > >
> > > (The example reads a file for simplicity, but the project also provides
> > > connectors for Kafka, RabbitMQ, ...)
> > >
> >
>

--20cf3011d96fbdd5b60500f82fc1--
#|#<CA+faj9xWVOkTa8xLkb2=um1L6R05qttW-oopHZgTYLJ=5dbuPQ@mail.gmail.com>##//##<CAAdrtT089Rd3xMCyTXFxRSZnyPDmBZdij=F0L8p3izZCVhx2Vg@mail.gmail.com>#|#2014-06-18-12:32:00#|#Fabian Hueske <fhueske@apache.org>#|#Re: [stratosphere-users] FlatJoin implementation#|#
--20cf302ef8f40dad9f04fc1b7177
Content-Type: text/plain; charset=UTF-8

Why not?
You do

data1.join(data2).where(0).equalTo(0).projectFirst(0,1).projectSecond(1).types(Long.class,
Long.class, Long.class).flatMap(new MyFM())

The flatMap MyFM function works on Tuple3<Long, Long, Long> and not on a
Tuple2<Tuple2<Long,Long>, Tuple2<Long,Long>.


2014-06-18 13:34 GMT+02:00 Ufuk Celebi <u.celebi@fu-berlin.de>:

>
> On 18 Jun 2014, at 13:30, Robert Metzger <rmetzger@apache.org> wrote:
> > ---------- Forwarded message ----------
> > From: Fabian Hueske <fhueske@gmail.com>
> > Date: Wed, Jun 18, 2014 at 11:20 AM
> > Subject: Re: [stratosphere-users] FlatJoin implementation
> > To: "stratosphere-users@googlegroups.com" <
> > stratosphere-users@googlegroups.com>
> >
> > To the topic: This feature has been requested by quite a few people. So I
> > think it makes sense to provide this interface (plus joinFilter). The
> same
> > applies to Cross which is less often used though...
>
> +1
>
> > A less confusing workaround could be to use join.project() an flatMap.
>
> Do you mean join.project() and flatMap as a workaround for a flatJoin()?
> That will not work, will it?

--20cf302ef8f40dad9f04fc1b7177--
#|#<43FA1ED9-0718-4A37-9335-C39E4091D58F@fu-berlin.de>##//##<CAAdrtT0E4yBYTxbKHQrzLOapc7JQ-jB4s-vd-z6Wwc=kqhs5mg@mail.gmail.com>#|#2015-04-03-16:35:44#|#Fabian Hueske <fhueske@gmail.com>#|#Re: Release 0.9.0-milestone-1 preview#|#
--047d7b343338aab2f50512d4883d
Content-Type: text/plain; charset=UTF-8

All issues I listed are relevant for the milestone release, IMO.
On Apr 3, 2015 6:27 PM, "Stephan Ewen" <sewen@apache.org> wrote:

> I think this thread if for the milestone release.
>
> Let's track the issues that must be included in 0.9 in a separate track.
> You can also mark them as release blockers in JIRA.
>
> On Fri, Apr 3, 2015 at 11:16 AM, Fabian Hueske <fhueske@gmail.com> wrote:
>
> > Thanks Robert for pushing this forward.
> >
> > I'd like to have the following issues fixed in the release:
> > - FLINK[1656] by PR #525
> > - FLINK[1776] by PR #532
> > - FLINK[1664] by PR #541
> > - FLINK[1817] by PR #565
> > - Failed tests on Windows by PR #491
> >
> > Especially the first two fixes crucial.
> > They address semantic properties are which are newly designed for 0.9 and
> > prevent invalid execution plan.
> >
> > 2015-04-03 10:49 GMT+02:00 Robert Metzger <rmetzger@apache.org>:
> >
> > > Hi All,
> > >
> > > As discussed on this list, we've decided to create a release outside
> the
> > > regular 3 monthly release schedule for the ApacheCon announcement and
> for
> > > giving our users a convenient way of trying out the great new features.
> > >
> > > This thread is not an official release vote. It is meant for testing
> the
> > > release script and for having a reference version to test against.
> > > Also, I hope that we don't need multiple [VOTE] threads.
> > >
> > > I've started creating a wiki page with a list of steps to verify a
> > release.
> > > Please add items if I forgot something:
> > > https://cwiki.apache.org/confluence/display/FLINK/Releasing
> > >
> > >
> > > The release artifacts:
> > > http://people.apache.org/~rmetzger/flink-0.9.0-milestone-1-rc0/
> > >
> > > The staging repository:
> > > https://repository.apache.org/content/repositories/orgapacheflink-1034
> > >
> > >
> > > The release commit (currently only in my GitHub account):
> > >
> > >
> >
> https://github.com/rmetzger/flink/commit/484df55a42f0708bb5bbbd6c19778e2d7c078f1c
> > >
> > >
> > > Lets collect everything we want to fix for the final release here.
> > > Please remember that we have to start the [VOTE] next week, otherwise
> > we'll
> > > miss the ApacheCon announcement.
> > >
> >
>

--047d7b343338aab2f50512d4883d--
#|#<CANC1h_t++hjthhb_D=T=ZyXe2ovhGpPASVV7=PgqJA06xdqipQ@mail.gmail.com>##//##<CAAdrtT0HF9S1Dg-rvQ2h0dvKRUJAmFeZhQL51maExsS95eGBUA@mail.gmail.com>#|#2014-10-31-21:18:04#|#Fabian Hueske <fhueske@apache.org>#|#Re: Hi / Aggregation support#|#
--089e0115ee9ce9e0630506be86a8
Content-Type: text/plain; charset=UTF-8

Hi Viktor,

welcome on the dev mailing list! :-)

I agree that Flink's aggregations should be improved in various aspects:
- support more aggregation functions. Currently only MIN, MAX, SUM are
supported. Adding COUNT and AVG would be nice!
- support for multiple aggregations per field
- support for aggregations on POJO DataSets

How about to always return Tuples as the result of an aggregation. For
example something like:

DataSet<Tuple2<String, Integer>> ds = ...
DataSet<Tuple4<Tuple2<String,Integer>,Integer, Integer, Long> result ds.groupBy(0).min(1).andMax(1).andCnt();

or

DataSet<Tuple2<String, Integer>> ds = ...
DataSet<Tuple4<String,Integer, Integer, Long> result ds.groupBy(0).key(0).andMin(1).andMax(1).andCnt();

In the first version, an arbitrary element of the group is added to the
result to identify the keys. The second example explicitly extracts the key
from the original input data. POJO data types can be handled similarly by
specifying the member fields to aggregate (or copy as key) by name.

Doing aggregations "in-place" within an input data type (and leaving other
fields untouched) could be a special variant of this operator.

2014-10-31 18:50 GMT+01:00 Rosenfeld, Viktor <viktor.rosenfeld@tu-berlin.de>
:

> Hi everybody,
>
> First, I want to introduce myself to the community. I am a PhD student who
> wants to work with and improve Flink.
>
> Second, I thought to work on improving aggregations as a start. My first
> goal is to simplify the computaton of a field average. Basically, I want to
> turn this plan:
>
>     val input = env.fromCollection( Array(1L, 2L, 3L, 4L) )
>
>     input
>     .map { in => (in, 1L) }
>     .sum(0).andSum(1)
>     .map { in => in._1.toDouble / in._2.toDouble }
>     .print
>
> into this:
>
>     // val input = ...
>     input.average(0).print()
>
> My basic idea is to internally still add the counter field and execute the
> map and sum steps but to hide them from the user.
>
> Next, I want to support multiple aggregations so one can write something
> like:
>
>     input.min(0).max(0).sum(0).count(0).average(0)
>
> Internally, there should only be one pass over the input data and average
> should reuse the work done by sum and count.
>
> In September there was some discussion [1] on the semantics of the min/max
> aggregations vs. minBy/maxBy. The consensus was that min/max should not
> simply return the respective field value but return the entire tuple.
> However, for count/sum/average there is no specific tuple and it would also
> not work for combinations of min/max.
>
> One possible route is to simply return a random element, similar to MySQL.
> I think this can be very surprising to the user especially when min/max are
> combined.
>
> Another possibility is to return the tuple only for single invocations of
> min or max and return the field value for the other aggregation functions
> or combinations. This is also inconstent but appears to be more inline with
> people's expectation. Also, there might be two or more tuples with the same
> min/max value and then the question is which should be returned.
>
> I haven't yet thought about aggregations in a streaming context and I
> would appreciate any input on this.
>
> Best,
> Viktor
>
> [1]
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Aggregations-td1706.html
>
>

--089e0115ee9ce9e0630506be86a8--
#|#<44B1AB07-F993-436F-AE23-8CC4CCC08A54@tu-berlin.de>##//##<CAAdrtT0M59wwSAurhkPkwyZyjWOe20bGQ+OJ7hMn0Hsou10PCw@mail.gmail.com>#|#2014-09-24-21:04:23#|#Fabian Hueske <fhueske@apache.org>#|#Re: Planning Flink release 0.7-incubating#|#
--20cf3040e4a8df6c3a0503d605fd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I agree, a hard feature stop deadline might not be the best practice.

How about the following procedure:
We decide two (or three) weeks before a targeted release date about which
JIRAs to include. JIRAs that are selected for a release should be completed
or really close to completion (via progress estimates in JIRA).
After we decided which JIRAs to include in a release, we can use JIRA to
track the progress and dedicate another week exclusively for testing after
the last feature was completed.


2014-09-24 19:10 GMT+02:00 M=C3=A1rton Balassi <balassi.marton@gmail.com>:

> As for the streaming team we're also getting ready for the release, but a
> couple of days will be needed to finish the features that we would like to
> include.
>
>    - A little work is still needed for reduce operations and
>    groups/connected streams (any comment on Gyula's recent e-mail is really
>    appreciated :) )
>    - The examples are being updated to match the standard, check out the
>    WordCount. (
>
> https://github.com/mbalassi/incubator-flink/blob/streaming-new/flink-addons/flink-streaming/flink-streaming-examples/src/main/java/org/apache/flink/streaming/examples/wordcount/WordCount.java
> )
>    Hopefully it gives you some deja vu. :)
>
>
> On Wed, Sep 24, 2014 at 6:53 PM, Ufuk Celebi <uce@apache.org> wrote:
>
> >
> > On 24 Sep 2014, at 18:37, Robert Metzger <rmetzger@apache.org> wrote:
> >
> > > Hey guys,
> > >
> > > exactly 3 weeks ago, we discussed to do a feature freeze for the
> > > 0.7-incubating release today.
> > >
> > > From our initial feature list:
> > > - *Flink Streaming* "Beta Preview". I would suggest to ship the
> > streaming,
> > > but clearly mark it as a preview in the documentation.
> > > -* Java API Pojo improvements*: Code generation, key selection using a
> > > string-expression: https://issues.apache.org/jira/browse/FLINK-1032
> > >  - *Reworked Scala API*. Bring the Scala API in sync with the latest
> > > developments in the Java API:
> > > https://issues.apache.org/jira/browse/FLINK-641
> > >  -* Akka-based RPC service*:
> > > https://issues.apache.org/jira/browse/FLINK-1019
> > >  - *Kryo-based serialization*. This feature has been requested by many
> > > users. Mostly because they wanted to use Collections inside POJOs:
> > > https://issues.apache.org/jira/browse/FLINK-610
> > > - Rework JobManager internals to support incremental program rollout &
> > > execution
> > > - First parts of dynamic memory assignments
> > >
> > > The following features are in the master, as of today:
> > > - *Flink Streaming*
> > > - *Reworked Scala API*
> > > -* New Scheduler*
> > >
> > > We certainly need some days to test everything until we can start the
> > vote.
> > > Based on our experience with the last major release, I would really
> like
> > to
> > > do the testing and bugfixing BEFORE the first release candidate. For
> the
> > > 0.6-incubating release, we had 6  candidates)
> > >
> > > How do you guys feel about this? Should we wait a few more days for the
> > > release so that a few more features make it into the release?
> > >
> > > I'm undecided on this. On the one hand, its really nice to release on a
> > > regular schedule, but it also eats up some time and causes overhead
> > > (different branches etc.).
> > > I would really like to have the Java API Pojo improvements in the
> > release.
> > > I think I can finish it until end of this week.
> > >
> > > Opinions?
> >
> > I agree that the finished features (especially the Scala API) are nice
> for
> > a new release, but still I would like to wait a few more days.
> >
> > Some of the missing features are on the brink of being finished (e.g. the
> > Pojo improvements). I wouldn't want to invest a week in bug fixing and
> > doing the release vote, when the new features are likely to be finished
> > just a few days afterwards. And the upcoming features will definitely be
> > worth a release, so users can work with them. ;)
>

--20cf3040e4a8df6c3a0503d605fd--
#|#<CAKADb_PpAgcZ6teBqTni-4UMwT8fqyehsk6V8q0Ebhf2SVEPpA@mail.gmail.com>##//##<CAAdrtT0OXG7fUq8Z73Jog0DTshXLqN+BkdFvWoXGnxjFdq8wpA@mail.gmail.com>#|#2015-01-22-10:11:36#|#Fabian Hueske <fhueske@apache.org>#|#Re: Very strange behaviour of groupBy() -> sort() -> first()#|#
--001a113ba316954609050d3ae5d4
Content-Type: text/plain; charset=UTF-8

BTW, I think as well that global sorting is an important feature and
definitely missing in Flink (FLINK-598).
Enabling local sorting for data sinks is one step on the way which can be
rather easily solved (FLINK-1105).

If you would like to contribute to make sorting possible, I would be very
happy to guide you ;-)

Cheers, Fabian

2015-01-21 22:33 GMT+01:00 Fabian Hueske <fhueske@apache.org>:

> This should directly go into the API, IMO.
> As I said, there are several open JIRAs for this issue.
>
> 2015-01-21 22:29 GMT+01:00 Felix Neutatz <neutatz@googlemail.com>:
>
>> Thanks, @Fabian, your workaround works :)
>>
>> But I think this feature is really missing. Shall we add this
>> functionality
>> natively or via the proposed lib package?
>>
>> 2015-01-21 20:38 GMT+01:00 Fabian Hueske <fhueske@gmail.com>:
>>
>> > Chesnay is right.
>> > Right now, it is not possible to do want you want in a straightforward
>> way
>> > because Flink does not support to fully sort a data set (there are
>> several
>> > related issues in JIRA).
>> >
>> > A workaround would be to attach a constant value to each tuple, group on
>> > that (all tuples are sent to the same group), sort that group, and apply
>> > the first operator.
>> >
>> > 2015-01-21 20:22 GMT+01:00 Chesnay Schepler <
>> chesnay.schepler@fu-berlin.de
>> > >:
>> >
>> > > If i remember correctly first() returns the first n values for every
>> > > group. the javadocs actually don't make this behaviour very clear.
>> > >
>> > >
>> > > On 21.01.2015 19:18, Felix Neutatz wrote:
>> > >
>> > >> Hi,
>> > >>
>> > >> my use case is the following:
>> > >>
>> > >> I have a Tuple2<String,Long>. I want to group by the String and sum
>> up
>> > the
>> > >> Long values accordingly. This works fine with these lines:
>> > >>
>> > >> DataSet<Lineitem> lineitems = getLineitemDataSet(env);
>> > >> lineitems.project(new int
>> > []{3,0}).groupBy(0).aggregate(Aggregations.SUM,
>> > >> 1);
>> > >>
>> > >> After the aggregation I want to print the 10 groups with the highest
>> > sum,
>> > >> like:
>> > >>
>> > >> string1, 100L
>> > >> string2, 50L
>> > >> string3, 1L
>> > >>
>> > >> I tried that:
>> > >>
>> > >> lineitems.project(new int
>> > []{3,0}).groupBy(0).aggregate(Aggregations.SUM,
>> > >> 1).groupBy(0).sortGroup(1, Order.DESCENDING).first(3).print();
>> > >>
>> > >> But instead of 3 records, I get a lot more.
>> > >>
>> > >> Can see my error?
>> > >>
>> > >> Best regards,
>> > >>
>> > >> Felix
>> > >>
>> > >>
>> > >
>> >
>>
>
>

--001a113ba316954609050d3ae5d4--
#|#<CAAdrtT3h_Sris893NQvhe+Pz7_vn+fYJxTP-n_4Hgaaz1ya8OQ@mail.gmail.com>##//##<CAAdrtT0d4cz_2hSROXaEkWaxbaW1s2saX16FiEtCyFrppwCBag@mail.gmail.com>#|#2014-11-28-00:04:32#|#Fabian Hueske <fhueske@apache.org>#|#Re: New Flink website layout and frontage#|#
--089e01635396916e8e0508dfff9a
Content-Type: text/plain; charset=UTF-8

Thanks for putting this together. Looks really good!

IMO, the tag line should make clear that Flink is a system for parallel
data processing.


2014-11-27 23:35 GMT+01:00 Markl, Volker, Prof. Dr. <
volker.markl@tu-berlin.de>:

> +1
>
> Von meinem iPhone gesendet
>
> > Am 27.11.2014 um 20:37 schrieb "Gyula Fora" <gyula.fora@gmail.com>:
> >
> > +1
> >
> > I also think that at looks very good!
> > Stylish :)
> >
> >> On 27 Nov 2014, at 20:32, Stephan Ewen <sewen@apache.org> wrote:
> >>
> >> I like it as well. Much better than the current website :-)
> >>
> >> On Thu, Nov 27, 2014 at 8:30 PM, Aljoscha Krettek <aljoscha@apache.org>
> >> wrote:
> >>
> >>> The formatting of the Scala example code is a bit off in places. Other
> >>> than that it looks very good.
> >>>
> >>> On Thu, Nov 27, 2014 at 8:22 PM, Kostas Tzoumas <ktzoumas@apache.org>
> >>> wrote:
> >>>> Actually, I cancelled the commit.
> >>>>
> >>>> Perhaps this is a good time to comment on the new frontpage (the rest
> of
> >>>> the content is the same).
> >>>>
> >>>> You can see the frontage in pdf (two parts, as you would scroll down)
> >>> here:
> >>>>
> >>>
> https://www.dropbox.com/sh/4b9f4judlczfzkp/AADIsdw1vxyNVZH6oyWsAIWNa?dl=0
> >>>>
> >>>> I think we can sort out aesthetic concerns even after the website is
> up,
> >>> so
> >>>> I would like to focus this discussion more on the content.
> >>>>
> >>>>
> >>>> On Thu, Nov 27, 2014 at 7:48 PM, Kostas Tzoumas <ktzoumas@apache.org>
> >>> wrote:
> >>>>
> >>>>> Hi everyone,
> >>>>>
> >>>>> Ufuk, Robert, and I worked on a new layout for the Flink website, and
> >>>>> added some content to the frontpage to (hopefully) convey more
> clearly
> >>> what
> >>>>> Flink is all about and what are Flink's differentiating features (a
> >>> point
> >>>>> of criticism to the old website was often that it is very
> non-descript).
> >>>>>
> >>>>> The new layout is currently being uploaded, and it should be up in a
> >>>>> couple of hours.
> >>>>>
> >>>>> Let us know what you think when the new website is up!
> >>>>>
> >>>>> Kostas
> >>>>>
> >>>>>
> >>>>>
> >>>
> >
>

--089e01635396916e8e0508dfff9a--
#|#<EF85369C-6BA1-4753-B49F-49A63F300700@win.tu-berlin.de>##//##<CAAdrtT0nz_qPC9hRxMhvpWfMW5RHKwkBpQh7Ki-4a85VFkovrg@mail.gmail.com>#|#2014-11-14-13:32:37#|#Fabian Hueske <fhueske@apache.org>#|#Re: HBase 0.98 addon for Flink 0.8#|#
--089e0163539642682a0507d1a7ed
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

In this case, the initialization happens when the InputFormat is
instantiated at the submission client and the Table info is serialized as
part of the InputFormat and shipped out to all TaskManagers for execution.
However, if the initialization is done within configure it happens on each
TaskManager when initializing the InputFormat.
These are two separate JVMs in a distributed setting with different
classpaths.

How do you submit your job for execution?

2014-11-14 13:58 GMT+01:00 Flavio Pompermaier <pompermaier@okkam.it>:

> The strange thing us that everything works if I create HTable outside
> configure()..
> On Nov 14, 2014 10:32 AM, "Stephan Ewen" <sewen@apache.org> wrote:
>
> > I think that this is a case where the wrong classloader is used:
> >
> > If the HBase classes are part of the flink lib directory, they are loaded
> > with the system class loader. When they look for anything in the
> classpath,
> > they will do so with the system classloader.
> >
> > You configuration is in the user code jar that you submit, so it is only
> > available through the user-code classloader.
> >
> > Any way you can load the configuration yourself and give that
> configuration
> > to HBase?
> >
> > Stephan
> > Am 13.11.2014 22:06 schrieb "Flavio Pompermaier" <pompermaier@okkam.it>:
> >
> > > The only config files available are within the submitted jar. Things
> > works
> > > in eclipse using local environment while fails deploying to the cluster
> > > On Nov 13, 2014 10:01 PM, <fhueske@gmail.com> wrote:
> > >
> > > > Does the HBase jar in the lib folder contain a config that could be
> > used
> > > > instead of the config in the job jar file? Or is simply no config at
> > all
> > > > available when the configure method is called?
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > --
> > > > Fabian Hueske
> > > > Phone:      +49 170 5549438
> > > > Email:      fhueske@gmail.com
> > > > Web:         http://www.user.tu-berlin.de/fabian.hueske
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > From: Flavio Pompermaier
> > > > Sent: =E2=80=8EThursday=E2=80=8E, =E2=80=8E13=E2=80=8E. =E2=80=8ENovember=E2=80=8E, =E2=80=8E2014 =E2=80=8E21=E2=80=8E:=E2=80=8E43
> > > > To: dev@flink.incubator.apache.org
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > The hbase jar is in the lib directory on each node while the config
> > files
> > > > are within the jar file I submit from the web client.
> > > > On Nov 13, 2014 9:37 PM, <fhueske@gmail.com> wrote:
> > > >
> > > > > Have you added the hbase.jar file with your HBase config to the
> ./lib
> > > > > folders of your Flink setup (JobManager, TaskManager) or is it
> > bundled
> > > > with
> > > > > your job.jar file?
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > --
> > > > > Fabian Hueske
> > > > > Phone:      +49 170 5549438
> > > > > Email:      fhueske@gmail.com
> > > > > Web:         http://www.user.tu-berlin.de/fabian.hueske
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > From: Flavio Pompermaier
> > > > > Sent: =E2=80=8EThursday=E2=80=8E, =E2=80=8E13=E2=80=8E. =E2=80=8ENovember=E2=80=8E, =E2=80=8E2014 =E2=80=8E18=E2=80=8E:=E2=80=8E36
> > > > > To: dev@flink.incubator.apache.org
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > Any help with this? :(
> > > > >
> > > > > On Thu, Nov 13, 2014 at 2:06 PM, Flavio Pompermaier <
> > > > pompermaier@okkam.it>
> > > > > wrote:
> > > > >
> > > > > > We definitely discovered that instantiating HTable and Scan in
> > > > > configure()
> > > > > > method of TableInputFormat causes problem in distributed
> > environment!
> > > > > > If you look at my implementation at
> > > > > >
> > > > >
> > > >
> > >
> >
> https://github.com/fpompermaier/incubator-flink/blob/master/flink-addons/flink-hbase/src/main/java/org/apache/flink/addons/hbase/TableInputFormat.java
> > > > > > you can see that Scan and HTable were made transient and
> recreated
> > > > within
> > > > > > configure but this causes HBaseConfiguration.create() to fail
> > > searching
> > > > > for
> > > > > > classpath files...could you help us understanding why?
> > > > > >
> > > > > > On Wed, Nov 12, 2014 at 8:10 PM, Flavio Pompermaier <
> > > > > pompermaier@okkam.it>
> > > > > > wrote:
> > > > > >
> > > > > >> Usually, when I run a mapreduce job both on Spark and Hadoop I
> > just
> > > > put
> > > > > >> *-site.xml files into the war I submit to the cluster and that's
> > > it. I
> > > > > >> think the problem appeared when I made the HTable a private
> > > transient
> > > > > field
> > > > > >> and the table istantiation was moved in the configure method.
> > > > > >> Could it be a valid reason? we still have to make a deeper debug
> > but
> > > > I'm
> > > > > >> trying ro figure out where to investigate..
> > > > > >> On Nov 12, 2014 8:03 PM, "Robert Metzger" <rmetzger@apache.org>
> > > > wrote:
> > > > > >>
> > > > > >>> Hi,
> > > > > >>> Maybe its an issue with the classpath? As far as I know is
> Hadoop
> > > > > reading
> > > > > >>> the configuration files from the classpath. Maybe is the
> > > > hbase-site.xml
> > > > > >>> file not accessible through the classpath when running on the
> > > > cluster?
> > > > > >>>
> > > > > >>> On Wed, Nov 12, 2014 at 7:40 PM, Flavio Pompermaier <
> > > > > >>> pompermaier@okkam.it>
> > > > > >>> wrote:
> > > > > >>>
> > > > > >>> > Today we tried tp execute a job on the cluster instead of on
> > > local
> > > > > >>> executor
> > > > > >>> > and we faced the problem that the hbase-site.xml was
> basically
> > > > > >>> ignored. Is
> > > > > >>> > there a reason why the TableInputFormat is working correctly
> on
> > > > local
> > > > > >>> > environment while it doesn't on a cluster?
> > > > > >>> > On Nov 10, 2014 10:56 AM, "Fabian Hueske" <
> fhueske@apache.org>
> > > > > wrote:
> > > > > >>> >
> > > > > >>> > > I don't think we need to bundle the HBase input and output
> > > format
> > > > > in
> > > > > >>> a
> > > > > >>> > > single PR.
> > > > > >>> > > So, I think we can proceed with the IF only and target the
> OF
> > > > > later.
> > > > > >>> > > However, the fix for Kryo should be in the master before
> > > merging
> > > > > the
> > > > > >>> PR.
> > > > > >>> > > Till is currently working on that and said he expects this
> to
> > > be
> > > > > >>> done by
> > > > > >>> > > end of the week.
> > > > > >>> > >
> > > > > >>> > > Cheers, Fabian
> > > > > >>> > >
> > > > > >>> > >
> > > > > >>> > > 2014-11-07 12:49 GMT+01:00 Flavio Pompermaier <
> > > > > pompermaier@okkam.it
> > > > > >>> >:
> > > > > >>> > >
> > > > > >>> > > > I fixed also the profile for Cloudera CDH5.1.3. You can
> > build
> > > > it
> > > > > >>> with
> > > > > >>> > the
> > > > > >>> > > > command:
> > > > > >>> > > >       mvn clean install -Dmaven.test.skip=3Dtrue
> > > > -Dhadoop.profile=3D2
> > > > > >>> > > >  -Pvendor-repos,cdh5.1.3
> > > > > >>> > > >
> > > > > >>> > > > However, it would be good to generate the specific jar
> when
> > > > > >>> > > > releasing..(e.g.
> > > > > >>> > > >
> flink-addons:flink-hbase:0.8.0-hadoop2-cdh5.1.3-incubating)
> > > > > >>> > > >
> > > > > >>> > > > Best,
> > > > > >>> > > > Flavio
> > > > > >>> > > >
> > > > > >>> > > > On Fri, Nov 7, 2014 at 12:44 PM, Flavio Pompermaier <
> > > > > >>> > > pompermaier@okkam.it>
> > > > > >>> > > > wrote:
> > > > > >>> > > >
> > > > > >>> > > > > I've just updated the code on my fork (synch with
> current
> > > > > master
> > > > > >>> and
> > > > > >>> > > > > applied improvements coming from comments on related
> PR).
> > > > > >>> > > > > I still have to understand how to write results back to
> > an
> > > > > HBase
> > > > > >>> > > > > Sink/OutputFormat...
> > > > > >>> > > > >
> > > > > >>> > > > >
> > > > > >>> > > > > On Mon, Nov 3, 2014 at 12:05 PM, Flavio Pompermaier <
> > > > > >>> > > > pompermaier@okkam.it>
> > > > > >>> > > > > wrote:
> > > > > >>> > > > >
> > > > > >>> > > > >> Thanks for the detailed answer. So if I run a job from
> > my
> > > > > >>> machine
> > > > > >>> > I'll
> > > > > >>> > > > >> have to download all the scanned data in a
> table..right?
> > > > > >>> > > > >>
> > > > > >>> > > > >> Always regarding the GenericTableOutputFormat it is
> not
> > > > clear
> > > > > >>> to me
> > > > > >>> > > how
> > > > > >>> > > > >> to proceed..
> > > > > >>> > > > >> I saw in the hadoop compatibility addon that it is
> > > possible
> > > > to
> > > > > >>> have
> > > > > >>> > > such
> > > > > >>> > > > >> compatibility using HBaseUtils class so the open
> method
> > > > should
> > > > > >>> > become
> > > > > >>> > > > >> something like:
> > > > > >>> > > > >>
> > > > > >>> > > > >> @Override
> > > > > >>> > > > >> public void open(int taskNumber, int numTasks) throws
> > > > > >>> IOException {
> > > > > >>> > > > >> if (Integer.toString(taskNumber + 1).length() > 6) {
> > > > > >>> > > > >> throw new IOException("Task id too large.");
> > > > > >>> > > > >> }
> > > > > >>> > > > >> TaskAttemptID taskAttemptID =3D
> > > > > >>> > TaskAttemptID.forName("attempt__0000_r_"
> > > > > >>> > > > >> + String.format("%" + (6 -
> Integer.toString(taskNumber +
> > > > > >>> > 1).length())
> > > > > >>> > > +
> > > > > >>> > > > >> "s"," ").replace(" ", "0")
> > > > > >>> > > > >> + Integer.toString(taskNumber + 1)
> > > > > >>> > > > >> + "_0");
> > > > > >>> > > > >>  this.configuration.set("mapred.task.id",
> > > > > >>> > taskAttemptID.toString());
> > > > > >>> > > > >> this.configuration.setInt("mapred.task.partition",
> > > > taskNumber
> > > > > +
> > > > > >>> 1);
> > > > > >>> > > > >> // for hadoop 2.2
> > > > > >>> > > > >> this.configuration.set("mapreduce.task.attempt.id",
> > > > > >>> > > > >> taskAttemptID.toString());
> > > > > >>> > > > >> this.configuration.setInt("mapreduce.task.partition",
> > > > > >>> taskNumber +
> > > > > >>> > 1);
> > > > > >>> > > > >>  try {
> > > > > >>> > > > >> this.context =3D
> > > > > >>> > > > >>
> > > > HadoopUtils.instantiateTaskAttemptContext(this.configuration,
> > > > > >>> > > > >> taskAttemptID);
> > > > > >>> > > > >> } catch (Exception e) {
> > > > > >>> > > > >> throw new RuntimeException(e);
> > > > > >>> > > > >> }
> > > > > >>> > > > >> final HFileOutputFormat2 outFormat =3D new
> > > > HFileOutputFormat2();
> > > > > >>> > > > >> try {
> > > > > >>> > > > >> this.writer =3D outFormat.getRecordWriter(this.context);
> > > > > >>> > > > >> } catch (InterruptedException iex) {
> > > > > >>> > > > >> throw new IOException("Opening the writer was
> > > interrupted.",
> > > > > >>> iex);
> > > > > >>> > > > >> }
> > > > > >>> > > > >> }
> > > > > >>> > > > >>
> > > > > >>> > > > >> But I'm not sure about how to pass the JobConf to the
> > > class,
> > > > > if
> > > > > >>> to
> > > > > >>> > > merge
> > > > > >>> > > > >> config fileas, where HFileOutputFormat2 writes the
> data
> > > and
> > > > > how
> > > > > >>> to
> > > > > >>> > > > >> implement the public void writeRecord(Record record)
> > API.
> > > > > >>> > > > >> Could I do a little chat off the mailing list with the
> > > > > >>> implementor
> > > > > >>> > of
> > > > > >>> > > > >> this extension?
> > > > > >>> > > > >>
> > > > > >>> > > > >> On Mon, Nov 3, 2014 at 11:51 AM, Fabian Hueske <
> > > > > >>> fhueske@apache.org>
> > > > > >>> > > > >> wrote:
> > > > > >>> > > > >>
> > > > > >>> > > > >>> Hi Flavio
> > > > > >>> > > > >>>
> > > > > >>> > > > >>> let me try to answer your last question on the user's
> > > list
> > > > > (to
> > > > > >>> the
> > > > > >>> > > best
> > > > > >>> > > > >>> of
> > > > > >>> > > > >>> my HBase knowledge).
> > > > > >>> > > > >>> "I just wanted to known if and how regiom splitting
> is
> > > > > >>> handled. Can
> > > > > >>> > > you
> > > > > >>> > > > >>> explain me in detail how Flink and HBase works?what
> is
> > > not
> > > > > >>> fully
> > > > > >>> > > clear
> > > > > >>> > > > to
> > > > > >>> > > > >>> me is when computation is done by region servers and
> > when
> > > > > data
> > > > > >>> > start
> > > > > >>> > > > flow
> > > > > >>> > > > >>> to a Flink worker (that in ky test job is only my pc)
> > and
> > > > how
> > > > > >>> ro
> > > > > >>> > > > >>> undertsand
> > > > > >>> > > > >>> better the important logged info to understand if my
> > job
> > > is
> > > > > >>> > > performing
> > > > > >>> > > > >>> well"
> > > > > >>> > > > >>>
> > > > > >>> > > > >>> HBase partitions its tables into so called "regions"
> of
> > > > keys
> > > > > >>> and
> > > > > >>> > > stores
> > > > > >>> > > > >>> the
> > > > > >>> > > > >>> regions distributed in the cluster using HDFS. I
> think
> > an
> > > > > HBase
> > > > > >>> > > region
> > > > > >>> > > > >>> can
> > > > > >>> > > > >>> be thought of as a HDFS block. To make reading an
> HBase
> > > > table
> > > > > >>> > > > efficient,
> > > > > >>> > > > >>> region reads should be locally done, i.e., an
> > InputFormat
> > > > > >>> should
> > > > > >>> > > > >>> primarily
> > > > > >>> > > > >>> read region that are stored on the same machine as
> the
> > IF
> > > > is
> > > > > >>> > running
> > > > > >>> > > > on.
> > > > > >>> > > > >>> Flink's InputSplits partition the HBase input by
> > regions
> > > > and
> > > > > >>> add
> > > > > >>> > > > >>> information about the storage location of the region.
> > > > During
> > > > > >>> > > execution,
> > > > > >>> > > > >>> input splits are assigned to InputFormats that can do
> > > local
> > > > > >>> reads.
> > > > > >>> > > > >>>
> > > > > >>> > > > >>> Best, Fabian
> > > > > >>> > > > >>>
> > > > > >>> > > > >>> 2014-11-03 11:13 GMT+01:00 Stephan Ewen <
> > > sewen@apache.org
> > > > >:
> > > > > >>> > > > >>>
> > > > > >>> > > > >>> > Hi!
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>> > The way of passing parameters through the
> > configuration
> > > > is
> > > > > >>> very
> > > > > >>> > old
> > > > > >>> > > > >>> (the
> > > > > >>> > > > >>> > original HBase format dated back to that time). I
> > would
> > > > > >>> simply
> > > > > >>> > make
> > > > > >>> > > > the
> > > > > >>> > > > >>> > HBase format take those parameters through the
> > > > constructor.
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>> > Greetings,
> > > > > >>> > > > >>> > Stephan
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>> > On Mon, Nov 3, 2014 at 10:59 AM, Flavio
> Pompermaier <
> > > > > >>> > > > >>> pompermaier@okkam.it>
> > > > > >>> > > > >>> > wrote:
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>> > > The problem is that I also removed the
> > > > > >>> GenericTableOutputFormat
> > > > > >>> > > > >>> because
> > > > > >>> > > > >>> > > there is an incompatibility between hadoop1 and
> > > hadoop2
> > > > > for
> > > > > >>> > class
> > > > > >>> > > > >>> > > TaskAttemptContext and TaskAttemptContextImpl..
> > > > > >>> > > > >>> > > then it would be nice if the user doesn't have to
> > > worry
> > > > > >>> about
> > > > > >>> > > > passing
> > > > > >>> > > > >>> > > pact.hbase.jtkey and pact.job.id parameters..
> > > > > >>> > > > >>> > > I think it is probably a good idea to remove
> > hadoop1
> > > > > >>> > > compatibility
> > > > > >>> > > > >>> and
> > > > > >>> > > > >>> > keep
> > > > > >>> > > > >>> > > enable HBase addon only for hadoop2 (as before)
> and
> > > > > decide
> > > > > >>> how
> > > > > >>> > to
> > > > > >>> > > > >>> mange
> > > > > >>> > > > >>> > > those 2 parameters..
> > > > > >>> > > > >>> > >
> > > > > >>> > > > >>> > > On Mon, Nov 3, 2014 at 10:19 AM, Stephan Ewen <
> > > > > >>> > sewen@apache.org>
> > > > > >>> > > > >>> wrote:
> > > > > >>> > > > >>> > >
> > > > > >>> > > > >>> > > > It is fine to remove it, in my opinion.
> > > > > >>> > > > >>> > > >
> > > > > >>> > > > >>> > > > On Mon, Nov 3, 2014 at 10:11 AM, Flavio
> > > Pompermaier <
> > > > > >>> > > > >>> > > pompermaier@okkam.it>
> > > > > >>> > > > >>> > > > wrote:
> > > > > >>> > > > >>> > > >
> > > > > >>> > > > >>> > > > > That is one class I removed because it was
> > using
> > > > the
> > > > > >>> > > deprecated
> > > > > >>> > > > >>> API
> > > > > >>> > > > >>> > > > > GenericDataSink..I can restore them but the
> it
> > > will
> > > > > be
> > > > > >>> a
> > > > > >>> > good
> > > > > >>> > > > >>> idea to
> > > > > >>> > > > >>> > > > > remove those warning (also because from what
> I
> > > > > >>> understood
> > > > > >>> > the
> > > > > >>> > > > >>> Record
> > > > > >>> > > > >>> > > APIs
> > > > > >>> > > > >>> > > > > are going to be removed).
> > > > > >>> > > > >>> > > > >
> > > > > >>> > > > >>> > > > > On Mon, Nov 3, 2014 at 9:51 AM, Fabian
> Hueske <
> > > > > >>> > > > >>> fhueske@apache.org>
> > > > > >>> > > > >>> > > > wrote:
> > > > > >>> > > > >>> > > > >
> > > > > >>> > > > >>> > > > > > I'm not familiar with the HBase connector
> > code,
> > > > but
> > > > > >>> are
> > > > > >>> > you
> > > > > >>> > > > >>> maybe
> > > > > >>> > > > >>> > > > looking
> > > > > >>> > > > >>> > > > > > for the GenericTableOutputFormat?
> > > > > >>> > > > >>> > > > > >
> > > > > >>> > > > >>> > > > > > 2014-11-03 9:44 GMT+01:00 Flavio
> Pompermaier
> > <
> > > > > >>> > > > >>> pompermaier@okkam.it
> > > > > >>> > > > >>> > >:
> > > > > >>> > > > >>> > > > > >
> > > > > >>> > > > >>> > > > > > > | was trying to modify the example
> setting
> > > > > >>> > > > hbaseDs.output(new
> > > > > >>> > > > >>> > > > > > > HBaseOutputFormat()); but I can't see any
> > > > > >>> > > HBaseOutputFormat
> > > > > >>> > > > >>> > > > > class..maybe
> > > > > >>> > > > >>> > > > > > we
> > > > > >>> > > > >>> > > > > > > shall use another class?
> > > > > >>> > > > >>> > > > > > >
> > > > > >>> > > > >>> > > > > > > On Mon, Nov 3, 2014 at 9:39 AM, Flavio
> > > > > Pompermaier
> > > > > >>> <
> > > > > >>> > > > >>> > > > > pompermaier@okkam.it
> > > > > >>> > > > >>> > > > > > >
> > > > > >>> > > > >>> > > > > > > wrote:
> > > > > >>> > > > >>> > > > > > >
> > > > > >>> > > > >>> > > > > > > > Maybe that's something I could add to
> the
> > > > HBase
> > > > > >>> > example
> > > > > >>> > > > and
> > > > > >>> > > > >>> > that
> > > > > >>> > > > >>> > > > > could
> > > > > >>> > > > >>> > > > > > be
> > > > > >>> > > > >>> > > > > > > > better documented in the Wiki.
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > > Since we're talking about the wiki..I
> was
> > > > > >>> looking at
> > > > > >>> > > the
> > > > > >>> > > > >>> Java
> > > > > >>> > > > >>> > > API (
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > >
> > > > > >>> > > > >>> > > > > >
> > > > > >>> > > > >>> > > > >
> > > > > >>> > > > >>> > > >
> > > > > >>> > > > >>> > >
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>>
> > > > > >>> > > >
> > > > > >>> > >
> > > > > >>> >
> > > > > >>>
> > > > >
> > > >
> > >
> >
> http://flink.incubator.apache.org/docs/0.6-incubating/java_api_guide.html
> > > > > >>> > > > >>> )
> > > > > >>> > > > >>> > > > > > > > and the link to the KMeans example is
> not
> > > > > working
> > > > > >>> > > (where
> > > > > >>> > > > it
> > > > > >>> > > > >>> > says
> > > > > >>> > > > >>> > > > For
> > > > > >>> > > > >>> > > > > a
> > > > > >>> > > > >>> > > > > > > > complete example program, have a look
> at
> > > > KMeans
> > > > > >>> > > > Algorithm).
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > > Best,
> > > > > >>> > > > >>> > > > > > > > Flavio
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > > On Mon, Nov 3, 2014 at 9:12 AM, Flavio
> > > > > >>> Pompermaier <
> > > > > >>> > > > >>> > > > > > pompermaier@okkam.it
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > > wrote:
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > >> Ah ok, perfect! That was the reason
> why
> > I
> > > > > >>> removed it
> > > > > >>> > > :)
> > > > > >>> > > > >>> > > > > > > >>
> > > > > >>> > > > >>> > > > > > > >> On Mon, Nov 3, 2014 at 9:10 AM,
> Stephan
> > > > Ewen <
> > > > > >>> > > > >>> > sewen@apache.org>
> > > > > >>> > > > >>> > > > > > wrote:
> > > > > >>> > > > >>> > > > > > > >>
> > > > > >>> > > > >>> > > > > > > >>> You do not really need a HBase data
> > sink.
> > > > You
> > > > > >>> can
> > > > > >>> > > call
> > > > > >>> > > > >>> > > > > > > >>> "DataSet.output(new
> > > > > >>> > > > >>> > > > > > > >>> HBaseOutputFormat())"
> > > > > >>> > > > >>> > > > > > > >>>
> > > > > >>> > > > >>> > > > > > > >>> Stephan
> > > > > >>> > > > >>> > > > > > > >>> Am 02.11.2014 23:05 schrieb "Flavio
> > > > > >>> Pompermaier" <
> > > > > >>> > > > >>> > > > > > pompermaier@okkam.it
> > > > > >>> > > > >>> > > > > > > >:
> > > > > >>> > > > >>> > > > > > > >>>
> > > > > >>> > > > >>> > > > > > > >>> > Just one last thing..I removed the
> > > > > >>> HbaseDataSink
> > > > > >>> > > > >>> because I
> > > > > >>> > > > >>> > > > think
> > > > > >>> > > > >>> > > > > it
> > > > > >>> > > > >>> > > > > > > was
> > > > > >>> > > > >>> > > > > > > >>> > using the old APIs..can someone
> help
> > me
> > > > in
> > > > > >>> > updating
> > > > > >>> > > > >>> that
> > > > > >>> > > > >>> > > class?
> > > > > >>> > > > >>> > > > > > > >>> >
> > > > > >>> > > > >>> > > > > > > >>> > On Sun, Nov 2, 2014 at 10:55 AM,
> > Flavio
> > > > > >>> > > Pompermaier <
> > > > > >>> > > > >>> > > > > > > >>> pompermaier@okkam.it>
> > > > > >>> > > > >>> > > > > > > >>> > wrote:
> > > > > >>> > > > >>> > > > > > > >>> >
> > > > > >>> > > > >>> > > > > > > >>> > > Indeed this time the build has
> been
> > > > > >>> successful
> > > > > >>> > :)
> > > > > >>> > > > >>> > > > > > > >>> > >
> > > > > >>> > > > >>> > > > > > > >>> > > On Sun, Nov 2, 2014 at 10:29 AM,
> > > Fabian
> > > > > >>> Hueske
> > > > > >>> > <
> > > > > >>> > > > >>> > > > > > fhueske@apache.org
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > > >>> > wrote:
> > > > > >>> > > > >>> > > > > > > >>> > >
> > > > > >>> > > > >>> > > > > > > >>> > >> You can also setup Travis to
> build
> > > > your
> > > > > >>> own
> > > > > >>> > > Github
> > > > > >>> > > > >>> > > > > repositories
> > > > > >>> > > > >>> > > > > > by
> > > > > >>> > > > >>> > > > > > > >>> > linking
> > > > > >>> > > > >>> > > > > > > >>> > >> it to your Github account. That
> > way
> > > > > >>> Travis can
> > > > > >>> > > > >>> build all
> > > > > >>> > > > >>> > > > your
> > > > > >>> > > > >>> > > > > > > >>> branches
> > > > > >>> > > > >>> > > > > > > >>> > >> (and
> > > > > >>> > > > >>> > > > > > > >>> > >> you can also trigger rebuilds if
> > > > > something
> > > > > >>> > > fails).
> > > > > >>> > > > >>> > > > > > > >>> > >> Not sure if we can manually
> > trigger
> > > > > >>> retrigger
> > > > > >>> > > > >>> builds on
> > > > > >>> > > > >>> > > the
> > > > > >>> > > > >>> > > > > > Apache
> > > > > >>> > > > >>> > > > > > > >>> > >> repository.
> > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > >>> > > > >>> > > > > > > >>> > >> Support for Hadoop 1 and 2 is
> > > indeed a
> > > > > >>> very
> > > > > >>> > good
> > > > > >>> > > > >>> > addition
> > > > > >>> > > > >>> > > > :-)
> > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > >>> > > > >>> > > > > > > >>> > >> For the discusion about the PR
> > > > itself, I
> > > > > >>> would
> > > > > >>> > > > need
> > > > > >>> > > > >>> a
> > > > > >>> > > > >>> > bit
> > > > > >>> > > > >>> > > > more
> > > > > >>> > > > >>> > > > > > > time
> > > > > >>> > > > >>> > > > > > > >>> to
> > > > > >>> > > > >>> > > > > > > >>> > >> become more familiar with
> HBase. I
> > > do
> > > > > >>> also not
> > > > > >>> > > > have
> > > > > >>> > > > >>> a
> > > > > >>> > > > >>> > > HBase
> > > > > >>> > > > >>> > > > > > setup
> > > > > >>> > > > >>> > > > > > > >>> > >> available
> > > > > >>> > > > >>> > > > > > > >>> > >> here.
> > > > > >>> > > > >>> > > > > > > >>> > >> Maybe somebody else of the
> > community
> > > > who
> > > > > >>> was
> > > > > >>> > > > >>> involved
> > > > > >>> > > > >>> > > with a
> > > > > >>> > > > >>> > > > > > > >>> previous
> > > > > >>> > > > >>> > > > > > > >>> > >> version of the HBase connector
> > could
> > > > > >>> comment
> > > > > >>> > on
> > > > > >>> > > > your
> > > > > >>> > > > >>> > > > question.
> > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > >>> > > > >>> > > > > > > >>> > >> Best, Fabian
> > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > >>> > > > >>> > > > > > > >>> > >> 2014-11-02 9:57 GMT+01:00 Flavio
> > > > > >>> Pompermaier <
> > > > > >>> > > > >>> > > > > > > pompermaier@okkam.it
> > > > > >>> > > > >>> > > > > > > >>> >:
> > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > >>> > > > >>> > > > > > > >>> > >> > As suggestes by Fabian I moved
> > the
> > > > > >>> > discussion
> > > > > >>> > > on
> > > > > >>> > > > >>> this
> > > > > >>> > > > >>> > > > > mailing
> > > > > >>> > > > >>> > > > > > > >>> list.
> > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > >>> > > > >>> > > > > > > >>> > >> > I think that what is still to
> be
> > > > > >>> discussed
> > > > > >>> > is
> > > > > >>> > > > >>> how  to
> > > > > >>> > > > >>> > > > > > retrigger
> > > > > >>> > > > >>> > > > > > > >>> the
> > > > > >>> > > > >>> > > > > > > >>> > >> build
> > > > > >>> > > > >>> > > > > > > >>> > >> > on Travis (I don't have an
> > > account)
> > > > > and
> > > > > >>> if
> > > > > >>> > the
> > > > > >>> > > > PR
> > > > > >>> > > > >>> can
> > > > > >>> > > > >>> > be
> > > > > >>> > > > >>> > > > > > > >>> integrated.
> > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > >>> > > > >>> > > > > > > >>> > >> > Maybe what I can do is to move
> > the
> > > > > HBase
> > > > > >>> > > example
> > > > > >>> > > > >>> in
> > > > > >>> > > > >>> > the
> > > > > >>> > > > >>> > > > test
> > > > > >>> > > > >>> > > > > > > >>> package
> > > > > >>> > > > >>> > > > > > > >>> > >> (right
> > > > > >>> > > > >>> > > > > > > >>> > >> > now I left it in the main
> > folder)
> > > so
> > > > > it
> > > > > >>> will
> > > > > >>> > > > force
> > > > > >>> > > > >>> > > Travis
> > > > > >>> > > > >>> > > > to
> > > > > >>> > > > >>> > > > > > > >>> rebuild.
> > > > > >>> > > > >>> > > > > > > >>> > >> > I'll do it within a couple of
> > > hours.
> > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > >>> > > > >>> > > > > > > >>> > >> > Another thing I forgot to say
> is
> > > > that
> > > > > >>> the
> > > > > >>> > > hbase
> > > > > >>> > > > >>> > > extension
> > > > > >>> > > > >>> > > > is
> > > > > >>> > > > >>> > > > > > now
> > > > > >>> > > > >>> > > > > > > >>> > >> compatible
> > > > > >>> > > > >>> > > > > > > >>> > >> > with both hadoop 1 and 2.
> > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > >>> > > > >>> > > > > > > >>> > >> > Best,
> > > > > >>> > > > >>> > > > > > > >>> > >> > Flavio
> > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > >>> > > > >>> > > > > > > >>> > >
> > > > > >>> > > > >>> > > > > > > >>> >
> > > > > >>> > > > >>> > > > > > > >>>
> > > > > >>> > > > >>> > > > > > > >>
> > > > > >>> > > > >>> > > > > > > >
> > > > > >>> > > > >>> > > > > > >
> > > > > >>> > > > >>> > > > > >
> > > > > >>> > > > >>> > > > >
> > > > > >>> > > > >>> > > >
> > > > > >>> > > > >>> > >
> > > > > >>> > > > >>> >
> > > > > >>> > > > >>>
> > > > > >>> > > > >>
> > > > > >>> > > > >>
> > > > > >>> > > > >>
> > > > > >>> > > > >
> > > > > >>> > > >
> > > > > >>> > >
> > > > > >>> >
> > > > > >>>
> > > > > >>
> > > > > >
> > > > > >
> > >
> >
>

--089e0163539642682a0507d1a7ed--
#|#<CAELUF_AeuTg_fDq6QFbC8YmZOPhC9w1UM0S3xVGoyHWrfK5tUw@mail.gmail.com>##//##<CAAdrtT0p3Y9hDg84WQT5MHRbhcOBv4F05wG3DBXdcQ2KdtPfNQ@mail.gmail.com>#|#2014-08-27-08:04:49#|#Fabian Hueske <fhueske@apache.org>#|#Re: [stratosphere-dev] Re: Project for GSoC#|#
--089e01635062ea330e050197dd6b
Content-Type: text/plain; charset=UTF-8

Hi Anirvan,

we have a JIRA that tracks the HadoopCompatibility feature:
https://apache.org/jira/browse/FLINK-838
<https://issues.apache.org/jira/browse/FLINK-838>
The basic mechanism is done, but has not been merged because we found a
cleaner way to integrate the feature.

There are different levels of Hadoop Compatibility and I did not fully
understand which kind of compatibility you need.

Conceptual Compatibility:
Flink already offers UDF interfaces which are very similar to Hadoop's Map,
Combine, and Reduce functions (FlatMap, FlatCombine, GroupReduce). These
interfaces are not source-code compatible, but porting the code should be
trivial. This is already there. If you're fine with porting your code from
Hadoop to Flink (which should be a very small effort) you're good to go.

Function-Level-Source Compatibility:
Providing UDF wrappers to use Hadoop Map and Reduce functions in Flink
programs is not difficult (since they are very similar to their Flink
versions). This is not yet included in the codebase but could be added at
rather short notice. We do already have interface compatibility for Hadoop
Input- and OutputFormats. So you can use these without changing the code.

Job-Level-Source Compatibility:
This is what we aimed for with the GSoC project. Here we want to support
the execution of a full Hadoop MR Job in Flink without changing the code.
However, this turned out to be a bit tricky if custom partitioner, sort,
and grouping-comparators are used in the job. Adding this feature will take
a bit more time.

Function- and Job-Level-Source compatibility will enable the use of already
existing Hadoop code. If you are implementing new analysis jobs anyways,
I'd go for a Flink implementation which eases many things such as secondary
sort, unions of multiple inputs, etc.

Cheers,
Fabian




2014-08-26 11:29 GMT+02:00 Robert Metzger <rmetzger@apache.org>:

> Hi Anirvan,
>
> I'm forwarding this message to dev@flink.incubator.apache.org. You need to
> send a (empty) message to dev-subscribe@flink.incubator.apache.org to
> subscribe to the dev list.
> The dev@ list is for discussions with the developers, planning etc. The
> user@flink.incubator.apache.org list is for user questions (for example
> troubles using the API, conceptual questions etc.)
> I think the message below is more suited for the dev@ list, since its
> basically a feature request.
>
> Regarding the names: We don't use Stratosphere anymore. Our codebase has
> been renamed to Flink and the "org.apache.flink" namespace. So ideally this
> confusion is finally out of the world.
>
> For those who want to have a look into the history of the message, see the
> Google Groups archive here:
> https://groups.google.com/forum/#!topic/stratosphere-dev/qYvJRSoMYWQ
>
> ---------- Forwarded message ----------
> From: Nirvanesque <nirvanesque.paris@gmail.com>
> Date: Tue, Aug 26, 2014 at 11:12 AM
> Subject: [stratosphere-dev] Re: Project for GSoC
> To: stratosphere-dev@googlegroups.com
> Cc: tsekis79@gmail.com
>
>
> Hello Artem and mentors,
>
> First of all nice greetings from INRIA, France.
> Hope you had an enjoyable experience in GSOC!
> Thanks to Robert (rmetzger) for forwarding me here ...
>
> At INRIA, we are starting to adopt Stratosphere / Flink.
> The top-level goal is to enhance performance in User Defined Functions
> (UDFs) with long workflows using multiple M-R, by using the larger set of
> Second Order Functions (SOFs) in Stratosphere / Flink.
> We will demonstrate this improvement by implementing some Use Cases for
> business purposes.
> For this purpose, we have chosen some customer analysis Use Cases using
> weblogs and related data, for 2 companies (who appeared interested to try
> using Stratosphere / Flink )
> - a mobile phone app developer: http://www.tribeflame.com
> - an anti-virus & Internet security software company: www.f-secure.com
> I will be happy to share with you these Use Cases, if you are interested.
> Just ask me here.
>
> At present, we are typically in the profiles of Alice-Bob-Sam, as described
> in your GSoC proposal
> <
> https://github.com/stratosphere/stratosphere/wiki/GSoC-2014-Project-Proposal-Draft-by-Artem-Tsikiridis
> >.
> :-)
> Hadoop seems to be the starting square for the Stratosphere / Flink
> journey.
> Same is the situation with developers in the above 2 companies :-)
>
> Briefly,
> We have installed and run some example programmes from Flink / Stratosphere
> (versions 0.5.2 and 0.6). We use a cluster (the grid5000 for our Hadoop &
> Stratosphere installations)
> We have some good understanding of Hadoop and its use in Streaming and
> Pipes in conjunction with scripting languages (Python & R specifically)
> In the first phase, we would like to run some "Hadoop-like" jobs (mainly
> multiple M-R workflows) on Stratosphere, preferably with extensive Java or
> Scala programming.
> I refer to your GSoC project map
> <
> https://github.com/stratosphere/stratosphere/wiki/%5BGSoC-14%5D-A-Hadoop-abstraction-layer-for-Stratosphere-%28Project-Map-and-Notes%29
> >
> which seems very interesting.
> If we could have a Hadoop abstraction as you have mentioned, that would be
> ideal for our first phase.
> In later phases, when we implement complex join and group operations, we
> would dive deeper into Stratosphere / Flink Java or Scala APIs
>
> Hence, I would like to know, what is the current status in this direction?
> What has been implemented already? In which version onwards? How to try
> them?
> What is yet to be implemented? When - which versions?
>
> You may also like to see my discussion with Robert on this page
> <
> http://flink.incubator.apache.org/docs/0.6-incubating/cli.html#comment-1558297261
> >.
>
> I am still mining in different discussions - here as well as on JIRA.
> Please do refer me to the relevant links, JIRA tickets, etc if that saves
> your time in re-typing large replies.
> It will also help us to understand the train of collective thinking in the
> Stratosphere / Flink roadmap.
>
> Thanks in advance,
> Anirvan
> PS : Apologies for using names / rechristened names (e.g. Flink /
> Stratosphere) as I am not sure, which name exactly to use currently.
>
>
> On Tuesday, February 25, 2014 10:23:09 PM UTC+1, Artem Tsikiridis wrote:
> >
> > Hello Fabian,
> >
> > On Tuesday, February 25, 2014 11:20:10 AM UTC+2, fhu...@gmail.com wrote:
> > > Hi Artem,
> > >
> > > thanks a lot for your interest in Stratosphere and participating in our
> > GSoC projects!
> > >
> > > As you know, Hadoop is the big elephant out there in the Big Data
> jungle
> > and widely adopted. Therefore, a Hadoop compatibility layer is a very!
> > important feature for any large scale data processing system.
> > > Stratosphere builds on foundations of MapReduce but generalizes its
> > concepts and provides a more efficient runtime.
> >
> > Great!
> >
> > > When you have a look at the Stratosphere WordCount example program, you
> > will see, that the programming principles of Stratosphere and Hadoop
> > MapReduce are quite similar, although Stratosphere is not compatible with
> > the Hadoop interfaces.
> >
> > Yes, I've looked into the example (Wordcount, k-means) I also run the big
> > test job you have locally and it seems to be ok.
> >
> > > With the proposed project we want to achieve, that Hadoop MapReduce
> jobs
> > can be executed on Stratosphere without changing a line of code (if
> > possible).
> > >
> > > We have already some pieces for that in place. InputFormats are done
> > (see https://github.com/stratosphere/stratosphere/
> > tree/master/stratosphere-addons/hadoop-compatibility), OutputFormats are
> > work in progress. The biggest missing piece is executing Hadoop Map and
> > Reduce tasks in Stratosphere. Hadoop provides quite a few interfaces
> (e.g.,
> > overwriting partitioning function and sorting comparators, counters,
> > distributed cache, ...). It would of course be desirable to support as
> many
> > of these interfaces as possible, but they can by added step-by-step once
> > the first Hadoop jobs are running on Stratosphere.
> >
> > So If I understand correctly, the idea is to create logical wrappers for
> > all interfaces used by Hadoop Jobs (the way it has been done with the
> > hadoop datatypes) so it can be run as completely transparently as
> possible
> > on Stratosphere in an efficient way. I agree, there are many interfaces,
> > but it's very interesting considering the way Stratosphere defines tasks,
> > which is a bit different (though, as you said, the principle is similar).
> >
> > I assume the focus is on the YARN version of Hadoop (new api)?
> >
> > And one last question, serialization for Stratosphere is java's default
> > mechanism, right?
> >
> > >
> > > Regarding your question about cloud deployment scripts, one of our team
> > members is currently working on this (see this thread:
> > https://groups.google.com/forum/#!topic/stratosphere-dev/QZPYu9fpjMo).
> > > I am not sure, if this is still in the making or already done. If you
> > are interested in this as well, just drop a line to the thread.
> Although, I
> > am not very familiar with the detail of this, my gut feeling is that this
> > would be a bit too less for an individual project. However, there might
> be
> > ways to extend this. So if you have any ideas, share them with us and we
> > will be happy to discuss them.
> >
> > Thank you for pointing up the topic. I will let you know if I come up
> with
> > anything for this. Probably after I try deploying it on openstack.
> >
> > >
> > > Again, thanks a lot for your interest and please don't hesitate to ask
> > questions. :-)
> >
> > Thank you for the helpful answers.
> >
> > Kind regards,
> > Artem
> >
> >
> > >
> > > Best,
> > > Fabian
> > >
> > >
> > > On Tuesday, February 25, 2014 9:12:10 AM UTC+1, tsek...@gmail.com
> > wrote:
> > > Dear Stratosphere devs and fellow GSoC potential students,
> > > Hello!
> > > I'm Artem, an undergraduate student from Athens, Greece. You can find
> me
> > on github (https://github.com/atsikiridis) and occasionally on
> > stackoverflow (http://stackoverflow.com/users/2568511/artem-tsikiridis).
> > Currently, however, I'm in Switzerland where I am doing my internship at
> > CERN as back-end software developer for INSPIRE, a library for High
> Energy
> > Physics (we're running on http://inspirehep.net/). The service is in
> > python( based on the open-source project http://invenio.net) and my
> > responsibilities are mostly the integration with Redis, database
> > abstractions, testing (unit, regression) and helping
> > > our team to integrate modern technologies and frameworks to the current
> > code base.
> > > Moreover, I am very interested in big data technologies, therefore
> > before coming to CERN I've been trying  to make my first steps in
> research
> > at the Big Data lab of AUEB, my home university. Mostly, the main
> objective
> > of the project I had been involved with, was the implementation of a
> > dynamic caching mechanism for Hadoop (in a way trying our cache instead
> of
> > the built-in distributed cache). Other techs involved where Redis,
> > Memcached, Ehacache (Terracotta). With this project we gained some
> insights
> > about the internals of hadoop (new api. old api, how tasks work, hadoop
> > serialization, the daemons running etc.) and hdfs, deployed clusters on
> > cloud computing platforms (Openstack with Nova,  Amazon EC2 with boto).
> We
> > also used the Java Remote API for some tests.
> > > Unfortunately, I have not used Stratosphere before in a research /prod
> > environment. I have only played with the examples on my local machine. It
> > is very interesting and I would love to learn more.
> > > There will probably be a learning curve for me on the Stratosphere side
> > but implementing a Hadoop Compatibility Layer seems like a very
> interesting
> > project and I believe I can be of use :)
> > > Finally, I was wondering whether there are some command-line tools for
> > deploying Stratosphere automatically for EC2 or Openstack clouds (for
> > example, Stratosphere specific abstractions on top of python boto api).
> Do
> > you that would make sense as a project?
> > > Pardon me for the length of this.
> > > Kind regards,
> > > Artem
>
>  --
> You received this message because you are subscribed to the Google Groups
> "stratosphere-dev" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to stratosphere-dev+unsubscribe@googlegroups.com.
> Visit this group at http://groups.google.com/group/stratosphere-dev.
> For more options, visit https://groups.google.com/d/optout.
>

--089e01635062ea330e050197dd6b--
#|#<CAGr9p8AhKO=iN9eu3AC7Y=r=EeyRVGdfBT+2Yyu3eijDQqz38A@mail.gmail.com>##//##<CAAdrtT0vgxPvWcRa86B6+gJ70A+j-XU2dkaaxRYTvRHtQh5GXQ@mail.gmail.com>#|#2015-04-03-11:11:43#|#Fabian Hueske <fhueske@gmail.com>#|#Re: NullPointerException in DeltaIteration when no ForwardedFileds annotation#|#
--001a1133b64abe4edd0512cff97a
Content-Type: text/plain; charset=UTF-8

That looks pretty much like a bug.

As you said, fwd fields annotations are optional and may improve the
performance of a program, but never change its semantics (if set correctly).

I'll have a look at it later.
Would be great if you could provide some data to reproduce the bug.
On Apr 3, 2015 12:48 PM, "Vasiliki Kalavri" <vasilikikalavri@gmail.com>
wrote:

> Hello to my squirrels,
>
> I've been getting a NullPointerException for a DeltaIteration program I'm
> trying to implement and I could really use your help :-)
> It seems that some of the input Tuples of the Join operator that I'm using
> to create the next workset / solution set delta are null.
> It also seems that adding ForwardedFields annotations solves the issue.
>
> I managed to reproduce the behavior using the ConnectedComponents example,
> by removing the "@ForwardedFieldsFirst("*")" annotation from
> the ComponentIdFilter join.
> The exception message is the following:
>
> Caused by: java.lang.NullPointerException
> at
>
> org.apache.flink.examples.java.graph.ConnectedComponents$ComponentIdFilter.join(ConnectedComponents.java:186)
> at
>
> org.apache.flink.examples.java.graph.ConnectedComponents$ComponentIdFilter.join(ConnectedComponents.java:1)
> at
>
> org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:198)
> at
>
> org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496)
> at
>
> org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:139)
> at
>
> org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:92)
> at
>
> org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362)
> at
>
> org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)
> at java.lang.Thread.run(Thread.java:745)
>
> I get this error locally with any sufficiently big dataset (~10000 nodes).
> When the annotation is in place, it works without problem.
> I also generated the optimizer plans for the two cases:
> - with annotation (working):
> https://gist.github.com/vasia/4f4dc6b0cc6c72b5b64b
> - without annotation (failing):
> https://gist.github.com/vasia/086faa45b980bf7f4c09
>
> After visualizing the plans, the main difference I see is that in the
> working case, the next workset node and the solution set delta nodes are
> merged, while in the failing case they are separate.
>
> Shouldn't this work with and without annotation (but be more efficient with
> the annotation in place)? Or am I missing something here?
>
> Thanks in advance for any help :))
>
> Cheers,
> - Vasia.
>

--001a1133b64abe4edd0512cff97a--
#|#<CAJZ2dcU=0Cr92HSA_=2iPFH9KK4_24MBDARszozOJ0TuT6JvsA@mail.gmail.com>##//##<CAAdrtT0zFxMH_MmO9ydyp_WMnRbfbtd7gEnUg_UO8BgLDgC0kA@mail.gmail.com>#|#2014-08-28-09:16:08#|#Fabian Hueske <fhueske@apache.org>#|#Re: Delta Iteration Obstuse Error Message#|#
--089e013d0c9ee8b95b0501acfa1f
Content-Type: text/plain; charset=UTF-8

I pushed a fix to allow null tuples as input for ProjectJoin (FLINK-1074).

You can update to the latest master or port the fix (just two lines:
https://github.com/apache/incubator-flink/commit/00840599a7a498cbd19d524ab5ad698365cbab4f
)

Cheers, Fabian


2014-08-28 2:09 GMT+02:00 Jack David Galilee <jgal2833@uni.sydney.edu.au>:

> Awesome, thanks Stephan for getting back to me so fast.
>
> I had assumed that was what I was what was going on with regards to
> getInput1() or getInput2().
> Good to know what I thought was happening was happening.
>
> The interesting thing that might help is that the null value is not being
> written out in my ReduceGroup
> function so it is coming from somewhere else in between. I did a trace of
> my program and it didn't write
> out a null value to the output collector, but if I remember correctly it
> was called one extra time than I'd
> expected.
> ________________________________________
> From: ewenstephan@gmail.com <ewenstephan@gmail.com> on behalf of Stephan
> Ewen <sewen@apache.org>
> Sent: 28 August 2014 09:52
> To: dev@flink.incubator.apache.org
> Subject: Re: Delta Iteration Obstuse Error Message
>
> Hi Jack!
>
> You are doing something very interesting there. I am not sure I am getting
> everything, but are some issues I can see...
>
>  - In the line where you join with the solution set:
> "join(iteration.getSolutionSet()).where(0).equalTo(0).getInput1(); // <--
> Referencing the solution set"
>    What happens is that after constructing the join, you grab that join's
> first input data set. This means, that you actually ignore the join and
> simply use its "input1", in that case the result of the reduceGroup
> operation. That explains why the program does not depend on the solution
> set. If you change the code to "getInput2()" then you construct teh join
> and take only its second input (the solution set). The other inputs are the
> "dangling", meaning they are not really consumed and thus regarded unused.
>
>   - The second error you get is a bug in Flink. This projection in the join
> cannot handle null inputs, which may occur when joining with the solution
> set. Let us fix that!
>
> Greetings,
> Stephan
>
>
>
> On Wed, Aug 27, 2014 at 11:10 AM, Jack David Galilee <
> jgal2833@uni.sydney.edu.au> wrote:
>
> > Hi Everyone,
> >
> >
> > I had an epiphany while reading through the Flink 0.6 API documentation
> > and decided to try a new method for my iterative algorithm, but it just
> > results in a weirder error. I've also included the error I was getting
> for
> > the suggestion that was posted earlier.
> >
> >
> > I'm sorry for not being able to provide full source code. If it is any
> > help all of my functions now produce Tuple2<String, String>(); Where the
> > initial dataset is also Tuple2<String,String>. The goal is to write out
> the
> > union of the results from all iterations where intersection of the set of
> > keys for iteration i and iteration i - 1 is the empty set.
> >
> >
> >         DeltaIteration<Tuple2<String, String>, Tuple2<String, String>>
> > iteration = transactions.
> >                 iterateDelta(initial, maxIterations, 0);
> >
> >
> >         DataSet<Tuple2<String, String>> ... = ....
> >                 flatMap(new
> > ...()).withBroadcastSet(iteration.getWorkset(), "..."). // <--
> Referencing
> > the working set
> >                 groupBy(0).
> >                 reduceGroup(new ...()).
> >                 withParameters(intValue).
> >
> > join(iteration.getSolutionSet()).where(0).equalTo(0).getInput1(); // <--
> > Referencing the solution set
> > //             projectFirst(1).projectSecond(1).types(String.class,
> > String.class);
> >
> >
> > Raises Exception. If I change it to get input2(), I get the same error,
> > but for the working set which is referenced through the broadcast.
> >
> >
> > Exception in thread "main" org.apache.flink.compiler.CompilerException:
> > Error: The step function does not reference the solution set.
> >     at
> >
> org.apache.flink.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:856)
> >     at
> >
> org.apache.flink.compiler.PactCompiler$GraphCreatingVisitor.postVisit(PactCompiler.java:616)
> >     at
> >
> org.apache.flink.api.common.operators.DualInputOperator.accept(DualInputOperator.java:283)
> >     at
> >
> org.apache.flink.api.common.operators.base.GenericDataSinkBase.accept(GenericDataSinkBase.java:289)
> >
> >
> > If I remove the getInput1() call and uncomment that last line it yields.
> I
> > was concerned that I was accidentally writing out a null value
> >
> > somewhere but I can't find out.
> >
> >
> > Exception in thread "main"
> > org.apache.flink.runtime.client.JobExecutionException:
> > java.lang.NullPointerException
> >     at
> >
> org.apache.flink.api.java.operators.JoinOperator$ProjectFlatJoinFunction.join(JoinOperator.java:935)
> >     at
> >
> org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:143)
> >     at
> >
> org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:510)
> >     at
> >
> org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:137)
> >     at
> >
> org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:92)
> >     at
> >
> org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:375)
> >     at
> >
> org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:265)
> >     at java.lang.Thread.run(Thread.java:744)
> >
> >
> > After more investigation it appears that the null pointer exists
> somewhere
> > between the the reduceGroup operator and next mapOperator as the next
> > mapOperator does not run after the reduceGroup.
> >
> >
> >
> > Thanks,
> >
> > Jack
> >
>

--089e013d0c9ee8b95b0501acfa1f--
#|#<1409184563963.15704@uni.sydney.edu.au>##//##<CAAdrtT0zq7unkbbO2Y_+Tr=7WuYybBQ=CW=NqyvYW7THxPq+aA@mail.gmail.com>#|#2014-10-30-16:56:58#|#Fabian Hueske <fhueske@apache.org>#|#Re: load balancing groups#|#
--089e0112cec22fe1900506a6c35f
Content-Type: text/plain; charset=UTF-8

Hi Martin,

Flink does not have features to mitigate data skew at the moment, such as
dynamic partitioning.
That would also "only" allow to process large groups as an individual
partitions and multiple smaller groups together in other partitions.
The issue of having a large group would not be solved with that. This is
more on the application-level right now and could for example be solved by
adding something like a group-cross operator...

I think your approach of emitting multiple smaller partitions from a
group-reduce, reshuffle (there is a rebalance operator [1]), and apply a
flatmap sounds like a good idea to me.
At least, I didn't come up with a better approach ;-)

Cheers, Fabian

[1]
http://flink.incubator.apache.org/docs/0.7-incubating/programming_guide.html#transformations

2014-10-28 20:53 GMT+01:00 Martin Neumann <mneumann@spotify.com>:

> I have some problem with load balancing and was wondering how to deal with
> this kind of problem in Flink.
> The input I have is a data set of grouped ID's that I join with metadata
> for each ID. Then I need to compare each Item in a group with each other
> item in that group and if necessary splitting it into different subgroups.
> In flink its a join followed by a group reduce.
>
> The problem is that the groups differ a lot in size. 90% of the groups are
> done in 5 minutes while the rest takes 2 hours. In order to get this more
> efficient I would need to distribute the N to N comparison that currently
> is done in the group reduce function. Anyone has an idea how I can do that
> in a simple way?
>
> My current Idea is to make the group reduce step emit computation
> partitions and then do another flat-map step to do the actual computation.
> Would this solve the problem?
>
> cheers Martin
>

--089e0112cec22fe1900506a6c35f--
#|#<CAHuUK3Xs1ibW=kikxQPFyV=H3gmFHPMrs3_VJB3R9sGONVOB+g@mail.gmail.com>##//##<CAAdrtT1JxVHPn1ogn-7f4m9eVJj-MXhbQek-xzuUuvqdg60QyQ@mail.gmail.com>#|#2014-06-26-07:35:23#|#Fabian Hueske <fhueske@apache.org>#|#Re: Maintaining Scala and Java APIs consistencies?#|#
--20cf303dd74ac9680c04fcb83ab2
Content-Type: text/plain; charset=UTF-8

Right now, both APIs are independently developed.
Of course we aim to provide the same functionality (in a similar way if
possible) with both APIs but we do not have a formal process to achieve
this.
The core set of functionality is covered by some example programs which are
implemented for both APIs.

I think this will become a more important issue in the future, esp. if we
add more APIs (Python is on the way).
Are there any best practices to maintain API consistency?

Fabian


2014-06-26 8:05 GMT+02:00 Henry Saputra <henry.saputra@gmail.com>:

> Hi All,
>
> How does Flink maintain the Scala and Java consistencies and check
> whether new API in Java also expose or implemented in Scala?
>
> Thanks,
>
> Henry
>

--20cf303dd74ac9680c04fcb83ab2--
#|#<CALuGr6ZXEjY81XEJtch882FKPKgJSvkieQOqCGvYiaDFruU-DQ@mail.gmail.com>##//##<CAAdrtT1Ovq+THVepp_-CL+FoSzov5swWNce4KYxLiLFR=SiBtA@mail.gmail.com>#|#2015-03-16-16:32:01#|#Fabian Hueske <fhueske@gmail.com>#|#Re: [DISCUSS] Name of Expression API and DataSet abstraction#|#
--089e0129440cd245aa05116a5cbc
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I am also more in favor of Rel and Relation, but DataTable nicely follows
the terms DataSet and DataStream.
 On Mar 16, 2015 4:58 PM, "Aljoscha Krettek" <aljoscha@apache.org> wrote:

> I like Relation or Rel, is shorter.
> On Mar 16, 2015 4:52 PM, "Hermann G=C3=A1bor" <reckoner42@gmail.com> wrote:
>
> > +1 for DataTable
> >
> > On Mon, Mar 16, 2015 at 4:11 PM Till Rohrmann <trohrmann@apache.org>
> > wrote:
> >
> > > +1 for DataTable
> > >
> > > On Mon, Mar 16, 2015 at 10:34 AM, M=C3=A1rton Balassi <
> > balassi.marton@gmail.com
> > > >
> > > wrote:
> > >
> > > > +1 for Max's suggestion.
> > > >
> > > > On Mon, Mar 16, 2015 at 10:32 AM, Ufuk Celebi <uce@apache.org>
> wrote:
> > > >
> > > > > On Fri, Mar 13, 2015 at 6:08 PM, Maximilian Michels <
> mxm@apache.org>
> > > > > wrote:
> > > > >
> > > > > >
> > > > > > Thanks for starting the discussion. We should definitely not keep
> > > > > > flink-expressions.
> > > > > >
> > > > > > I'm in favor of DataTable for the DataSet abstraction equivalent.
> > For
> > > > > > consistency, the package name should then be flink-table. At
> first
> > > > > > sight, the name seems kind of plain but I think it is quite
> > intuitive
> > > > > > because the API enables you to work in a SQL like fashion.
> > > > > >
> > > > >
> > > > >
> > > > > +1
> > > > >
> > > > > I think this is a very good suggestion. :-)
> > > > >
> > > > > (There is an associated issue, we shouldn't forget to close:
> > > > > https://issues.apache.org/jira/browse/FLINK-1623)
> > > > >
> > > >
> > >
> >
>

--089e0129440cd245aa05116a5cbc--
#|#<CANMXwW3qmjby8RO=7+=shTTYZ9dsUxsmCn0QxJbtjMTvM7BGtw@mail.gmail.com>##//##<CAAdrtT1gUaswQ4=XOqiJ_9VD2Rb99kSzzvnDSjgZP4RL+pypOw@mail.gmail.com>#|#2015-06-03-21:57:31#|#Fabian Hueske <fhueske@gmail.com>#|#Re: How do we want to maintain our documentation?#|#
--089e0160acae7ec1c80517a424b1
Content-Type: text/plain; charset=UTF-8

+1 for Robert's suggestion.

In fact, I thought this was already our practice. Also I would not allow
exceptions from that rule in the "stable" codebase. Writing documentation
and describing how stuff should be used lets you think about it in a
different way and can help to make the feature better. Also features which
are not documented do not exist. We might be less strict with changes to
flink-staging but as soon as it moves out, documentation must be "complete".

However, such a rule will only help to have (more) complete documentation,
but not necessarily good documentation. I observed that documentation tends
to cluttered and more fragmented if more people add and change stuff. I
guess from time to time, documentation just needs to be restructured and
partially rewritten.

2015-06-03 18:19 GMT+02:00 Vasiliki Kalavri <vasilikikalavri@gmail.com>:

> Hi,
>
> in principle I agree with Robert's suggestion.
>
> I can only see two cases where this might not work well:
>
> - if the change is something big / totally new and the documentation to be
> written is large. In this case, I would prefer to have a separate issue for
> docs to ensure better quality and review, otherwise we might end up with
> following "improve docs" issues
>
> - if the code change is blocking some other issue. Then, it might make
> sense to merge the code fast and update the docs fast.
>
> In any case though, I agree that it's not a good idea to have someone
> adding the missing docs just before the release.
>
> Cheers,
> -Vasia.
>
> On 3 June 2015 at 17:27, Till Rohrmann <trohrmann@apache.org> wrote:
>
> > For me it also makes sense that the contributor who has implemented the
> > changes and thus knows them best should update the documentation
> > accordingly.
> >
> > +1
> >
> > On Wed, Jun 3, 2015 at 5:25 PM, Chiwan Park <chiwanpark@icloud.com>
> wrote:
> >
> > > +1 Good. PR should contain documentation.
> > >
> > > Regards,
> > > Chiwan Park
> > >
> > > > On Jun 4, 2015, at 12:24 AM, Lokesh Rajaram <
> rajaram.lokesh@gmail.com>
> > > wrote:
> > > >
> > > > +1. I like this idea, not sure if my vote counts :)
> > > >
> > > > On Wed, Jun 3, 2015 at 8:21 AM, Robert Metzger <rmetzger@apache.org>
> > > wrote:
> > > >
> > > >> Hi,
> > > >>
> > > >> as part of making our codebase ready for the upcoming 0.9 release,
> > I've
> > > >> started to go over the documentation of Flink.
> > > >>
> > > >> It seems that our current approach for documenting stuff:
> > > >> - We implement and merge a feature
> > > >> - We open a JIRA for documenting it.
> > > >>
> > > >> Before the release, we realize that we have many open documentation
> > > issues
> > > >> (currently 26) and hope that somebody (in this case me) is fixing
> > them.
> > > >>
> > > >> Some of the pull requests also contain documentation, but certainly
> > not
> > > all
> > > >> of them.
> > > >>
> > > >>
> > > >> I am proposing to:
> > > >> - add a rule to our coding guidelines [1] which states that every
> > change
> > > >> that affects the documentation needs to update the documentation
> > > >> accordingly.
> > > >> - Committers have to make sure that pull request are following the
> > rule
> > > >> before accepting the change. Otherwise, we reject the pull request.
> > > >>
> > > >>
> > > >> [1] http://flink.apache.org/coding-guidelines.html
> > > >>
> > >
> > >
> > >
> > >
> > >
> > >
> >
>

--089e0160acae7ec1c80517a424b1--
#|#<CAJZ2dcWYUBJ92q+eDVF=eHw-RUd8SqUZDkRuKLSgvuUsz1tejQ@mail.gmail.com>##//##<CAAdrtT1kEUMtNJrRW2C5CozfqOnEkGZtgdsp8do+ZAF5+qxnYg@mail.gmail.com>#|#2014-12-11-10:02:53#|#Fabian Hueske <fhueske@apache.org>#|#Re: Forking off the 0.8 release branch.#|#
--089e013a15989581870509eddf47
Content-Type: text/plain; charset=UTF-8

+1

2014-12-11 11:00 GMT+01:00 Till Rohrmann <trohrmann@apache.org>:
>
> +1
>
> Looking forward merging the Akka changes into the 0.9-SNAPSHOT master.
>
> On Wed, Dec 10, 2014 at 9:06 PM, Robert Metzger <rmetzger@apache.org>
> wrote:
>
> > +1.
> >
> > I think adding the TypeHints pull request to the release is a good idea.
> > This allows us to give users a workaround if the type extraction is not
> > working.
> >
> > I'll have a look at the Hbase hadoop1 support.
> >
> > On Wed, Dec 10, 2014 at 7:35 PM, Stephan Ewen <sewen@apache.org> wrote:
> >
> > > Hi everyone!
> > >
> > > I would suggest to fork a 0.8 release branch very soon, to not stall
> the
> > > development of features that go into 0.9. The new Akka based
> coordination
> > > and the network stack extensions are required for many new features
> which
> > > are in the pipeline.
> > >
> > > How about we merge the below listed pull requests into the master, and
> > then
> > > branch the 0.8 release branch off and bump the master version to
> > > 0.9-SNAPSHOT.
> > >
> > > After we have forked the release branch, Marton (as release manager)
> > would
> > > create release candidates and collect status of the tests and patches.
> We
> > > can fix bugs on the release branch and cherry-pick the patches to the
> > > 0.9-SNAPSHOT branch.
> > >
> > > ----------------------------------------------
> > > Important Pull Requests to merge:
> > > ----------------------------------------------
> > >
> > > [FLINK-1287] LocalizableSplitAssigner prefers splits with less degrees
> of
> > > freedom
> > >
> > > Fixed java quickstart example
> > >
> > > [FLINK-998] Close TCP connections after destroying logical channels
> > >
> > > [FLINK-1302] Make JDBCInputFormat implement the NonParallelInput
> > interface
> > >
> > > [FLINK-1305] [FLINK-1304] Test for HadoopInputWrapper and NullWritable
> > > support
> > >
> > > Upgraded HBase addon to HBase 0.98.x and new Tuple APIs + fix of
> > > ExecutionEnvironment
> > > (this is partially merged, there are a few POM entries missing for
> > Hadoop 1
> > > support)
> > >
> > >
> > > ----------------------------------------------
> > > Optional Pull Requests to merge:
> > > ----------------------------------------------
> > >
> > > Add support for Subclasses, Interfaces, Abstract Classes as POJOs
> > >
> > > [FLINK-1245] Introduce TypeHints for Java API operators
> > >
> > > enable CSV Reader to ignore invalid lines like an empty line at the end
> > and
> > > comments - FLINK-1208
> > >
> > >
> > > Please veto the important pull requests (if they should not go in) and
> +1
> > > or -1 the optional ones, as you deem fitting.
> > >
> > >
> > > Greetings,
> > > Stephan
> > >
> >
>

--089e013a15989581870509eddf47--
#|#<CAC27z=N9Q6iz9A2=ib_CPgd+m4wbkhua2gnNen1nAHPSJZTN0w@mail.gmail.com>##//##<CAAdrtT1mGWp2Q01tGauxi6hqK+gE-bFu3w9nhJyOZhQTcJ2Lgg@mail.gmail.com>#|#2014-10-20-11:28:06#|#Fabian Hueske <fhueske@apache.org>#|#Re: [VOTE] Release Apache Flink 0.7.0 (incubating) (RC2)#|#
--001a1132e71e3f0b740505d90012
Content-Type: text/plain; charset=UTF-8

+1

- Checked all signatures and hashes
- Built from source archive (mvn clean install)
- Ran all examples with provided data locally on previous build

2014-10-18 22:38 GMT+02:00 Robert Metzger <rmetzger@apache.org>:

> Please vote on releasing the following candidate as Apache Flink
> (incubating) version 0.7.0
>
> This release will be the second major release for Flink in the incubator.
>
> -------------------------------------------------------------
> The commit to be voted on is in the branch "release-0.7.0-rc2" (commit
> cf4c6e7077fa4c3e3d96758b2f563ec17148a786):
> *http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/cf4c6e70
> <http://git-wip-us.apache.org/repos/asf/incubator-flink/commit/cf4c6e70>*
>
> The release artifacts to be voted on can be found at:
> *http://people.apache.org/~rmetzger/flink-0.7.0-incubating-rc2/
> <http://people.apache.org/~rmetzger/flink-0.7.0-incubating-rc2/>*
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/rmetzger.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapacheflink-1016
> -------------------------------------------------------------
>
>
>
> Please vote on releasing this package as Apache Flink 0.7.0 (incubating).
>
> The vote is open for the next 72 hours and passes if a majority of at least
> three +1 PPMC votes are cast.
>
> [ ] +1 Release this package as Apache Flink 0.7.0 (incubating)
> [ ] -1 Do not release this package because ...
>

--001a1132e71e3f0b740505d90012--
#|#<CAGr9p8AdtRt_6B5e+7r7zcSoovxSSUn6C1w0WqaTaN9cH_R8GQ@mail.gmail.com>##//##<CAAdrtT24SPd_X7d_DA5cQvXAK9w56o_dOCP4dnf3i6-WBwXEtw@mail.gmail.com>#|#2015-04-16-13:40:57#|#Fabian Hueske <fhueske@gmail.com>#|#Re: Apache Ignite#|#
--089e0160c1627d174a0513d79b16
Content-Type: text/plain; charset=UTF-8

Yes, I think that is an interesting idea. The question would be on which
level the integration would happen.
Adding Ignite as a data store to read data from and write it to should not
be too difficult.
A tighter integration such as using it as distributed hash table for ML
models might be interesting as well.

2015-04-12 11:53 GMT-05:00 sirinath <sirinath1978m@gmail.com>:

> I am wodering if tighter integration between Apache Ignite and Flink would
> be
> of benefit to both communities
>
>
>
> --
> View this message in context:
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Apache-Ignite-tp5115.html
> Sent from the Apache Flink (Incubator) Mailing List archive. mailing list
> archive at Nabble.com.
>

--089e0160c1627d174a0513d79b16--
#|#<1428857594665-5115.post@n3.nabble.com>##//##<CAAdrtT24vzmbfxWgn263WXDj4racgFd4CVowYY3AwQ_f2H-i7g@mail.gmail.com>#|#2014-11-03-08:51:32#|#Fabian Hueske <fhueske@apache.org>#|#Re: HBase 0.98 addon for Flink 0.8#|#
--001a1132e0baa4e8880506f0720c
Content-Type: text/plain; charset=UTF-8

I'm not familiar with the HBase connector code, but are you maybe looking
for the GenericTableOutputFormat?

2014-11-03 9:44 GMT+01:00 Flavio Pompermaier <pompermaier@okkam.it>:

> | was trying to modify the example setting hbaseDs.output(new
> HBaseOutputFormat()); but I can't see any HBaseOutputFormat class..maybe we
> shall use another class?
>
> On Mon, Nov 3, 2014 at 9:39 AM, Flavio Pompermaier <pompermaier@okkam.it>
> wrote:
>
> > Maybe that's something I could add to the HBase example and that could be
> > better documented in the Wiki.
> >
> > Since we're talking about the wiki..I was looking at the Java API (
> >
> http://flink.incubator.apache.org/docs/0.6-incubating/java_api_guide.html)
> > and the link to the KMeans example is not working (where it says For a
> > complete example program, have a look at KMeans Algorithm).
> >
> > Best,
> > Flavio
> >
> >
> > On Mon, Nov 3, 2014 at 9:12 AM, Flavio Pompermaier <pompermaier@okkam.it
> >
> > wrote:
> >
> >> Ah ok, perfect! That was the reason why I removed it :)
> >>
> >> On Mon, Nov 3, 2014 at 9:10 AM, Stephan Ewen <sewen@apache.org> wrote:
> >>
> >>> You do not really need a HBase data sink. You can call
> >>> "DataSet.output(new
> >>> HBaseOutputFormat())"
> >>>
> >>> Stephan
> >>> Am 02.11.2014 23:05 schrieb "Flavio Pompermaier" <pompermaier@okkam.it
> >:
> >>>
> >>> > Just one last thing..I removed the HbaseDataSink because I think it
> was
> >>> > using the old APIs..can someone help me in updating that class?
> >>> >
> >>> > On Sun, Nov 2, 2014 at 10:55 AM, Flavio Pompermaier <
> >>> pompermaier@okkam.it>
> >>> > wrote:
> >>> >
> >>> > > Indeed this time the build has been successful :)
> >>> > >
> >>> > > On Sun, Nov 2, 2014 at 10:29 AM, Fabian Hueske <fhueske@apache.org
> >
> >>> > wrote:
> >>> > >
> >>> > >> You can also setup Travis to build your own Github repositories by
> >>> > linking
> >>> > >> it to your Github account. That way Travis can build all your
> >>> branches
> >>> > >> (and
> >>> > >> you can also trigger rebuilds if something fails).
> >>> > >> Not sure if we can manually trigger retrigger builds on the Apache
> >>> > >> repository.
> >>> > >>
> >>> > >> Support for Hadoop 1 and 2 is indeed a very good addition :-)
> >>> > >>
> >>> > >> For the discusion about the PR itself, I would need a bit more
> time
> >>> to
> >>> > >> become more familiar with HBase. I do also not have a HBase setup
> >>> > >> available
> >>> > >> here.
> >>> > >> Maybe somebody else of the community who was involved with a
> >>> previous
> >>> > >> version of the HBase connector could comment on your question.
> >>> > >>
> >>> > >> Best, Fabian
> >>> > >>
> >>> > >> 2014-11-02 9:57 GMT+01:00 Flavio Pompermaier <
> pompermaier@okkam.it
> >>> >:
> >>> > >>
> >>> > >> > As suggestes by Fabian I moved the discussion on this mailing
> >>> list.
> >>> > >> >
> >>> > >> > I think that what is still to be discussed is how  to retrigger
> >>> the
> >>> > >> build
> >>> > >> > on Travis (I don't have an account) and if the PR can be
> >>> integrated.
> >>> > >> >
> >>> > >> > Maybe what I can do is to move the HBase example in the test
> >>> package
> >>> > >> (right
> >>> > >> > now I left it in the main folder) so it will force Travis to
> >>> rebuild.
> >>> > >> > I'll do it within a couple of hours.
> >>> > >> >
> >>> > >> > Another thing I forgot to say is that the hbase extension is now
> >>> > >> compatible
> >>> > >> > with both hadoop 1 and 2.
> >>> > >> >
> >>> > >> > Best,
> >>> > >> > Flavio
> >>> > >>
> >>> > >
> >>> >
> >>>
> >>
> >
>

--001a1132e0baa4e8880506f0720c--
#|#<CAELUF_DPYyqB7-aPsDYfz+ohB-zO87SoEDQvRRMoTH+CNToabA@mail.gmail.com>##//##<CAAdrtT29SY0VdtR4RFdpQM8bfhpPFmapJbT21MnOisD33rGihg@mail.gmail.com>#|#2014-10-08-10:43:02#|#Fabian Hueske <fhueske@apache.org>#|#Re: Minor change to the versioning scheme#|#
--20cf303a2fc95e901e0504e6f9fa
Content-Type: text/plain; charset=UTF-8

+1

2014-10-08 12:38 GMT+02:00 Robert Metzger <rmetzger@apache.org>:

> Hi guys,
>
> I noticed that we are doing the versioning of release a bit differently
> than most other projects.
> What we do differently is the numbering of major releases.
> Let me explain ...
> ... our initial release are numbered like this:
> 0.5
> 0.6
> 0.7
>
> Our bugfix releases are:
> 0.5.1
> 0.6.1
> 0.6.2 ..etc.
>
> I suggest to call the initial major releases
> 0.7.0
> 0.8.0 and so on.
>
>
> What is the advantage of this?
> --> The names of our branches.
> I would suggest to have a branch for each major-release-tree that is called
> "release-0.x"
> From this branch, we create the initial release and all subsequent bugfix
> releases.
>
> It will be easier for users to understand how we name our branches if we
> follow this approach, because all 3-digit branches are released versions,
> 2-digit branches are work in progress
>
>
> I hope my little ascii-art-picture arrives properly at your side ;)
>
> --------master------------------------------------------ <--- bugfixes and
> features here.
>              \                        \
>               \                      release-0.7------  <--- 0.7 bugfixes
> go here
>                \                             \
>                 \                           release-0.7.1
>                  --- release-0.6-----------------------  <--0.6 bugfixes go
> here
>                            \                       \
>                            release-0.6.0    release-0.6.1 <--immutable
> release tags.
>
>
> If we all agree on this, I'll document it on the website.
>

--20cf303a2fc95e901e0504e6f9fa--
#|#<CAGr9p8DQAz1s3W7K09L61m5eg0ekBbWemKQoZ35DKjd50vU6eg@mail.gmail.com>##//##<CAAdrtT2EZQTb41AxpVRfO8_N0MGW8Lm6kWzdK-L_=NMGOnKhTQ@mail.gmail.com>#|#2014-09-08-22:09:13#|#Fabian Hueske <fhueske@apache.org>#|#Re: 0.6.1 Bugfix Release#|#
--20cf303a3107547db60502951018
Content-Type: text/plain; charset=UTF-8

Sounds good.
+1

2014-09-08 16:03 GMT+02:00 Stephan Ewen <sewen@apache.org>:

> I would include the following commits:
>
> Fix DeltaIteration Bug with Immutable Types
>
> https://github.com/apache/incubator-flink/commit/3aa5511ddd474659b7592969dc923301e87820b5
>
> Disable merging of Iteration Aux Tasks
>
> https://github.com/apache/incubator-flink/commit/80af60b986f4aaf57ab56b243d3c897dabb0bddd
>
> [FLINK-1084] Fix broken links in "How to add an operator"
>
> https://github.com/apache/incubator-flink/commit/c9dd60385e05f9723d01554b8674e0c620de49c2
>
> [FLINK-1078] PrintingOutputFormat uses same partition indexing as
> FileOutputFormat.
>
> https://github.com/apache/incubator-flink/commit/0a6e53ddc05ba901e4b04c9208c2893e686bdb42
>
> [FLINK-1079] Fix inconsistent parameter naming
>
> https://github.com/apache/incubator-flink/commit/fbed013db60d7c45dcd11b6303ffa16220557e13
>
> [FLINK-1075] Removed the AsynchronousPartialSorter.
>
> https://github.com/apache/incubator-flink/commit/cbbcf7820885a8a9734ffeba637b0182a6637939
>
> [FLINK-1074] Fix for NULL input tuples in ProjectJoin
>
> https://github.com/apache/incubator-flink/commit/00840599a7a498cbd19d524ab5ad698365cbab4f
>
> Split between source distribution license/notice and binary distribution
> license/notice.
>
> https://github.com/apache/incubator-flink/commit/8bc7894fe5583176a2b35163980fb45657c80bed
>
> Update LICENSE and add license headers to CSS and JS in flink clients
>
> https://github.com/apache/incubator-flink/commit/61bb30b804b274f4d051d47c46553036573ac715
>
>
>
> Stephan
>
>
> On Mon, Sep 8, 2014 at 3:29 PM, Ufuk Celebi <uce@apache.org> wrote:
> >
> > Hey all,
> >
> > the plan for the 0.7 release is to do a feature freeze on September 24
> and
> > then start the vote a few days afterwards.
> >
> > I think we should have a 0.6.1 bugfix release in the meantime. The 0.6
> > release contains a buggy commit, which I've reverted for the current
> master
> > and the release-0.6.1 branch. I think it is important that users don't
> walk
> > into the problem (see FLINK-1063).
> >
> > What are your thoughts on this? Are there further bugfixes we could
> include?
> >
> > Best,
> >
> > Ufuk
>

--20cf303a3107547db60502951018--
#|#<CANC1h_uhk1_KgdR+r_HLhp6WZw6VeRmesgk3n_CzfVwMYHzvhA@mail.gmail.com>##//##<CAAdrtT2GmjNsqqjkp4LAbxiELc2FJEstRgbKNqkFUPexaYTTtA@mail.gmail.com>#|#2014-11-03-10:51:31#|#Fabian Hueske <fhueske@apache.org>#|#Re: HBase 0.98 addon for Flink 0.8#|#
--089e01177bbbac13e70506f21fa5
Content-Type: text/plain; charset=UTF-8

Hi Flavio

let me try to answer your last question on the user's list (to the best of
my HBase knowledge).
"I just wanted to known if and how regiom splitting is handled. Can you
explain me in detail how Flink and HBase works?what is not fully clear to
me is when computation is done by region servers and when data start flow
to a Flink worker (that in ky test job is only my pc) and how ro undertsand
better the important logged info to understand if my job is performing well"

HBase partitions its tables into so called "regions" of keys and stores the
regions distributed in the cluster using HDFS. I think an HBase region can
be thought of as a HDFS block. To make reading an HBase table efficient,
region reads should be locally done, i.e., an InputFormat should primarily
read region that are stored on the same machine as the IF is running on.
Flink's InputSplits partition the HBase input by regions and add
information about the storage location of the region. During execution,
input splits are assigned to InputFormats that can do local reads.

Best, Fabian

2014-11-03 11:13 GMT+01:00 Stephan Ewen <sewen@apache.org>:

> Hi!
>
> The way of passing parameters through the configuration is very old (the
> original HBase format dated back to that time). I would simply make the
> HBase format take those parameters through the constructor.
>
> Greetings,
> Stephan
>
>
> On Mon, Nov 3, 2014 at 10:59 AM, Flavio Pompermaier <pompermaier@okkam.it>
> wrote:
>
> > The problem is that I also removed the GenericTableOutputFormat because
> > there is an incompatibility between hadoop1 and hadoop2 for class
> > TaskAttemptContext and TaskAttemptContextImpl..
> > then it would be nice if the user doesn't have to worry about passing
> > pact.hbase.jtkey and pact.job.id parameters..
> > I think it is probably a good idea to remove hadoop1 compatibility and
> keep
> > enable HBase addon only for hadoop2 (as before) and decide how to mange
> > those 2 parameters..
> >
> > On Mon, Nov 3, 2014 at 10:19 AM, Stephan Ewen <sewen@apache.org> wrote:
> >
> > > It is fine to remove it, in my opinion.
> > >
> > > On Mon, Nov 3, 2014 at 10:11 AM, Flavio Pompermaier <
> > pompermaier@okkam.it>
> > > wrote:
> > >
> > > > That is one class I removed because it was using the deprecated API
> > > > GenericDataSink..I can restore them but the it will be a good idea to
> > > > remove those warning (also because from what I understood the Record
> > APIs
> > > > are going to be removed).
> > > >
> > > > On Mon, Nov 3, 2014 at 9:51 AM, Fabian Hueske <fhueske@apache.org>
> > > wrote:
> > > >
> > > > > I'm not familiar with the HBase connector code, but are you maybe
> > > looking
> > > > > for the GenericTableOutputFormat?
> > > > >
> > > > > 2014-11-03 9:44 GMT+01:00 Flavio Pompermaier <pompermaier@okkam.it
> >:
> > > > >
> > > > > > | was trying to modify the example setting hbaseDs.output(new
> > > > > > HBaseOutputFormat()); but I can't see any HBaseOutputFormat
> > > > class..maybe
> > > > > we
> > > > > > shall use another class?
> > > > > >
> > > > > > On Mon, Nov 3, 2014 at 9:39 AM, Flavio Pompermaier <
> > > > pompermaier@okkam.it
> > > > > >
> > > > > > wrote:
> > > > > >
> > > > > > > Maybe that's something I could add to the HBase example and
> that
> > > > could
> > > > > be
> > > > > > > better documented in the Wiki.
> > > > > > >
> > > > > > > Since we're talking about the wiki..I was looking at the Java
> > API (
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> http://flink.incubator.apache.org/docs/0.6-incubating/java_api_guide.html)
> > > > > > > and the link to the KMeans example is not working (where it
> says
> > > For
> > > > a
> > > > > > > complete example program, have a look at KMeans Algorithm).
> > > > > > >
> > > > > > > Best,
> > > > > > > Flavio
> > > > > > >
> > > > > > >
> > > > > > > On Mon, Nov 3, 2014 at 9:12 AM, Flavio Pompermaier <
> > > > > pompermaier@okkam.it
> > > > > > >
> > > > > > > wrote:
> > > > > > >
> > > > > > >> Ah ok, perfect! That was the reason why I removed it :)
> > > > > > >>
> > > > > > >> On Mon, Nov 3, 2014 at 9:10 AM, Stephan Ewen <
> sewen@apache.org>
> > > > > wrote:
> > > > > > >>
> > > > > > >>> You do not really need a HBase data sink. You can call
> > > > > > >>> "DataSet.output(new
> > > > > > >>> HBaseOutputFormat())"
> > > > > > >>>
> > > > > > >>> Stephan
> > > > > > >>> Am 02.11.2014 23:05 schrieb "Flavio Pompermaier" <
> > > > > pompermaier@okkam.it
> > > > > > >:
> > > > > > >>>
> > > > > > >>> > Just one last thing..I removed the HbaseDataSink because I
> > > think
> > > > it
> > > > > > was
> > > > > > >>> > using the old APIs..can someone help me in updating that
> > class?
> > > > > > >>> >
> > > > > > >>> > On Sun, Nov 2, 2014 at 10:55 AM, Flavio Pompermaier <
> > > > > > >>> pompermaier@okkam.it>
> > > > > > >>> > wrote:
> > > > > > >>> >
> > > > > > >>> > > Indeed this time the build has been successful :)
> > > > > > >>> > >
> > > > > > >>> > > On Sun, Nov 2, 2014 at 10:29 AM, Fabian Hueske <
> > > > > fhueske@apache.org
> > > > > > >
> > > > > > >>> > wrote:
> > > > > > >>> > >
> > > > > > >>> > >> You can also setup Travis to build your own Github
> > > > repositories
> > > > > by
> > > > > > >>> > linking
> > > > > > >>> > >> it to your Github account. That way Travis can build all
> > > your
> > > > > > >>> branches
> > > > > > >>> > >> (and
> > > > > > >>> > >> you can also trigger rebuilds if something fails).
> > > > > > >>> > >> Not sure if we can manually trigger retrigger builds on
> > the
> > > > > Apache
> > > > > > >>> > >> repository.
> > > > > > >>> > >>
> > > > > > >>> > >> Support for Hadoop 1 and 2 is indeed a very good
> addition
> > > :-)
> > > > > > >>> > >>
> > > > > > >>> > >> For the discusion about the PR itself, I would need a
> bit
> > > more
> > > > > > time
> > > > > > >>> to
> > > > > > >>> > >> become more familiar with HBase. I do also not have a
> > HBase
> > > > > setup
> > > > > > >>> > >> available
> > > > > > >>> > >> here.
> > > > > > >>> > >> Maybe somebody else of the community who was involved
> > with a
> > > > > > >>> previous
> > > > > > >>> > >> version of the HBase connector could comment on your
> > > question.
> > > > > > >>> > >>
> > > > > > >>> > >> Best, Fabian
> > > > > > >>> > >>
> > > > > > >>> > >> 2014-11-02 9:57 GMT+01:00 Flavio Pompermaier <
> > > > > > pompermaier@okkam.it
> > > > > > >>> >:
> > > > > > >>> > >>
> > > > > > >>> > >> > As suggestes by Fabian I moved the discussion on this
> > > > mailing
> > > > > > >>> list.
> > > > > > >>> > >> >
> > > > > > >>> > >> > I think that what is still to be discussed is how  to
> > > > > retrigger
> > > > > > >>> the
> > > > > > >>> > >> build
> > > > > > >>> > >> > on Travis (I don't have an account) and if the PR can
> be
> > > > > > >>> integrated.
> > > > > > >>> > >> >
> > > > > > >>> > >> > Maybe what I can do is to move the HBase example in
> the
> > > test
> > > > > > >>> package
> > > > > > >>> > >> (right
> > > > > > >>> > >> > now I left it in the main folder) so it will force
> > Travis
> > > to
> > > > > > >>> rebuild.
> > > > > > >>> > >> > I'll do it within a couple of hours.
> > > > > > >>> > >> >
> > > > > > >>> > >> > Another thing I forgot to say is that the hbase
> > extension
> > > is
> > > > > now
> > > > > > >>> > >> compatible
> > > > > > >>> > >> > with both hadoop 1 and 2.
> > > > > > >>> > >> >
> > > > > > >>> > >> > Best,
> > > > > > >>> > >> > Flavio
> > > > > > >>> > >>
> > > > > > >>> > >
> > > > > > >>> >
> > > > > > >>>
> > > > > > >>
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

--089e01177bbbac13e70506f21fa5--
#|#<CANC1h_sBi719_nUYxMrRW+ZDoOLry+Uke2yiiSHMREb8PrjNjw@mail.gmail.com>##//##<CAAdrtT2JYV-EhNB1ikn5XXrbmHR28ycYE2=7QePF2rasKjLn9Q@mail.gmail.com>#|#2014-06-25-18:10:18#|#Fabian Hueske <fhueske@apache.org>#|#Re: ConstantFields in Java API#|#
--20cf303dd74acded8604fcacfba1
Content-Type: text/plain; charset=UTF-8

Hi Janani,

You can also use the projection join as:

ds1.join(ds2).where(0).equalTo(0).projectFirst(0).projectSecond(1,2).types(Long.class,
Long.class, Long.class)

That should also automatically set the correct annotations.

Best, Fabian
On Jun 25, 2014 7:41 PM, "Janani Chakkaradhari" <janani.cse05@gmail.com>
wrote:

> Hi,
>
> Can someone check the following UDF for constantFileds setup?
>
> In simple, I wanted to do :
> First Input of Join :    Tuple3<Long, FMCounter, Double> first,
> Second Input of Join: Tuple2<Long, Long> second
> Output:                     Tuple3<second.f0, first.f1, first.f1>
>
>
> UDF:
>     @ConstantFieldsFirst("2 -> 2")
>     @ConstantFieldsSecond("0 -> 0")
>     public static final class SendingMessageToNeighbors
>         extends
>         JoinFunction<Tuple3<Long, FMCounter, Double>,
>         Tuple2<Long, Long>, Tuple3<Long, FMCounter, Double>> {
>
>         @Override
>         public Tuple3<Long, FMCounter, Double> join(
>             Tuple3<Long, FMCounter, Double> vertex_workset,
>             Tuple2<Long, Long> neighbors) throws Exception {
>             return new Tuple3<Long,
>                     FMCounter,
> Double>(neighbors.f0,vertex_workset.f1,vertex_workset.f2);
>         }
>
>     }
>
> Here, I only mapped for two fields (0 and 2) in output tuple. Is it fine?
>
> Regards,
> Janani
>

--20cf303dd74acded8604fcacfba1--
#|#<CACuZrWNTMka8vPzPzeAkyAJ4Gwmem4DJS9RTM+1CEQCYJR_3qA@mail.gmail.com>##//##<CAAdrtT2Kh0QgmFcULK+DOh7m4QB3KcNbHfDVRmV1xGcCpfDCUA@mail.gmail.com>#|#2014-09-22-14:54:54#|#Fabian Hueske <fhueske@apache.org>#|#Re: grouping and sort grouping with KeySelector#|#
--14dae9d2f888a7e8560503a8a00f
Content-Type: text/plain; charset=UTF-8

Hi Martin,

group sorting is currently only possible with field-index keys and not with
KeySelectors.
A workaround would be to use a map to convert your DataSet into
Tuple3<YouPojoType, GroupType, SortType> and do a groupBy(1).sortGroup(2,
Order:ASCENDING).
This is actually also what the KeySelector would do under the hood.

Order.ANY gives the optimizer a bit more freedom to decide whether to sort
ascending or descending. However, I guess this is only relevant in very few
cases.

Best,
Fabian

2014-09-22 16:31 GMT+02:00 Martin Neumann <mneumann@spotify.com>:

> Hej,
>
> The data set I'm working on is quite large so I created a pojo class for it
> to make it less messy. I want do do a simple map, group reduce job. But the
> groups needs to be sorted on a secondary key.
>
> From what I understand I would get that by doing:
> dataset.groupBy( key1 ).sortGroup( key2 , Order.ASCENDING )
>
> Is that correct?
> And if so how do I do the same call using a KeySelector. While groupBy can
> take a KeySelector sortGroup cant.
>
> Also can someone explain to me what Order.ANY is about?
>
>
> cheers Martin
>

--14dae9d2f888a7e8560503a8a00f--
#|#<CAHuUK3XgrCdUpJTEz19qCufGermOHicMoFemdbptZLmt959gAA@mail.gmail.com>##//##<CAAdrtT2h9=f=ZSKQnr1yjn-VFbPssDvus7=0NBKj7XjnThbpcA@mail.gmail.com>#|#2014-11-15-22:56:58#|#Fabian Hueske <fhueske@apache.org>#|#Re: Problem with starting Flink "start-local.sh" on Server#|#
--001a113949a264d6ed0507eda3b6
Content-Type: text/plain; charset=UTF-8

Hi,

did you check the files files in ./log?
The .log and .out files contain the Flink's log output and the redirected
standard out and should help to figure out what is going wrong at system
start.

Cheers, Fabian


2014-11-15 21:30 GMT+01:00 Saied Tehrani <saied@fabgate.co>:

> Hi there,
>
> I have successfully installed Apache Flink on my local machine. I am trying
> to install Apache Flink on a virtual server on "Google Cloud Platform".
>
> I have followed this tutorial to run Apache Flink on a server, but it
> doesn't seem to work:
>
> http://flink.incubator.apache.org/docs/0.7-incubating/setup_quickstart.html
>
>
> I run the command "flink-0.8-incubating-SNAPSHOT/bin/*start-local.sh*" and
> get the following message:
>
> > /Starting job manager/
>
>
> If I run the command "*nc -vz localhost 8081*" I get:
>
> > /localhost [127.0.0.1] 8081 (tproxy) : Connection refused/
>
>
> but it should actually deliver something like this, which I get on my local
> machine:
>
> > /Connection to localhost port 8081 [tcp/sunproxyadmin] succeeded!/
>
>
> If I run "flink-0.8-incubating-SNAPSHOT/bin/*stop-local.sh*" I get:
>
> > /No job manager to stop/
>
>
> So it seems it was not running in the first place. Any ideas why that might
> be the case?
>
> Cheers
>
>
>
> --
> View this message in context:
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/Problem-with-starting-Flink-start-local-sh-on-Server-tp2518.html
> Sent from the Apache Flink (Incubator) Mailing List archive. mailing list
> archive at Nabble.com.
>

--001a113949a264d6ed0507eda3b6--
#|#<1416083432521-2518.post@n3.nabble.com>##//##<CAAdrtT2i6ne=9F4F95g9+uL6yRAT7rPsJO5_cKUCkSkzwrFoHQ@mail.gmail.com>#|#2014-11-14-20:26:17#|#Fabian Hueske <fhueske@apache.org>#|#Re: HBase 0.98 addon for Flink 0.8#|#
--001a1139c256cedd850507d76a4d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

What exactly is required to configure the TableInputFormat?
Would it be easier and more flexible to just set the hostname of the HBase
master, the table name, etc, directly as strings in the InputFormat?

2014-11-14 15:34 GMT+01:00 Flavio Pompermaier <pompermaier@okkam.it>:

> Both from shell with run command and from web client
> On Nov 14, 2014 2:32 PM, "Fabian Hueske" <fhueske@apache.org> wrote:
> >
> > In this case, the initialization happens when the InputFormat is
> > instantiated at the submission client and the Table info is serialized as
> > part of the InputFormat and shipped out to all TaskManagers for
> execution.
> > However, if the initialization is done within configure it happens on
> each
> > TaskManager when initializing the InputFormat.
> > These are two separate JVMs in a distributed setting with different
> > classpaths.
> >
> > How do you submit your job for execution?
> >
> > 2014-11-14 13:58 GMT+01:00 Flavio Pompermaier <pompermaier@okkam.it>:
> >
> > > The strange thing us that everything works if I create HTable outside
> > > configure()..
> > > On Nov 14, 2014 10:32 AM, "Stephan Ewen" <sewen@apache.org> wrote:
> > >
> > > > I think that this is a case where the wrong classloader is used:
> > > >
> > > > If the HBase classes are part of the flink lib directory, they are
> loaded
> > > > with the system class loader. When they look for anything in the
> > > classpath,
> > > > they will do so with the system classloader.
> > > >
> > > > You configuration is in the user code jar that you submit, so it is
> only
> > > > available through the user-code classloader.
> > > >
> > > > Any way you can load the configuration yourself and give that
> > > configuration
> > > > to HBase?
> > > >
> > > > Stephan
> > > > Am 13.11.2014 22:06 schrieb "Flavio Pompermaier" <
> pompermaier@okkam.it
> >:
> > > >
> > > > > The only config files available are within the submitted jar.
> Things
> > > > works
> > > > > in eclipse using local environment while fails deploying to the
> cluster
> > > > > On Nov 13, 2014 10:01 PM, <fhueske@gmail.com> wrote:
> > > > >
> > > > > > Does the HBase jar in the lib folder contain a config that could
> be
> > > > used
> > > > > > instead of the config in the job jar file? Or is simply no config
> at
> > > > all
> > > > > > available when the configure method is called?
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > > --
> > > > > > Fabian Hueske
> > > > > > Phone:      +49 170 5549438
> > > > > > Email:      fhueske@gmail.com
> > > > > > Web:         http://www.user.tu-berlin.de/fabian.hueske
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > > From: Flavio Pompermaier
> > > > > > Sent: =E2=80=8EThursday=E2=80=8E, =E2=80=8E13=E2=80=8E. =E2=80=8ENovember=E2=80=8E, =E2=80=8E2014 =E2=80=8E21=E2=80=8E:=E2=80=8E43
> > > > > > To: dev@flink.incubator.apache.org
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > > > The hbase jar is in the lib directory on each node while the
> config
> > > > files
> > > > > > are within the jar file I submit from the web client.
> > > > > > On Nov 13, 2014 9:37 PM, <fhueske@gmail.com> wrote:
> > > > > >
> > > > > > > Have you added the hbase.jar file with your HBase config to the
> > > ./lib
> > > > > > > folders of your Flink setup (JobManager, TaskManager) or is it
> > > > bundled
> > > > > > with
> > > > > > > your job.jar file?
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > --
> > > > > > > Fabian Hueske
> > > > > > > Phone:      +49 170 5549438
> > > > > > > Email:      fhueske@gmail.com
> > > > > > > Web:         http://www.user.tu-berlin.de/fabian.hueske
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > From: Flavio Pompermaier
> > > > > > > Sent: =E2=80=8EThursday=E2=80=8E, =E2=80=8E13=E2=80=8E. =E2=80=8ENovember=E2=80=8E, =E2=80=8E2014 =E2=80=8E18=E2=80=8E:=E2=80=8E36
> > > > > > > To: dev@flink.incubator.apache.org
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > Any help with this? :(
> > > > > > >
> > > > > > > On Thu, Nov 13, 2014 at 2:06 PM, Flavio Pompermaier <
> > > > > > pompermaier@okkam.it>
> > > > > > > wrote:
> > > > > > >
> > > > > > > > We definitely discovered that instantiating HTable and Scan
> in
> > > > > > > configure()
> > > > > > > > method of TableInputFormat causes problem in distributed
> > > > environment!
> > > > > > > > If you look at my implementation at
> > > > > > > >
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
>
> https://github.com/fpompermaier/incubator-flink/blob/master/flink-addons/flink-hbase/src/main/java/org/apache/flink/addons/hbase/TableInputFormat.java
> > > > > > > > you can see that Scan and HTable were made transient and
> > > recreated
> > > > > > within
> > > > > > > > configure but this causes HBaseConfiguration.create() to fail
> > > > > searching
> > > > > > > for
> > > > > > > > classpath files...could you help us understanding why?
> > > > > > > >
> > > > > > > > On Wed, Nov 12, 2014 at 8:10 PM, Flavio Pompermaier <
> > > > > > > pompermaier@okkam.it>
> > > > > > > > wrote:
> > > > > > > >
> > > > > > > >> Usually, when I run a mapreduce job both on Spark and Hadoop
> I
> > > > just
> > > > > > put
> > > > > > > >> *-site.xml files into the war I submit to the cluster and
> that's
> > > > > it. I
> > > > > > > >> think the problem appeared when I made the HTable a private
> > > > > transient
> > > > > > > field
> > > > > > > >> and the table istantiation was moved in the configure
> method.
> > > > > > > >> Could it be a valid reason? we still have to make a deeper
> debug
> > > > but
> > > > > > I'm
> > > > > > > >> trying ro figure out where to investigate..
> > > > > > > >> On Nov 12, 2014 8:03 PM, "Robert Metzger" <
> rmetzger@apache.org>
> > > > > > wrote:
> > > > > > > >>
> > > > > > > >>> Hi,
> > > > > > > >>> Maybe its an issue with the classpath? As far as I know is
> > > Hadoop
> > > > > > > reading
> > > > > > > >>> the configuration files from the classpath. Maybe is the
> > > > > > hbase-site.xml
> > > > > > > >>> file not accessible through the classpath when running on
> the
> > > > > > cluster?
> > > > > > > >>>
> > > > > > > >>> On Wed, Nov 12, 2014 at 7:40 PM, Flavio Pompermaier <
> > > > > > > >>> pompermaier@okkam.it>
> > > > > > > >>> wrote:
> > > > > > > >>>
> > > > > > > >>> > Today we tried tp execute a job on the cluster instead of
> on
> > > > > local
> > > > > > > >>> executor
> > > > > > > >>> > and we faced the problem that the hbase-site.xml was
> > > basically
> > > > > > > >>> ignored. Is
> > > > > > > >>> > there a reason why the TableInputFormat is working
> correctly
> > > on
> > > > > > local
> > > > > > > >>> > environment while it doesn't on a cluster?
> > > > > > > >>> > On Nov 10, 2014 10:56 AM, "Fabian Hueske" <
> > > fhueske@apache.org>
> > > > > > > wrote:
> > > > > > > >>> >
> > > > > > > >>> > > I don't think we need to bundle the HBase input and
> output
> > > > > format
> > > > > > > in
> > > > > > > >>> a
> > > > > > > >>> > > single PR.
> > > > > > > >>> > > So, I think we can proceed with the IF only and target
> the
> > > OF
> > > > > > > later.
> > > > > > > >>> > > However, the fix for Kryo should be in the master
> before
> > > > > merging
> > > > > > > the
> > > > > > > >>> PR.
> > > > > > > >>> > > Till is currently working on that and said he expects
> this
> > > to
> > > > > be
> > > > > > > >>> done by
> > > > > > > >>> > > end of the week.
> > > > > > > >>> > >
> > > > > > > >>> > > Cheers, Fabian
> > > > > > > >>> > >
> > > > > > > >>> > >
> > > > > > > >>> > > 2014-11-07 12:49 GMT+01:00 Flavio Pompermaier <
> > > > > > > pompermaier@okkam.it
> > > > > > > >>> >:
> > > > > > > >>> > >
> > > > > > > >>> > > > I fixed also the profile for Cloudera CDH5.1.3. You
> can
> > > > build
> > > > > > it
> > > > > > > >>> with
> > > > > > > >>> > the
> > > > > > > >>> > > > command:
> > > > > > > >>> > > >       mvn clean install -Dmaven.test.skip=3Dtrue
> > > > > > -Dhadoop.profile=3D2
> > > > > > > >>> > > >  -Pvendor-repos,cdh5.1.3
> > > > > > > >>> > > >
> > > > > > > >>> > > > However, it would be good to generate the specific
> jar
> > > when
> > > > > > > >>> > > > releasing..(e.g.
> > > > > > > >>> > > >
> > > flink-addons:flink-hbase:0.8.0-hadoop2-cdh5.1.3-incubating)
> > > > > > > >>> > > >
> > > > > > > >>> > > > Best,
> > > > > > > >>> > > > Flavio
> > > > > > > >>> > > >
> > > > > > > >>> > > > On Fri, Nov 7, 2014 at 12:44 PM, Flavio Pompermaier <
> > > > > > > >>> > > pompermaier@okkam.it>
> > > > > > > >>> > > > wrote:
> > > > > > > >>> > > >
> > > > > > > >>> > > > > I've just updated the code on my fork (synch with
> > > current
> > > > > > > master
> > > > > > > >>> and
> > > > > > > >>> > > > > applied improvements coming from comments on
> related
> > > PR).
> > > > > > > >>> > > > > I still have to understand how to write results
> back to
> > > > an
> > > > > > > HBase
> > > > > > > >>> > > > > Sink/OutputFormat...
> > > > > > > >>> > > > >
> > > > > > > >>> > > > >
> > > > > > > >>> > > > > On Mon, Nov 3, 2014 at 12:05 PM, Flavio Pompermaier
> <
> > > > > > > >>> > > > pompermaier@okkam.it>
> > > > > > > >>> > > > > wrote:
> > > > > > > >>> > > > >
> > > > > > > >>> > > > >> Thanks for the detailed answer. So if I run a job
> from
> > > > my
> > > > > > > >>> machine
> > > > > > > >>> > I'll
> > > > > > > >>> > > > >> have to download all the scanned data in a
> > > table..right?
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >> Always regarding the GenericTableOutputFormat it
> is
> > > not
> > > > > > clear
> > > > > > > >>> to me
> > > > > > > >>> > > how
> > > > > > > >>> > > > >> to proceed..
> > > > > > > >>> > > > >> I saw in the hadoop compatibility addon that it is
> > > > > possible
> > > > > > to
> > > > > > > >>> have
> > > > > > > >>> > > such
> > > > > > > >>> > > > >> compatibility using HBaseUtils class so the open
> > > method
> > > > > > should
> > > > > > > >>> > become
> > > > > > > >>> > > > >> something like:
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >> @Override
> > > > > > > >>> > > > >> public void open(int taskNumber, int numTasks)
> throws
> > > > > > > >>> IOException {
> > > > > > > >>> > > > >> if (Integer.toString(taskNumber + 1).length() > 6)
> {
> > > > > > > >>> > > > >> throw new IOException("Task id too large.");
> > > > > > > >>> > > > >> }
> > > > > > > >>> > > > >> TaskAttemptID taskAttemptID =3D
> > > > > > > >>> > TaskAttemptID.forName("attempt__0000_r_"
> > > > > > > >>> > > > >> + String.format("%" + (6 -
> > > Integer.toString(taskNumber +
> > > > > > > >>> > 1).length())
> > > > > > > >>> > > +
> > > > > > > >>> > > > >> "s"," ").replace(" ", "0")
> > > > > > > >>> > > > >> + Integer.toString(taskNumber + 1)
> > > > > > > >>> > > > >> + "_0");
> > > > > > > >>> > > > >>  this.configuration.set("mapred.task.id",
> > > > > > > >>> > taskAttemptID.toString());
> > > > > > > >>> > > > >> this.configuration.setInt("mapred.task.partition",
> > > > > > taskNumber
> > > > > > > +
> > > > > > > >>> 1);
> > > > > > > >>> > > > >> // for hadoop 2.2
> > > > > > > >>> > > > >> this.configuration.set("mapreduce.task.attempt.id
> ",
> > > > > > > >>> > > > >> taskAttemptID.toString());
> > > > > > > >>> > > > >>
> this.configuration.setInt("mapreduce.task.partition",
> > > > > > > >>> taskNumber +
> > > > > > > >>> > 1);
> > > > > > > >>> > > > >>  try {
> > > > > > > >>> > > > >> this.context =3D
> > > > > > > >>> > > > >>
> > > > > > HadoopUtils.instantiateTaskAttemptContext(this.configuration,
> > > > > > > >>> > > > >> taskAttemptID);
> > > > > > > >>> > > > >> } catch (Exception e) {
> > > > > > > >>> > > > >> throw new RuntimeException(e);
> > > > > > > >>> > > > >> }
> > > > > > > >>> > > > >> final HFileOutputFormat2 outFormat =3D new
> > > > > > HFileOutputFormat2();
> > > > > > > >>> > > > >> try {
> > > > > > > >>> > > > >> this.writer =3D
> outFormat.getRecordWriter(this.context);
> > > > > > > >>> > > > >> } catch (InterruptedException iex) {
> > > > > > > >>> > > > >> throw new IOException("Opening the writer was
> > > > > interrupted.",
> > > > > > > >>> iex);
> > > > > > > >>> > > > >> }
> > > > > > > >>> > > > >> }
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >> But I'm not sure about how to pass the JobConf to
> the
> > > > > class,
> > > > > > > if
> > > > > > > >>> to
> > > > > > > >>> > > merge
> > > > > > > >>> > > > >> config fileas, where HFileOutputFormat2 writes the
> > > data
> > > > > and
> > > > > > > how
> > > > > > > >>> to
> > > > > > > >>> > > > >> implement the public void writeRecord(Record
> record)
> > > > API.
> > > > > > > >>> > > > >> Could I do a little chat off the mailing list with
> the
> > > > > > > >>> implementor
> > > > > > > >>> > of
> > > > > > > >>> > > > >> this extension?
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >> On Mon, Nov 3, 2014 at 11:51 AM, Fabian Hueske <
> > > > > > > >>> fhueske@apache.org>
> > > > > > > >>> > > > >> wrote:
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >>> Hi Flavio
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > > >>> let me try to answer your last question on the
> user's
> > > > > list
> > > > > > > (to
> > > > > > > >>> the
> > > > > > > >>> > > best
> > > > > > > >>> > > > >>> of
> > > > > > > >>> > > > >>> my HBase knowledge).
> > > > > > > >>> > > > >>> "I just wanted to known if and how regiom
> splitting
> > > is
> > > > > > > >>> handled. Can
> > > > > > > >>> > > you
> > > > > > > >>> > > > >>> explain me in detail how Flink and HBase
> works?what
> > > is
> > > > > not
> > > > > > > >>> fully
> > > > > > > >>> > > clear
> > > > > > > >>> > > > to
> > > > > > > >>> > > > >>> me is when computation is done by region servers
> and
> > > > when
> > > > > > > data
> > > > > > > >>> > start
> > > > > > > >>> > > > flow
> > > > > > > >>> > > > >>> to a Flink worker (that in ky test job is only my
> pc)
> > > > and
> > > > > > how
> > > > > > > >>> ro
> > > > > > > >>> > > > >>> undertsand
> > > > > > > >>> > > > >>> better the important logged info to understand if
> my
> > > > job
> > > > > is
> > > > > > > >>> > > performing
> > > > > > > >>> > > > >>> well"
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > > >>> HBase partitions its tables into so called
> "regions"
> > > of
> > > > > > keys
> > > > > > > >>> and
> > > > > > > >>> > > stores
> > > > > > > >>> > > > >>> the
> > > > > > > >>> > > > >>> regions distributed in the cluster using HDFS. I
> > > think
> > > > an
> > > > > > > HBase
> > > > > > > >>> > > region
> > > > > > > >>> > > > >>> can
> > > > > > > >>> > > > >>> be thought of as a HDFS block. To make reading an
> > > HBase
> > > > > > table
> > > > > > > >>> > > > efficient,
> > > > > > > >>> > > > >>> region reads should be locally done, i.e., an
> > > > InputFormat
> > > > > > > >>> should
> > > > > > > >>> > > > >>> primarily
> > > > > > > >>> > > > >>> read region that are stored on the same machine
> as
> > > the
> > > > IF
> > > > > > is
> > > > > > > >>> > running
> > > > > > > >>> > > > on.
> > > > > > > >>> > > > >>> Flink's InputSplits partition the HBase input by
> > > > regions
> > > > > > and
> > > > > > > >>> add
> > > > > > > >>> > > > >>> information about the storage location of the
> region.
> > > > > > During
> > > > > > > >>> > > execution,
> > > > > > > >>> > > > >>> input splits are assigned to InputFormats that
> can do
> > > > > local
> > > > > > > >>> reads.
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > > >>> Best, Fabian
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > > >>> 2014-11-03 11:13 GMT+01:00 Stephan Ewen <
> > > > > sewen@apache.org
> > > > > > >:
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > > >>> > Hi!
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>> > The way of passing parameters through the
> > > > configuration
> > > > > > is
> > > > > > > >>> very
> > > > > > > >>> > old
> > > > > > > >>> > > > >>> (the
> > > > > > > >>> > > > >>> > original HBase format dated back to that time).
> I
> > > > would
> > > > > > > >>> simply
> > > > > > > >>> > make
> > > > > > > >>> > > > the
> > > > > > > >>> > > > >>> > HBase format take those parameters through the
> > > > > > constructor.
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>> > Greetings,
> > > > > > > >>> > > > >>> > Stephan
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>> > On Mon, Nov 3, 2014 at 10:59 AM, Flavio
> > > Pompermaier <
> > > > > > > >>> > > > >>> pompermaier@okkam.it>
> > > > > > > >>> > > > >>> > wrote:
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>> > > The problem is that I also removed the
> > > > > > > >>> GenericTableOutputFormat
> > > > > > > >>> > > > >>> because
> > > > > > > >>> > > > >>> > > there is an incompatibility between hadoop1
> and
> > > > > hadoop2
> > > > > > > for
> > > > > > > >>> > class
> > > > > > > >>> > > > >>> > > TaskAttemptContext and
> TaskAttemptContextImpl..
> > > > > > > >>> > > > >>> > > then it would be nice if the user doesn't
> have to
> > > > > worry
> > > > > > > >>> about
> > > > > > > >>> > > > passing
> > > > > > > >>> > > > >>> > > pact.hbase.jtkey and pact.job.id
> parameters..
> > > > > > > >>> > > > >>> > > I think it is probably a good idea to remove
> > > > hadoop1
> > > > > > > >>> > > compatibility
> > > > > > > >>> > > > >>> and
> > > > > > > >>> > > > >>> > keep
> > > > > > > >>> > > > >>> > > enable HBase addon only for hadoop2 (as
> before)
> > > and
> > > > > > > decide
> > > > > > > >>> how
> > > > > > > >>> > to
> > > > > > > >>> > > > >>> mange
> > > > > > > >>> > > > >>> > > those 2 parameters..
> > > > > > > >>> > > > >>> > >
> > > > > > > >>> > > > >>> > > On Mon, Nov 3, 2014 at 10:19 AM, Stephan Ewen
> <
> > > > > > > >>> > sewen@apache.org>
> > > > > > > >>> > > > >>> wrote:
> > > > > > > >>> > > > >>> > >
> > > > > > > >>> > > > >>> > > > It is fine to remove it, in my opinion.
> > > > > > > >>> > > > >>> > > >
> > > > > > > >>> > > > >>> > > > On Mon, Nov 3, 2014 at 10:11 AM, Flavio
> > > > > Pompermaier <
> > > > > > > >>> > > > >>> > > pompermaier@okkam.it>
> > > > > > > >>> > > > >>> > > > wrote:
> > > > > > > >>> > > > >>> > > >
> > > > > > > >>> > > > >>> > > > > That is one class I removed because it
> was
> > > > using
> > > > > > the
> > > > > > > >>> > > deprecated
> > > > > > > >>> > > > >>> API
> > > > > > > >>> > > > >>> > > > > GenericDataSink..I can restore them but
> the
> > > it
> > > > > will
> > > > > > > be
> > > > > > > >>> a
> > > > > > > >>> > good
> > > > > > > >>> > > > >>> idea to
> > > > > > > >>> > > > >>> > > > > remove those warning (also because from
> what
> > > I
> > > > > > > >>> understood
> > > > > > > >>> > the
> > > > > > > >>> > > > >>> Record
> > > > > > > >>> > > > >>> > > APIs
> > > > > > > >>> > > > >>> > > > > are going to be removed).
> > > > > > > >>> > > > >>> > > > >
> > > > > > > >>> > > > >>> > > > > On Mon, Nov 3, 2014 at 9:51 AM, Fabian
> > > Hueske <
> > > > > > > >>> > > > >>> fhueske@apache.org>
> > > > > > > >>> > > > >>> > > > wrote:
> > > > > > > >>> > > > >>> > > > >
> > > > > > > >>> > > > >>> > > > > > I'm not familiar with the HBase
> connector
> > > > code,
> > > > > > but
> > > > > > > >>> are
> > > > > > > >>> > you
> > > > > > > >>> > > > >>> maybe
> > > > > > > >>> > > > >>> > > > looking
> > > > > > > >>> > > > >>> > > > > > for the GenericTableOutputFormat?
> > > > > > > >>> > > > >>> > > > > >
> > > > > > > >>> > > > >>> > > > > > 2014-11-03 9:44 GMT+01:00 Flavio
> > > Pompermaier
> > > > <
> > > > > > > >>> > > > >>> pompermaier@okkam.it
> > > > > > > >>> > > > >>> > >:
> > > > > > > >>> > > > >>> > > > > >
> > > > > > > >>> > > > >>> > > > > > > | was trying to modify the example
> > > setting
> > > > > > > >>> > > > hbaseDs.output(new
> > > > > > > >>> > > > >>> > > > > > > HBaseOutputFormat()); but I can't see
> any
> > > > > > > >>> > > HBaseOutputFormat
> > > > > > > >>> > > > >>> > > > > class..maybe
> > > > > > > >>> > > > >>> > > > > > we
> > > > > > > >>> > > > >>> > > > > > > shall use another class?
> > > > > > > >>> > > > >>> > > > > > >
> > > > > > > >>> > > > >>> > > > > > > On Mon, Nov 3, 2014 at 9:39 AM,
> Flavio
> > > > > > > Pompermaier
> > > > > > > >>> <
> > > > > > > >>> > > > >>> > > > > pompermaier@okkam.it
> > > > > > > >>> > > > >>> > > > > > >
> > > > > > > >>> > > > >>> > > > > > > wrote:
> > > > > > > >>> > > > >>> > > > > > >
> > > > > > > >>> > > > >>> > > > > > > > Maybe that's something I could add
> to
> > > the
> > > > > > HBase
> > > > > > > >>> > example
> > > > > > > >>> > > > and
> > > > > > > >>> > > > >>> > that
> > > > > > > >>> > > > >>> > > > > could
> > > > > > > >>> > > > >>> > > > > > be
> > > > > > > >>> > > > >>> > > > > > > > better documented in the Wiki.
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > > Since we're talking about the
> wiki..I
> > > was
> > > > > > > >>> looking at
> > > > > > > >>> > > the
> > > > > > > >>> > > > >>> Java
> > > > > > > >>> > > > >>> > > API (
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > >
> > > > > > > >>> > > > >>> > > > > >
> > > > > > > >>> > > > >>> > > > >
> > > > > > > >>> > > > >>> > > >
> > > > > > > >>> > > > >>> > >
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > >
> > > > > > > >>> > >
> > > > > > > >>> >
> > > > > > > >>>
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> http://flink.incubator.apache.org/docs/0.6-incubating/java_api_guide.html
> > > > > > > >>> > > > >>> )
> > > > > > > >>> > > > >>> > > > > > > > and the link to the KMeans example
> is
> > > not
> > > > > > > working
> > > > > > > >>> > > (where
> > > > > > > >>> > > > it
> > > > > > > >>> > > > >>> > says
> > > > > > > >>> > > > >>> > > > For
> > > > > > > >>> > > > >>> > > > > a
> > > > > > > >>> > > > >>> > > > > > > > complete example program, have a
> look
> > > at
> > > > > > KMeans
> > > > > > > >>> > > > Algorithm).
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > > Best,
> > > > > > > >>> > > > >>> > > > > > > > Flavio
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > > On Mon, Nov 3, 2014 at 9:12 AM,
> Flavio
> > > > > > > >>> Pompermaier <
> > > > > > > >>> > > > >>> > > > > > pompermaier@okkam.it
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > > wrote:
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > >> Ah ok, perfect! That was the
> reason
> > > why
> > > > I
> > > > > > > >>> removed it
> > > > > > > >>> > > :)
> > > > > > > >>> > > > >>> > > > > > > >>
> > > > > > > >>> > > > >>> > > > > > > >> On Mon, Nov 3, 2014 at 9:10 AM,
> > > Stephan
> > > > > > Ewen <
> > > > > > > >>> > > > >>> > sewen@apache.org>
> > > > > > > >>> > > > >>> > > > > > wrote:
> > > > > > > >>> > > > >>> > > > > > > >>
> > > > > > > >>> > > > >>> > > > > > > >>> You do not really need a HBase
> data
> > > > sink.
> > > > > > You
> > > > > > > >>> can
> > > > > > > >>> > > call
> > > > > > > >>> > > > >>> > > > > > > >>> "DataSet.output(new
> > > > > > > >>> > > > >>> > > > > > > >>> HBaseOutputFormat())"
> > > > > > > >>> > > > >>> > > > > > > >>>
> > > > > > > >>> > > > >>> > > > > > > >>> Stephan
> > > > > > > >>> > > > >>> > > > > > > >>> Am 02.11.2014 23:05 schrieb
> "Flavio
> > > > > > > >>> Pompermaier" <
> > > > > > > >>> > > > >>> > > > > > pompermaier@okkam.it
> > > > > > > >>> > > > >>> > > > > > > >:
> > > > > > > >>> > > > >>> > > > > > > >>>
> > > > > > > >>> > > > >>> > > > > > > >>> > Just one last thing..I removed
> the
> > > > > > > >>> HbaseDataSink
> > > > > > > >>> > > > >>> because I
> > > > > > > >>> > > > >>> > > > think
> > > > > > > >>> > > > >>> > > > > it
> > > > > > > >>> > > > >>> > > > > > > was
> > > > > > > >>> > > > >>> > > > > > > >>> > using the old APIs..can someone
> > > help
> > > > me
> > > > > > in
> > > > > > > >>> > updating
> > > > > > > >>> > > > >>> that
> > > > > > > >>> > > > >>> > > class?
> > > > > > > >>> > > > >>> > > > > > > >>> >
> > > > > > > >>> > > > >>> > > > > > > >>> > On Sun, Nov 2, 2014 at 10:55
> AM,
> > > > Flavio
> > > > > > > >>> > > Pompermaier <
> > > > > > > >>> > > > >>> > > > > > > >>> pompermaier@okkam.it>
> > > > > > > >>> > > > >>> > > > > > > >>> > wrote:
> > > > > > > >>> > > > >>> > > > > > > >>> >
> > > > > > > >>> > > > >>> > > > > > > >>> > > Indeed this time the build
> has
> > > been
> > > > > > > >>> successful
> > > > > > > >>> > :)
> > > > > > > >>> > > > >>> > > > > > > >>> > >
> > > > > > > >>> > > > >>> > > > > > > >>> > > On Sun, Nov 2, 2014 at 10:29
> AM,
> > > > > Fabian
> > > > > > > >>> Hueske
> > > > > > > >>> > <
> > > > > > > >>> > > > >>> > > > > > fhueske@apache.org
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > > >>> > wrote:
> > > > > > > >>> > > > >>> > > > > > > >>> > >
> > > > > > > >>> > > > >>> > > > > > > >>> > >> You can also setup Travis to
> > > build
> > > > > > your
> > > > > > > >>> own
> > > > > > > >>> > > Github
> > > > > > > >>> > > > >>> > > > > repositories
> > > > > > > >>> > > > >>> > > > > > by
> > > > > > > >>> > > > >>> > > > > > > >>> > linking
> > > > > > > >>> > > > >>> > > > > > > >>> > >> it to your Github account.
> That
> > > > way
> > > > > > > >>> Travis can
> > > > > > > >>> > > > >>> build all
> > > > > > > >>> > > > >>> > > > your
> > > > > > > >>> > > > >>> > > > > > > >>> branches
> > > > > > > >>> > > > >>> > > > > > > >>> > >> (and
> > > > > > > >>> > > > >>> > > > > > > >>> > >> you can also trigger
> rebuilds if
> > > > > > > something
> > > > > > > >>> > > fails).
> > > > > > > >>> > > > >>> > > > > > > >>> > >> Not sure if we can manually
> > > > trigger
> > > > > > > >>> retrigger
> > > > > > > >>> > > > >>> builds on
> > > > > > > >>> > > > >>> > > the
> > > > > > > >>> > > > >>> > > > > > Apache
> > > > > > > >>> > > > >>> > > > > > > >>> > >> repository.
> > > > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > > > >>> > > > >>> > > > > > > >>> > >> Support for Hadoop 1 and 2
> is
> > > > > indeed a
> > > > > > > >>> very
> > > > > > > >>> > good
> > > > > > > >>> > > > >>> > addition
> > > > > > > >>> > > > >>> > > > :-)
> > > > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > > > >>> > > > >>> > > > > > > >>> > >> For the discusion about the
> PR
> > > > > > itself, I
> > > > > > > >>> would
> > > > > > > >>> > > > need
> > > > > > > >>> > > > >>> a
> > > > > > > >>> > > > >>> > bit
> > > > > > > >>> > > > >>> > > > more
> > > > > > > >>> > > > >>> > > > > > > time
> > > > > > > >>> > > > >>> > > > > > > >>> to
> > > > > > > >>> > > > >>> > > > > > > >>> > >> become more familiar with
> > > HBase. I
> > > > > do
> > > > > > > >>> also not
> > > > > > > >>> > > > have
> > > > > > > >>> > > > >>> a
> > > > > > > >>> > > > >>> > > HBase
> > > > > > > >>> > > > >>> > > > > > setup
> > > > > > > >>> > > > >>> > > > > > > >>> > >> available
> > > > > > > >>> > > > >>> > > > > > > >>> > >> here.
> > > > > > > >>> > > > >>> > > > > > > >>> > >> Maybe somebody else of the
> > > > community
> > > > > > who
> > > > > > > >>> was
> > > > > > > >>> > > > >>> involved
> > > > > > > >>> > > > >>> > > with a
> > > > > > > >>> > > > >>> > > > > > > >>> previous
> > > > > > > >>> > > > >>> > > > > > > >>> > >> version of the HBase
> connector
> > > > could
> > > > > > > >>> comment
> > > > > > > >>> > on
> > > > > > > >>> > > > your
> > > > > > > >>> > > > >>> > > > question.
> > > > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > > > >>> > > > >>> > > > > > > >>> > >> Best, Fabian
> > > > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > > > >>> > > > >>> > > > > > > >>> > >> 2014-11-02 9:57 GMT+01:00
> Flavio
> > > > > > > >>> Pompermaier <
> > > > > > > >>> > > > >>> > > > > > > pompermaier@okkam.it
> > > > > > > >>> > > > >>> > > > > > > >>> >:
> > > > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > As suggestes by Fabian I
> moved
> > > > the
> > > > > > > >>> > discussion
> > > > > > > >>> > > on
> > > > > > > >>> > > > >>> this
> > > > > > > >>> > > > >>> > > > > mailing
> > > > > > > >>> > > > >>> > > > > > > >>> list.
> > > > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > I think that what is still
> to
> > > be
> > > > > > > >>> discussed
> > > > > > > >>> > is
> > > > > > > >>> > > > >>> how  to
> > > > > > > >>> > > > >>> > > > > > retrigger
> > > > > > > >>> > > > >>> > > > > > > >>> the
> > > > > > > >>> > > > >>> > > > > > > >>> > >> build
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > on Travis (I don't have an
> > > > > account)
> > > > > > > and
> > > > > > > >>> if
> > > > > > > >>> > the
> > > > > > > >>> > > > PR
> > > > > > > >>> > > > >>> can
> > > > > > > >>> > > > >>> > be
> > > > > > > >>> > > > >>> > > > > > > >>> integrated.
> > > > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > Maybe what I can do is to
> move
> > > > the
> > > > > > > HBase
> > > > > > > >>> > > example
> > > > > > > >>> > > > >>> in
> > > > > > > >>> > > > >>> > the
> > > > > > > >>> > > > >>> > > > test
> > > > > > > >>> > > > >>> > > > > > > >>> package
> > > > > > > >>> > > > >>> > > > > > > >>> > >> (right
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > now I left it in the main
> > > > folder)
> > > > > so
> > > > > > > it
> > > > > > > >>> will
> > > > > > > >>> > > > force
> > > > > > > >>> > > > >>> > > Travis
> > > > > > > >>> > > > >>> > > > to
> > > > > > > >>> > > > >>> > > > > > > >>> rebuild.
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > I'll do it within a couple
> of
> > > > > hours.
> > > > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > Another thing I forgot to
> say
> > > is
> > > > > > that
> > > > > > > >>> the
> > > > > > > >>> > > hbase
> > > > > > > >>> > > > >>> > > extension
> > > > > > > >>> > > > >>> > > > is
> > > > > > > >>> > > > >>> > > > > > now
> > > > > > > >>> > > > >>> > > > > > > >>> > >> compatible
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > with both hadoop 1 and 2.
> > > > > > > >>> > > > >>> > > > > > > >>> > >> >
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > Best,
> > > > > > > >>> > > > >>> > > > > > > >>> > >> > Flavio
> > > > > > > >>> > > > >>> > > > > > > >>> > >>
> > > > > > > >>> > > > >>> > > > > > > >>> > >
> > > > > > > >>> > > > >>> > > > > > > >>> >
> > > > > > > >>> > > > >>> > > > > > > >>>
> > > > > > > >>> > > > >>> > > > > > > >>
> > > > > > > >>> > > > >>> > > > > > > >
> > > > > > > >>> > > > >>> > > > > > >
> > > > > > > >>> > > > >>> > > > > >
> > > > > > > >>> > > > >>> > > > >
> > > > > > > >>> > > > >>> > > >
> > > > > > > >>> > > > >>> > >
> > > > > > > >>> > > > >>> >
> > > > > > > >>> > > > >>>
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >>
> > > > > > > >>> > > > >
> > > > > > > >>> > > >
> > > > > > > >>> > >
> > > > > > > >>> >
> > > > > > > >>>
> > > > > > > >>
> > > > > > > >
> > > > > > > >
> > > > >
> > > >
> > >
>

--001a1139c256cedd850507d76a4d--
#|#<CAELUF_CD4r6s1p8UBMxS=2aiMW7pt0D_Rss6XORPjQM65QXV9A@mail.gmail.com>##//##<CAAdrtT2tLP1YcOE+Axjjnds=8uOX69Q0GG+t8APCq-qeOYPb6w@mail.gmail.com>#|#2015-06-12-12:06:33#|#Fabian Hueske <fhueske@gmail.com>#|#Re: Testing Apache Flink 0.9.0-rc1#|#
--001a11c3cc623ddd12051850f01e
Content-Type: text/plain; charset=UTF-8

+1 for b)

I'm organizing + merging the commits that need to go the new candidate
right now. Will let you know, when I am done.

2015-06-12 14:03 GMT+02:00 Till Rohrmann <till.rohrmann@gmail.com>:

> I'm in favour of option b) as well.
>
> On Fri, Jun 12, 2015 at 12:05 PM Ufuk Celebi <uce@apache.org> wrote:
>
> > Yes, the LICENSE files are definitely a release blocker.
> >
> > a) Either we wait with the RC until we have fixed the LICENSES, or
> >
> > b) Put out next RC to continue with testing and then update it with the
> > LICENSE [either we find something before the LICENSE update or we only
> have
> > to review the LICENSE change]
> >
> > Since this is not a vote yet, it doesn't really matter, but I'm leaning
> > towards b).
> >
> >
> > On Fri, Jun 12, 2015 at 11:43 AM, Till Rohrmann <till.rohrmann@gmail.com
> >
> > wrote:
> >
> > > What about the shaded jars?
> > >
> > > On Fri, Jun 12, 2015 at 11:32 AM Ufuk Celebi <uce@apache.org> wrote:
> > >
> > > > @Max: for the new RC. Can you make sure to set the variables
> correctly
> > > > with regard to stable/snapshot versions in the docs?
> > >
> >
>

--001a11c3cc623ddd12051850f01e--
#|#<CAC27z=OnKvHOkpoNJmYeM9JDUSAyfyafWVaPG=wFSyDGv7-jZg@mail.gmail.com>##//##<CAAdrtT2w=djd6gc3k55Am6fhiEfEqzxE8YWwXJ6vte8BAJn9OA@mail.gmail.com>#|#2015-04-03-09:17:17#|#Fabian Hueske <fhueske@gmail.com>#|#Re: Release 0.9.0-milestone-1 preview#|#
--089e0149397aeb8d7a0512ce65f8
Content-Type: text/plain; charset=UTF-8

Thanks Robert for pushing this forward.

I'd like to have the following issues fixed in the release:
- FLINK[1656] by PR #525
- FLINK[1776] by PR #532
- FLINK[1664] by PR #541
- FLINK[1817] by PR #565
- Failed tests on Windows by PR #491

Especially the first two fixes crucial.
They address semantic properties are which are newly designed for 0.9 and
prevent invalid execution plan.

2015-04-03 10:49 GMT+02:00 Robert Metzger <rmetzger@apache.org>:

> Hi All,
>
> As discussed on this list, we've decided to create a release outside the
> regular 3 monthly release schedule for the ApacheCon announcement and for
> giving our users a convenient way of trying out the great new features.
>
> This thread is not an official release vote. It is meant for testing the
> release script and for having a reference version to test against.
> Also, I hope that we don't need multiple [VOTE] threads.
>
> I've started creating a wiki page with a list of steps to verify a release.
> Please add items if I forgot something:
> https://cwiki.apache.org/confluence/display/FLINK/Releasing
>
>
> The release artifacts:
> http://people.apache.org/~rmetzger/flink-0.9.0-milestone-1-rc0/
>
> The staging repository:
> https://repository.apache.org/content/repositories/orgapacheflink-1034
>
>
> The release commit (currently only in my GitHub account):
>
> https://github.com/rmetzger/flink/commit/484df55a42f0708bb5bbbd6c19778e2d7c078f1c
>
>
> Lets collect everything we want to fix for the final release here.
> Please remember that we have to start the [VOTE] next week, otherwise we'll
> miss the ApacheCon announcement.
>

--089e0149397aeb8d7a0512ce65f8--
#|#<CAGr9p8DV_9NLuRCxHE4LuenseA1H8QMpiHGiHxHhNdL_tLWp4g@mail.gmail.com>##//##<CAAdrtT2xMm4V6jZu2FccVZTQUu5pjZrY_Ljd7cz68gor=+Z-2A@mail.gmail.com>#|#2015-06-05-08:48:57#|#Fabian Hueske <fhueske@gmail.com>#|#Re: GSoC work#|#
--089e0160acae96aaea0517c159c8
Content-Type: text/plain; charset=UTF-8

Hi Sachin,

nice blog! :-)
Happy you joined the community and looking forward for many cool features
to come.

Best, Fabian

2015-06-04 9:43 GMT+02:00 Till Rohrmann <trohrmann@apache.org>:

> +1 :-)
>
> On Wed, Jun 3, 2015 at 4:53 PM, Vasiliki Kalavri <
> vasilikikalavri@gmail.com>
> wrote:
>
> > Hi Sachin,
> >
> > great idea to keep a blog! Thanks a lot for sharing :))
> >
> > -V.
> >
> > On 3 June 2015 at 16:41, Sachin Goel <sachingoel0101@gmail.com> wrote:
> >
> > > Hi everyone
> > > I'm maintaining a blog detailing my work here:
> > > https://tolkienaboutcode.wordpress.com/
> > > I've already made two posts about my work so far. Any feedback would be
> > > welcome. :)
> > > The relevant JIRA issues are Flink-1727
> > > <https://issues.apache.org/jira/browse/FLINK-1727> and Flink-2131
> > > <https://issues.apache.org/jira/browse/FLINK-2131>.
> > >
> > > Cheers!
> > > Sachin Goel
> > >
> >
>

--089e0160acae96aaea0517c159c8--
#|#<CAC27z=OW1_t1EyQV5skdFZRevKu-ya2aYYBkruV1ys-TfFa_4Q@mail.gmail.com>##//##<CAAdrtT2yZeS9s=2fzjPDALJ7jy_JRGXhmAhnZenzDC9t3m8wtQ@mail.gmail.com>#|#2015-03-29-12:12:09#|#Fabian Hueske <fhueske@gmail.com>#|#=?UTF-8?B?UmU6IOetlOWkjTogW1ZPVEVdIE5hbWUgb2YgRXhwcmVzc2lvbiBBUEkgUmVwcmVzZW50YQ==?=#|#
--047d7b343338b5d02805126c44aa
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Cool, looking forward to use this great feature! :-)
On Mar 29, 2015 12:26 PM, "Aljoscha Krettek" <aljoscha@apache.org> wrote:

> I hereby close the vote. Thanks for all your votes!
>
> We have 15 votes:
>   +Relation: 4
>   +DataTable: 3
>   +Table: 8
>
> So I will rename ExpressionOperation to Table and finally merge the
> pull request with the rename and Java support.
>
> On Thu, Mar 26, 2015 at 12:46 PM, Matadorhong <hongsibao@huawei.com>
> wrote:
> > +Table
> >
> > =E5=8F=91=E4=BB=B6=E4=BA=BA: aalexandrov [via Apache Flink (Incubator) Mailing List archive.]
> [mailto:ml-node+s1008284n4743h62@n3.nabble.com]
> > =E5=8F=91=E9=80=81=E6=97=B6=E9=97=B4: 2015=E5=B9=B43=E6=9C=8826=E6=97=A5 19:40
> > =E6=94=B6=E4=BB=B6=E4=BA=BA: Hongsibao
> > =E4=B8=BB=E9=A2=98: Re: [VOTE] Name of Expression API Representation
> >
> > +Table
> >
> > 2015-03-26 10:28 GMT+01:00 Robert Metzger <[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D0>>:
> >
> >> +Table
> >>
> >>
> >> On Thu, Mar 26, 2015 at 10:13 AM, Aljoscha Krettek <[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D1>>
> >> wrote:
> >>
> >> > Thanks Henry. :D
> >> >
> >> > +Relation
> >> >
> >> > On Thu, Mar 26, 2015 at 9:36 AM, Till Rohrmann <[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D2>>
> >> > wrote:
> >> > > +Table
> >> > >
> >> > > On Thu, Mar 26, 2015 at 9:32 AM, M=C3=A1rton Balassi <
> >> > [hidden email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D3>>
> >> > > wrote:
> >> > >
> >> > >> +DataTable
> >> > >>
> >> > >> On Thu, Mar 26, 2015 at 9:29 AM, Markl, Volker, Prof. Dr. <
> >> > >> [hidden email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D4>> wrote:
> >> > >>
> >> > >> > +Table
> >> > >> >
> >> > >> > I also agree with that line of argument (think SQL ;-) )
> >> > >> >
> >> > >> > -----Urspr=C3=BCngliche Nachricht-----
> >> > >> > Von: Timo Walther [mailto:[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D5>]
> >> > >> > Gesendet: Donnerstag, 26. M=C3=A4rz 2015 09:28
> >> > >> > An: [hidden email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D6>
> >> > >> > Betreff: Re: [VOTE] Name of Expression API Representation
> >> > >> >
> >> > >> > +Table API
> >> > >> >
> >> > >> > Same thoughts as Stephan. Table is more common in the economy
> than
> >> > >> > Relation.
> >> > >> >
> >> > >> > On 25.03.2015 21:30, Stephan Ewen wrote:
> >> > >> > > +Table API / Table
> >> > >> > >
> >> > >> > > I have a feeling that Relation is a name mostly used by people
> >> with
> >> > a
> >> > >> > > deeper background in (relational) databases, while table is
> more
> >> the
> >> > >> > > pragmatic developer term. (As a reason for my choice) Am
> >> 25.03.2015
> >> > >> > > 20:37 schrieb "Fabian Hueske" <[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D7>>:
> >> > >> > >
> >> > >> > >> I think the voting scheme is clear.
> >> > >> > >> The mail that started the thread says:
> >> > >> > >>
> >> > >> > >> "The name with the most votes is chosen.
> >> > >> > >> If the vote ends with no name having the most votes, a new
> vote
> >> > with
> >> > >> > >> an alternative voting scheme will be done."
> >> > >> > >>
> >> > >> > >> So let's go with a single vote and handle corner cases as they
> >> > appear.
> >> > >> > >>
> >> > >> > >> 2015-03-25 20:24 GMT+01:00 Ufuk Celebi <[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D8>>:
> >> > >> > >>
> >> > >> > >>> +Table, DataTable
> >> > >> > >>>
> >> > >> > >>> ---
> >> > >> > >>>
> >> > >> > >>> How are votes counted? When voting for the name of the
> project,
> >> we
> >> > >> > >>> didn't vote for one name, but gave a preference ordering.
> >> > >> > >>>
> >> > >> > >>> In this case, I am for Table or DataTable, but what happens
> if I
> >> > >> > >>> vote for Table and then there is a tie between DataTable and
> >> > >> > >>> Relation? Will Table count for DataTable then?
> >> > >> > >>>
> >> > >> > >>> =E2=80=93 Ufuk
> >> > >> > >>>
> >> > >> > >>> On 25 Mar 2015, at 18:33, Vasiliki Kalavri
> >> > >> > >>> <[hidden email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D9>>
> >> > >> > >>> wrote:
> >> > >> > >>>
> >> > >> > >>>> +Relation
> >> > >> > >>>> On Mar 25, 2015 6:29 PM, "Henry Saputra" <
> >> > [hidden email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D10>>
> >> > >> > >>> wrote:
> >> > >> > >>>>> +Relation
> >> > >> > >>>>>
> >> > >> > >>>>> PS
> >> > >> > >>>>> Aljoscha, don't forget to cast your own vote :)
> >> > >> > >>>>>
> >> > >> > >>>>>
> >> > >> > >>>>> On Wednesday, March 25, 2015, Aljoscha Krettek
> >> > >> > >>>>> <[hidden
> email]</user/SendEmail.jtp?type=3Dnode&node=3D4743&i=3D11>>
> >> > >> > >>>>> wrote:
> >> > >> > >>>>>
> >> > >> > >>>>>> Please vote on the new name of the equivalent to DataSet
> and
> >> > >> > >>>>>> DataStream in the new expression-based API.
> >> > >> > >>>>>>
> >> > >> > >>>>>>  From the previous discussion thread three names emerged:
> >> > >> > >>>>>> Relation, Table and DataTable.
> >> > >> > >>>>>>
> >> > >> > >>>>>> The vote is open for the next 72 hours.
> >> > >> > >>>>>> The name with the most votes is chosen.
> >> > >> > >>>>>> If the vote ends with no name having the most votes, a new
> >> vote
> >> > >> > >>>>>> with an alternative voting scheme will be done.
> >> > >> > >>>>>>
> >> > >> > >>>>>> Please vote either of these:
> >> > >> > >>>>>>
> >> > >> > >>>>>> +Relation
> >> > >> > >>>>>> +Table
> >> > >> > >>>>>> +DataTable
> >> > >> > >>>>>>
> >> > >> > >>>
> >> > >> >
> >> > >> >
> >> > >>
> >> >
> >>
> >
> > ________________________________
> > If you reply to this email, your message will be added to the discussion
> below:
> >
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/VOTE-Name-of-Expression-API-Representation-tp4708p4743.html
> > To start a new topic under Apache Flink (Incubator) Mailing List
> archive., email ml-node+s1008284n1h60@n3.nabble.com<mailto:
> ml-node+s1008284n1h60@n3.nabble.com>
> > To unsubscribe from Apache Flink (Incubator) Mailing List archive.,
> click here<
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/template/NamlServlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3DaG9uZ3NpYmFvQGh1YXdlaS5jb218MXwxNDgwNTY1MzA4
> >.
> > NAML<
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/template/NamlServlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&base=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> http://apache-flink-incubator-mailing-list-archive.1008284.n3.nabble.com/VOTE-Name-of-Expression-API-Representation-tp4708p4744.html
> > Sent from the Apache Flink (Incubator) Mailing List archive. mailing
> list archive at Nabble.com.
>

--047d7b343338b5d02805126c44aa--
#|#<CANMXwW2fQcDzJxV51oGb=GpRKP7+vxvEZ_D0gkGA3svYoq_jJQ@mail.gmail.com>##//##<CAAdrtT3CzHme0-T2xD=hwCmnrxq2yeZc+XnQQoFYEdPvU2H1JA@mail.gmail.com>#|#2014-08-18-19:19:32#|#Fabian Hueske <fhueske@apache.org>#|#Re: Adding the streaming project to the main repository#|#
--089e013d0c9ed826e20500ec3e66
Content-Type: text/plain; charset=UTF-8

Hi folks,

great work!

Looking at the example I have a quick question. What's the semantics of the
Reduce operator? I guess its not a window reduce.
Is it backed by a hash table and every input tuple updates the hash table
and returns the updated value?

Cheers, Fabian


2014-08-18 20:53 GMT+02:00 Stephan Ewen <sewen@apache.org>:

> The streaming code is in "flink-addons", for new/experimental code.
>
> Documents should come over the next days/weeks, definitely before we make
> this part of the core.
>
> Right now, I would suggest to have a look at some of the examples, to get a
> feeling for the addon, check for example this here:
>
> https://github.com/apache/incubator-flink/tree/master/flink-addons/flink-streaming/flink-streaming-examples/src/main/java/org/apache/flink/streaming/examples/wordcount
>
> (The example reads a file for simplicity, but the project also provides
> connectors for Kafka, RabbitMQ, ...)
>

--089e013d0c9ed826e20500ec3e66--
#|#<CANC1h_sDae12d3Q2LXngbzoJPNpn3O=jk3EgH6ET5Bov=W_e3A@mail.gmail.com>##//##<CAAdrtT3hnB8Zw6k47c=vn_LpQEu1hGqxjSLenvgt6m8inZoh1w@mail.gmail.com>#|#2015-01-17-19:46:49#|#Fabian Hueske <fhueske@apache.org>#|#Re: How to use org.apache.hadoop.mapreduce.lib.input.MultipleInputs#|#
--089e0160ac829cf655050cde59d6
Content-Type: text/plain; charset=UTF-8

Why don't you just create two data sources that each wrap the ParquetFormat
using a HadoopInputFormat and join them as for example done in the TPCH Q3
example [1]

I always found the MultipleInputFormat to be an ugly workaround for
Hadoop's deficiency to read data from multiple sources.
AFAIK, Hadoop's MultipleInputFormat does not provide data colocation that a
join could exploit. Or is there any other beneficial property that I am not
aware of?

[1]
https://github.com/apache/flink/blob/master/flink-examples/flink-java-examples/src/main/java/org/apache/flink/examples/java/relational/TPCHQuery3.java



2015-01-17 20:15 GMT+01:00 Felix Neutatz <neutatz@googlemail.com>:

> Hi,
>
> is there any example which shows how I can load several files with
> different Hadoop input formats at once? My use case is that I want to load
> two tables (in Parquet format) via Hadoop and join them within Flink.
>
> Best regards,
>
> Felix
>

--089e0160ac829cf655050cde59d6--
#|#<CABq57MikkCiZ+J4LBTATw=gVMPseQVykK7Sv+P4XfkczHHT4rg@mail.gmail.com>##//##<CAAdrtT3rHVD3sH3Gcr9Zzrg5aYUBoJLGAy1tZqJ896eDme-C6g@mail.gmail.com>#|#2015-01-21-19:39:04#|#Fabian Hueske <fhueske@gmail.com>#|#Re: Very strange behaviour of groupBy() -> sort() -> first()#|#
--089e0160ac82a364a6050d2eb382
Content-Type: text/plain; charset=UTF-8

Chesnay is right.
Right now, it is not possible to do want you want in a straightforward way
because Flink does not support to fully sort a data set (there are several
related issues in JIRA).

A workaround would be to attach a constant value to each tuple, group on
that (all tuples are sent to the same group), sort that group, and apply
the first operator.

2015-01-21 20:22 GMT+01:00 Chesnay Schepler <chesnay.schepler@fu-berlin.de>:

> If i remember correctly first() returns the first n values for every
> group. the javadocs actually don't make this behaviour very clear.
>
>
> On 21.01.2015 19:18, Felix Neutatz wrote:
>
>> Hi,
>>
>> my use case is the following:
>>
>> I have a Tuple2<String,Long>. I want to group by the String and sum up the
>> Long values accordingly. This works fine with these lines:
>>
>> DataSet<Lineitem> lineitems = getLineitemDataSet(env);
>> lineitems.project(new int []{3,0}).groupBy(0).aggregate(Aggregations.SUM,
>> 1);
>>
>> After the aggregation I want to print the 10 groups with the highest sum,
>> like:
>>
>> string1, 100L
>> string2, 50L
>> string3, 1L
>>
>> I tried that:
>>
>> lineitems.project(new int []{3,0}).groupBy(0).aggregate(Aggregations.SUM,
>> 1).groupBy(0).sortGroup(1, Order.DESCENDING).first(3).print();
>>
>> But instead of 3 records, I get a lot more.
>>
>> Can see my error?
>>
>> Best regards,
>>
>> Felix
>>
>>
>

--089e0160ac82a364a6050d2eb382--
#|#<54BFFC64.6010708@fu-berlin.de>##//##<CAAdrtT3s-bV04dbRP4ut1TH1TNaLHCkKufwsbE32KiTi8dunxg@mail.gmail.com>#|#2015-02-23-21:39:47#|#Fabian Hueske <fhueske@gmail.com>#|#Re: Question about scheduling in 0.7#|#
--001a11392048cfabfb050fc838f5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Nico,

yes, Flink runs tasks in threads. Each TaskManager runs in its own JVM,
everything within a TaskManager is parallelized in threads. Since a TM can
offer multiple slots, also tasks across slots run in the same JVM and in
different threads.

Flink has a pipelined processing model, which means that multiple
sequential tasks can run at the same time and that data is pushed from task
to task in a pipelined fashion. While this was the only communication model
until now, there are currently efforts to also materialize intermediate
datasets (apart from full sorts) for more flexible data shipping strategies
and better fault tolerance.

Let me know if you have further questions about Flink internals.

Best, Fabian

2015-02-23 22:21 GMT+01:00 Nico Scherer <jdam@live.de>:

> Hi all,
>
> we=E2=80=99re currently playing/messing around with scheduling in flink 0.7. We
> found out that if we run a single job with a certain degree of parallelism,
> multiple tasks/vertices are executed within a single task manager at the
> same time (or at least before the prior stage is switched to finished).
> Further, we noticed that only as many flink instances are requested as the
> DoP is set and different stages are running in the same slot. We=E2=80=99re
> wondering how this is implemented? Is a single slot using threading to
> execute multiple tasks at once?
>
>
> Regards,
>
> Nico
>
>
>
>

--001a11392048cfabfb050fc838f5--
#|#<BLU436-SMTP193525828634317849DC116C1290@phx.gbl>##//##<CAAdrtT3wAjggZXS1+DSLMmGgiKYXzOS8=SCz0-dEAcSpMfoG1Q@mail.gmail.com>#|#2015-04-28-22:55:59#|#Fabian Hueske <fhueske@gmail.com>#|#Re: [DISCUSS] Flink and Ignite integration#|#
--001a11c35110418dde0514d0c246
Content-Type: text/plain; charset=UTF-8

Thanks Cos for starting this discussion, hi to the Ignite community!

The probably easiest and most straightforward integration of Flink and
Ignite would be to go through Ignite's IGFS. Flink can be easily extended
to support additional filesystems.

However, the Flink community is currently also looking for a solution to
checkpoint operator state of running stream processing programs. Flink
processes data streams in real time similar to Storm, i.e., it schedules
all operators of a streaming program and data is continuously flowing from
operator to operator. Instead of acknowledging each individual record,
Flink injects stream offset markers into the stream in regular intervals.
Whenever, an operator receives such a marker it checkpoints its current
state (currently to the master with some limitations). In case of a
failure, the stream is replayed (using a replayable source such as Kafka)
from the last checkpoint that was not received by all sink operators and
all operator states are reset to that checkpoint.
We had already looked at Ignite and were wondering whether Ignite could be
used to reliably persist the state of streaming operator.

The other points I mentioned on Twitter are just rough ideas at the moment.

Cheers, Fabian

2015-04-29 0:23 GMT+02:00 Dmitriy Setrakyan <dsetrakyan@apache.org>:

> Thanks Cos.
>
> Hello Flink Community.
>
> From Ignite standpoint we definitely would be interested in providing Flink
> processing API on top of Ignite Data Grid or IGFS. It would be interesting
> to hear what steps would be required for such integration or if there are
> other integration points.
>
> D.
>
> On Tue, Apr 28, 2015 at 2:57 PM, Konstantin Boudnik <cos@apache.org>
> wrote:
>
> > Following the lively exchange in Twitter (sic!) I would like to bring
> > together
> > Ignite and Flink communities to discuss the benefits of the integration
> and
> > see where we can start it.
> >
> > We have this recently opened ticket
> >   https://issues.apache.org/jira/browse/IGNITE-813
> >
> > and Fabian has listed the following points:
> >
> >  1) data store
> >  2) parameter server for ML models
> >  3) Checkpointing streaming op state
> >  4) continuously updating views from streams
> >
> > I'd add
> >  5) using Ignite IGFS to speed up Flink's access to HDFS data.
> >
> > I see a lot of interesting correlations between two projects and wonder
> if
> > Flink guys can step up with a few thoughts on where Flink can benefit the
> > most
> > from Ignite's in-memory fabric architecture? Perhaps, it can be used as
> > in-memory storage where the other components of the stack can quickly
> > access
> > and work w/ the data w/o a need to dump it back to slow storage?
> >
> > Thoughts?
> >   Cos
> >
>

--001a11c35110418dde0514d0c246--
